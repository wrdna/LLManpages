{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "VIc73r_ubp0v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIc73r_ubp0v",
    "outputId": "100c81e3-f775-4d5b-821c-3ee2674f1663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (0.44.1)\n",
      "Requirement already satisfied: accelerate in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: datasets in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: trl in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (0.12.1)\n",
      "Requirement already satisfied: torch in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from bitsandbytes) (2.5.0)\n",
      "Requirement already satisfied: numpy in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from accelerate) (0.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: rich in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from trl) (13.9.3)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from trl) (4.46.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.46.0->trl) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.46.0->trl) (0.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes accelerate datasets trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce26ac9-661f-44ae-8e40-3c4840621d2d",
   "metadata": {
    "id": "4ce26ac9-661f-44ae-8e40-3c4840621d2d"
   },
   "outputs": [],
   "source": [
    "# TOKEN\n",
    "\n",
    "\n",
    "TOKEN=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403cb14c-34fa-4c5e-9d2a-26929968f2aa",
   "metadata": {
    "id": "403cb14c-34fa-4c5e-9d2a-26929968f2aa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cdc8fe8-fb32-452f-9093-cf67ad47cb78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cdc8fe8-fb32-452f-9093-cf67ad47cb78",
    "outputId": "d83584a2-2383-4e2f-ad9d-f44561878fa3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "base_model = \"google-t5/t5-large\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "# Configuartion of model quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map = \"cuda:0\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ecebf44-561b-4352-87fa-54e4412161a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ecebf44-561b-4352-87fa-54e4412161a6",
    "outputId": "7bc2815e-112b-4dd3-aab0-5fabf1129a64",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d7b454-555f-4a17-ab02-d34c9cc3a484",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18d7b454-555f-4a17-ab02-d34c9cc3a484",
    "outputId": "2503b137-799c-4ef2-fe00-b4608371deed",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5TokenizerFast(name_or_path='google-t5/t5-large', vocab_size=32100, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b943957-e9ac-4130-a92c-bbe697b2beb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b943957-e9ac-4130-a92c-bbe697b2beb1",
    "outputId": "d358fe1a-9e97-4a9a-e118-e154c8b07772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><extra_id_0> fish.</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Exterior like fish \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100, temperature=0.7, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hcCK9vwqdxHp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcCK9vwqdxHp",
    "outputId": "bf4404d5-357a-4110-a5c2-1cf986492ef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><extra_id_0> &<extra_id_1>: Lemon, Olive Oil, & Ginger; Roasted chicken, Mediterranean<extra_id_2>ed Chicken<extra_id_3>: Organic, Mediterranean, Mediterranean Chicken Proteins: Italian, & Olive Oil; Mediterranean<extra_id_4> Mediterranean Roasted Chicken. Served with Lemon and olive oil.<extra_id_5> Chicken Mediterranean<extra_id_6> Chicken Mediterranean Roasted Chicken Ingredients:<extra_id_7>ing<extra_id_8> Mediterranean Roasted Chicken<extra_id_9> chicken<extra_id_10> chicken<extra_id_11> Mediterranean<extra_id_12> chicken<extra_id_13> chicken<extra_id_14> chicken<extra_id_15> chicken<extra_id_16> chicken<extra_id_17> Mediterranean<extra_id_18> Mediterranean<extra_id_19>in<extra_id_20> Ingredients<extra_id_21> Chicken<extra_id_22></s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Mediterranean Roasted Chicken\\n\\nIngredients:\\n\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=200, temperature=0.7, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49104eed-9477-46db-80d1-e81a3139b333",
   "metadata": {
    "id": "49104eed-9477-46db-80d1-e81a3139b333"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import peft\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77929a52-a207-40c8-ad02-7851d905fa81",
   "metadata": {
    "id": "77929a52-a207-40c8-ad02-7851d905fa81"
   },
   "source": [
    "# Datset Grabbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2104272f-3815-44fa-8994-2f873243012e",
   "metadata": {
    "id": "2104272f-3815-44fa-8994-2f873243012e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 20419 examples [00:01, 10350.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"json\", data_files={\"train\": \"masked_man_pages.json\"}, split=\"train\")\n",
    "df = pd.DataFrame(dataset)\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c937ca74-275b-44cf-9ae9-00d4c5cf5360",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "c937ca74-275b-44cf-9ae9-00d4c5cf5360",
    "outputId": "1cf3593c-1c30-49ae-d792-a793bec0031a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20419</td>\n",
       "      <td>20419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>17633</td>\n",
       "      <td>19219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>&lt;SECTION&gt;REPORTING BUGS&lt;/SECTION&gt;\\nGNU coreuti...</td>\n",
       "      <td>&lt;SECTION&gt;SYNOPSIS&lt;/SECTION&gt;\\nopenssl cmd -help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>102</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    input  \\\n",
       "count                                               20419   \n",
       "unique                                              17633   \n",
       "top     <SECTION>REPORTING BUGS</SECTION>\\nGNU coreuti...   \n",
       "freq                                                  102   \n",
       "\n",
       "                                                   output  \n",
       "count                                               20419  \n",
       "unique                                              19219  \n",
       "top     <SECTION>SYNOPSIS</SECTION>\\nopenssl cmd -help...  \n",
       "freq                                                   51  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85c14f2d-2007-416c-8ff9-c13939f930c5",
   "metadata": {
    "id": "85c14f2d-2007-416c-8ff9-c13939f930c5"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "df = df.drop_duplicates(subset=[\"input\"])\n",
    "df = df.drop_duplicates(subset=[\"output\"])\n",
    "\n",
    "df.describe()\n",
    "seed = 42  # Set your desired seed\n",
    "# df = df.sample(n=2000, random_state=seed)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5139340a-4675-4f4b-b0cf-6dcceb9c930a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5139340a-4675-4f4b-b0cf-6dcceb9c930a",
    "outputId": "e2e92346-be12-4893-ccb8-3ac3332ae37c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17628 entries, 0 to 17627\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   17628 non-null  object\n",
      " 1   output  17628 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 275.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "wAi9EBxqsBDZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "wAi9EBxqsBDZ",
    "outputId": "32437d23-bbb4-4caf-91cc-106db6dd3cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 1.0\n",
      "Mean: 1\n",
      "STD: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statistics as stats\n",
    "\n",
    "lengths = [len(str(item).split()) for item in df]\n",
    "lengths.sort(reverse=True)\n",
    "\n",
    "print(f\"Median: {stats.median(lengths)}\")\n",
    "print(f\"Mean: {stats.mean(lengths)}\")\n",
    "print(f\"STD: {stats.stdev(lengths)}\")\n",
    "\n",
    "plt.hist(lengths, bins=range(max(lengths) + 2), align='left', edgecolor='black')\n",
    "plt.xlabel('Sentence Length (number of words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Joke Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "776eddc0-093e-45f7-a6d2-97e37713c6ec",
   "metadata": {
    "id": "776eddc0-093e-45f7-a6d2-97e37713c6ec"
   },
   "outputs": [],
   "source": [
    "final_ds = dataset.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81e08a74-8b23-4088-b4de-4cef0d3cc6f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81e08a74-8b23-4088-b4de-4cef0d3cc6f3",
    "outputId": "b5d753e5-5551-43d4-d708-eec3b9af9dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 14102\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 3526\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(final_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c3d5ce2-f5ea-4e8e-a6d5-6e305dfc9de6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c3d5ce2-f5ea-4e8e-a6d5-6e305dfc9de6",
    "outputId": "f6360542-4591-41d5-decd-17187a8b5df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input', 'output']\n"
     ]
    }
   ],
   "source": [
    "print(final_ds[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13234f4c-f64c-448c-8553-2612011a49b2",
   "metadata": {
    "id": "13234f4c-f64c-448c-8553-2612011a49b2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f55eef0-152e-41a9-90c5-6f61b25fa041",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "33935c76b57948468b861d657fc9573f",
      "83a1afb24ea54eb18b30009a5be2f37f",
      "4758f4169f7e44e2818648873f08e049",
      "4e74976f45b441edb956a59c338c5772",
      "a20db37b9f7341b8a256e5ce7c4e4658",
      "f63784a5672f45c0bca36d5afc3e4793",
      "7c7aceab5c044f0eb395b649bc78df36",
      "71980821352641788e0b9c8a90b1abfb",
      "8a57a71ca40a4dbc82b5b290fc68284a",
      "f1afba226a4545f7b28c58f5bf970cd1",
      "3e5efa49cfc4412d9e8c0d9e26145e54",
      "73e2e453d9514749a2ce6e555b8857af",
      "59e3a962819345ed887a3746704de5d7",
      "12eece125e854c99b1b1a75a439fdd6b",
      "0a2fd6855667473ca87a48d95722f29f",
      "a59cb746f96e406380b053c9ca2ed057",
      "36ec7f05c7f741e0a5a6afb9faf0d5a5",
      "344eeba5c2cb4251a30ae4d71235cf89",
      "a929ec96f98240a18133cb3988c9139f",
      "2ee908ba9473404d9fd51bbf07b8ae98",
      "6ba03846819f4e3ca89dd28f4c939f70",
      "e3577a5d7b3d496999e9467ac6c35747"
     ]
    },
    "id": "2f55eef0-152e-41a9-90c5-6f61b25fa041",
    "outputId": "4a4ab7b5-77f7-4053-802a-2d6f81371032"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 14102/14102 [01:10<00:00, 200.73 examples/s]\n",
      "Map: 100%|| 3526/3526 [00:18<00:00, 194.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    # Minimal formatting, directly using input and output fields\n",
    "    return [f\"{example['input']}\\n{example['output']}\"]\n",
    "    \n",
    "def format_and_tokenize_batch(examples):\n",
    "    input_texts = [f\"manpage: {inp}\" for inp in examples[\"input\"]]\n",
    "    target_texts = examples[\"output\"]\n",
    "\n",
    "    input_encodings = tokenizer(\n",
    "        input_texts,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    target_encodings = tokenizer(\n",
    "        target_texts,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Apply label masking\n",
    "    labels = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in target]\n",
    "        for target in target_encodings[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# Apply the batch-friendly function\n",
    "tokenized_ds = final_ds.map(\n",
    "    format_and_tokenize_batch,\n",
    "    batched=True,  # Process examples in batches\n",
    "    remove_columns=[\"input\", \"output\"],  # Remove original columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc7c580c-b69a-47a2-b7d7-0a8a60afe7cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc7c580c-b69a-47a2-b7d7-0a8a60afe7cc",
    "outputId": "eda7ec6f-2b3d-4f2b-e8d3-84eee35dae85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '<SECTION>FILES</SECTION>\\n/usr/share/tabset\\n              tab stop initialization database',\n",
       " 'output': \"<SECTION>NAME</SECTION>\\ntabs - set terminal tab stops\\n<SECTION>SYNOPSIS</SECTION>\\ntabs [options] [tabstop-list]\\n<SECTION>DESCRIPTION</SECTION>\\nThe  tabs program clears and sets tab-stops on the terminal.  This uses\\n       the terminfo clear_all_tabs and set_tab capabilities.  If either is ab\\n       sent, tabs is unable to clear/set tab-stops.  The  terminal  should  be\\n       configured to use hard tabs, e.g.,\\n\\n           stty tab0\\n\\n       Like  clear(1),  tabs  writes to the standard output.  You can redirect\\n       the standard output to a file (which prevents tabs from actually chang\\n       ing the tabstops), and later cat the file to the screen,  setting  tab\\n       stops at that point.\\n\\n       These  are  hardware  tabs, which cannot be queried rapidly by applica\\n       tions running in the terminal, if at all.  Curses and other full-screen\\n       applications may use hardware tabs in optimizing their  output  to  the\\n       terminal.   If the hardware tabstops differ from the information in the\\n       terminal database, the result is unpredictable.  Before running  curses\\n       programs, you should either reset tab-stops to the standard interval\\n\\n           tabs -8\\n\\n       or  use the reset program, since the normal initialization sequences do\\n       not ensure that tab-stops are reset.\\n<SECTION>OPTIONS</SECTION>\\nGeneral Options\\n       -Tname\\n            Tell tabs which terminal type to  use.   If  this  option  is  not\\n            given,  tabs  will use the $TERM environment variable.  If that is\\n            not set, it will use the ansi+tabs entry.\\n\\n       -d   The debugging option shows a ruler  line,  followed  by  two  data\\n            lines.   The  first  data line shows the expected tab-stops marked\\n            with asterisks.  The second data line shows the actual  tab-stops,\\n            marked with asterisks.\\n\\n       -n   This  option tells tabs to check the options and run any debugging\\n            option, but not to modify the terminal settings.\\n\\n       -V   reports the version of ncurses which was used in this program, and\\n            exits.\\n\\n       The tabs program processes a single list of tab stops.  The last option\\n       to be processed which defines a list is the  one  that  determines  the\\n       list to be processed.\\n\\n   Implicit Lists\\n       Use  a  single number as an option, e.g., -5 to set tabs at the given\\n       interval (in this case 1, 6, 11, 16, 21, etc.).  Tabs are  repeated  up\\n       to the right margin of the screen.\\n\\n       Use -0 to clear all tabs.\\n\\n       Use -8 to set tabs to the standard interval.\\n\\n   Explicit Lists\\n       An  explicit list can be defined after the options (this does not use a\\n       -).  The values in the list must be in increasing numeric order,  and\\n       greater than zero.  They are separated by a comma or a blank, for exam\\n       ple,\\n\\n           tabs 1,6,11,16,21\\n           tabs 1 6 11 16 21\\n\\n       Use  a  +  to treat a number as an increment relative to the previous\\n       value, e.g.,\\n\\n           tabs 1,+5,+5,+5,+5\\n\\n       which is equivalent to the 1,6,11,16,21 example.\\n\\n   Predefined Tab Stops\\n       POSIX defines several predefined lists of tab stops.\\n\\n       -a   Assembler, IBM S/370, first format\\n            1,10,16,36,72\\n\\n       -a2  Assembler, IBM S/370, second format\\n            1,10,16,40,72\\n\\n       -c   COBOL, normal format\\n            1,8,12,16,20,55\\n\\n       -c2  COBOL compact format\\n            1,6,10,14,49\\n\\n       -c3  COBOL compact format extended\\n            1,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62,67\\n\\n       -f   FORTRAN\\n            1,7,11,15,19,23\\n\\n       -p   PL/I\\n            1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61\\n\\n       -s   SNOBOL\\n            1,10,55\\n\\n       -u   UNIVAC 1100 Assembler\\n            1,12,20,44\\n\\n   Margins\\n       A few terminals expose a means of changing their left  and  right  mar\\n       gins.  tabs supports this feature with an option.\\n\\n       +m margin\\n            The effect depends on whether the terminal has the margin capabil\\n            ities:\\n\\n               If  the  terminal provides the capability for setting the left\\n                margin, tabs uses this, and adjusts  the  available  tab  stop\\n                widths.\\n\\n               If the terminal does not provide the margin capabilities, tabs\\n                imitates their effect, putting tab stops at appropriate places\\n                on each line.  The terminal's left margin is not modified.\\n\\n            If the margin parameter is omitted, the default is 10.  Use +m0 to\\n            reset  the  left  margin, that is, to make it the left edge of the\\n            terminal's display.  Before setting a left margin, tabs resets the\\n            margin to reduce problems that might arise from moving the  cursor\\n            to the left of the current left margin.\\n\\n       When  setting  or  resetting  the  left margin, tabs may also reset the\\n       right margin.\\n<SECTION>PORTABILITY</SECTION>\\nIEEE  Std  1003.1/The  Open   Group   Base   Specifications   Issue   7\\n       (POSIX.1-2008) describes a tabs utility.  However,\\n\\n          this  standard  describes a +m option to set a terminal's left mar\\n           gin.  Very few of the entries in the terminal database provide  the\\n           set_left_margin (smgl) or set_left_margin_parm (smglp) capabilities\\n           needed to support the feature.\\n\\n          There  is no counterpart in X/Open Curses Issue 7 for this utility,\\n           unlike tput(1).\\n\\n       The -d (debug) and -n (no-op) options are ncurses extensions  not  pro\\n       vided by other implementations.\\n<SECTION>HISTORY</SECTION>\\nA  tabs  utility  appeared  in  PWB/Unix 1.0 (1977).  A reduced version\\n       shipped in Seventh Edition Unix (early 1979) and  in  3BSD  (later  the\\n       same year); it supported a -n option to set the first tab stop at the\\n       left margin.  That option is not specified by POSIX.\\n\\n       The  PWB/Unix  tabs  utility  returned  in  System III (1980), and used\\n       built-in tables to support a  half-dozen  hardcopy  terminal  (printer)\\n       types.   It  also had logic to support setting the left margin, as well\\n       as a feature for copying the tab settings from a file.\\n\\n       Versions of the program in later releases of AT&T Unix, such  as  SVr4,\\n       added  support  for  the  terminal database, but retained the tables to\\n       support the printers.  By this time, System V tput had incorporated the\\n       tab stop initialization feature of BSD's tset from 1982,  but  employed\\n       the terminfo database to do so.\\n\\n       The  +m  option was documented in the POSIX Base Specifications Issue 5\\n       (Unix98, 1997), then omitted in Issue 6 (Unix03, 2004) without  express\\n       motivation,  though an introductory comment and optionally adjusts the\\n       margin remains, overlooked in the removal.   The  tabs  utility  docu\\n       mented in Issues 6 and later has no mechanism for setting margins.  The\\n       +m option in ncurses tabs differs from the SVr4 feature by using termi\\n       nal capabilities rather than built-in tables.\\n\\n       POSIX documents no limit on the number of tab stops.  Other implementa\\n       tions  impose  one;  the limit is 20 in PWB/Unix's tabs utility.  While\\n       some terminals may not accept an arbitrary number of tab stops, ncurses\\n       tabs attempts to set tab stops up to  the  right  margin  if  the  list\\n       thereof is sufficiently long.\\n\\n       The  Rationale section of the Issue 6 tabs reference page details how\\n       the committee considered redesigning the tabs and tput utilities, with\\n       out settling on an improved solution.  It claims that\\n\\n           no known historical version of tabs  supports  the  capability  of\\n           setting arbitrary tab stops.\\n\\n       The  feature  described in subsection Explicit Lists above was imple\\n       mented in PWB/Unix, and permitted the setting  of  abitrary  tab  stops\\n       nevertheless.\\n<SECTION>SEE ALSO</SECTION>\\ninfocmp(1M), tset(1), curses(3X), terminfo(5)\\n\\nncurses 6.5                       2024-04-20                           tabs(1)\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ds[\"train\"][2]\n",
    "# tokenized_ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029b817-f0ba-4a35-ad5e-31f418ea4fdd",
   "metadata": {
    "id": "b029b817-f0ba-4a35-ad5e-31f418ea4fdd"
   },
   "source": [
    "# Setting up the model+training parameters\n",
    "Here we are going to setup our LoRA config. LoRA, or LOw-Rank Adaptation, uses advanced linear algebra to reduce the number of parameters we need to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f394f81-e0e8-4d3c-9ac4-4c9da7405ee4",
   "metadata": {
    "id": "4f394f81-e0e8-4d3c-9ac4-4c9da7405ee4"
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce867e72-532d-47e9-af2b-8104fc533159",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce867e72-532d-47e9-af2b-8104fc533159",
    "outputId": "df8756ff-d1e8-4966-c5fd-447720c94863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v', 'q', 'o', 'k', 'wi']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e6fd7b2-f8ba-48cc-85bc-674c98d69750",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e6fd7b2-f8ba-48cc-85bc-674c98d69750",
    "outputId": "e1fb0673-fedd-401d-d1f6-cdbcd320601e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input', 'output']\n"
     ]
    }
   ],
   "source": [
    "print(final_ds[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36ce5bdb-108d-4500-bc2b-bd1fec532316",
   "metadata": {
    "id": "36ce5bdb-108d-4500-bc2b-bd1fec532316"
   },
   "outputs": [],
   "source": [
    "# from numba import cuda\n",
    "# from datasets import load_metric\n",
    "#\n",
    "# label_pad_token_id = -100\n",
    "#\n",
    "# # Load the ROUGE metric\n",
    "# metric = load_metric(\"rouge\")\n",
    "#\n",
    "# # Data collator\n",
    "# data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "#     tokenizer,\n",
    "#     model=model,\n",
    "#     label_pad_token_id=label_pad_token_id,\n",
    "#     pad_to_multiple_of=8,\n",
    "# )\n",
    "#\n",
    "# new_model = \"t5-3b-man-pages\"\n",
    "# # Setting Hyperparamter\n",
    "# training_arguments = transformers.TrainingArguments(\n",
    "#     output_dir=new_model,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     optim=\"paged_adamw_32bit\",\n",
    "#     num_train_epochs=1,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps = 0.25,\n",
    "#     logging_steps=1,\n",
    "#     warmup_steps=10,\n",
    "#     logging_strategy=\"steps\",\n",
    "#     learning_rate=2e-4,\n",
    "#     fp16=True,\n",
    "#     bf16=False,\n",
    "#     group_by_length=True,\n",
    "#     report_to = \"none\",\n",
    "# )\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=tokenized_ds[\"train\"],\n",
    "#     eval_dataset=tokenized_ds[\"test\"],\n",
    "#     peft_config=peft_config,\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_seq_length=512,\n",
    "#     args=training_arguments,\n",
    "#     packing=False,\n",
    "#\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61d6b690-f21d-4aed-9323-df4910c52977",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61d6b690-f21d-4aed-9323-df4910c52977",
    "outputId": "5a4e11c2-ce26-4c59-a3d4-e36039564b45"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3179/1374162215.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "base_model = \"google-t5/t5-large\"\n",
    "new_model = \"t5-large-man-pages\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "# Configuartion of model quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "peft_config = peft.LoraConfig(\n",
    "    task_type=peft.TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=modules\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map = \"cuda:0\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch_dtype,\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "model_name = \"t5-large\"\n",
    "\n",
    "model = peft.get_peft_model(model, peft_config)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps = 0.125,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "label_pad_token_id=-100\n",
    "\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    # compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27420ff0-2a1d-4081-beb4-473c2658c944",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "27420ff0-2a1d-4081-beb4-473c2658c944",
    "outputId": "44d85d77-a105-409c-876b-b849a52fdf03",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28204' max='28204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28204/28204 2:52:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3526</td>\n",
       "      <td>2.957900</td>\n",
       "      <td>2.718952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7052</td>\n",
       "      <td>2.746200</td>\n",
       "      <td>2.541682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10578</td>\n",
       "      <td>2.677100</td>\n",
       "      <td>2.439495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14104</td>\n",
       "      <td>2.623300</td>\n",
       "      <td>2.382997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17630</td>\n",
       "      <td>2.571400</td>\n",
       "      <td>2.335387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21156</td>\n",
       "      <td>2.503100</td>\n",
       "      <td>2.309675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24682</td>\n",
       "      <td>2.456800</td>\n",
       "      <td>2.290718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=28204, training_loss=2.712096088102668, metrics={'train_runtime': 10378.3436, 'train_samples_per_second': 2.718, 'train_steps_per_second': 2.718, 'total_flos': 1.2270516582324634e+17, 'train_loss': 2.712096088102668, 'epoch': 2.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95e6de21-21d8-4ce7-8d24-52e4ec90e96f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "59f60b65f13f4bcb88c477598befc5b1",
      "565601837a54461abd32372cd98ca32c",
      "3a92aa796fe843a78b482d0db8f1b57a",
      "11eb7059317c4dfabc7d4648e2307f8e",
      "bf5c0a88a03c45d9830b72c768c71b1c",
      "dace2be826664e86906a2650aae18d6f",
      "3a5de6f2752f4ed88bf9cffd3a41a83b",
      "960e4129039145ac9882cd7f0f59005d",
      "f6ce6405728c4afeb7d4b8d8285ae671",
      "d487efb02cc145b1a1b0d18e9a641354",
      "433c8f1e498745cc8219e7feca67645e"
     ]
    },
    "id": "95e6de21-21d8-4ce7-8d24-52e4ec90e96f",
    "outputId": "e6100b4e-49ae-42ef-98b8-262b66d3567c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|| 9.48M/9.48M [00:03<00:00, 2.43MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/asdfasda112312/t5-large-man-pages/commit/711442f5cfd9f4a8b3c6f49648d6c903b726d4cf', commit_message='Upload model', commit_description='', oid='711442f5cfd9f4a8b3c6f49648d6c903b726d4cf', pr_url=None, repo_url=RepoUrl('https://huggingface.co/asdfasda112312/t5-large-man-pages', endpoint='https://huggingface.co', repo_type='model', repo_id='asdfasda112312/t5-large-man-pages'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.model.save_pretrained(f\"{model_name}-finetuned-xsum\")\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.model.push_to_hub(new_model, use_temp_dir=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g4iOTKCkJuVI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g4iOTKCkJuVI",
    "outputId": "c5a01f5a-7a05-4f43-b323-3aa5e39fb597"
   },
   "outputs": [],
   "source": [
    "# # prompt: zip t5-large-man-pages and then download\n",
    "# \n",
    "# !zip -r t5-large-man-pages.zip t5-large-man-pages\n",
    "# !ls\n",
    "# from google.colab import files\n",
    "# files.download('t5-large-man-pages.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b08501-f8b6-48ea-b25d-ddcaf295d831",
   "metadata": {
    "collapsed": true,
    "id": "d0b08501-f8b6-48ea-b25d-ddcaf295d831",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "dfa6611b-b945-4d11-ee04-695bc8871172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.0.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.1.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.1.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.1.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.1.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.2.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.2.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.2.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.2.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.3.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.3.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.3.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.3.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.4.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.4.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.4.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.4.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.5.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.5.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.5.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.5.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.6.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.6.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.6.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.6.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.6.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.6.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.7.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.7.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.7.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.7.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.7.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.7.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.8.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.8.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.8.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.8.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.8.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.8.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.9.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.9.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.9.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.9.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.9.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.9.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.10.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.10.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.10.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.10.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.10.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.10.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.11.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.11.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.11.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.11.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.11.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.11.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.12.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.12.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.12.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.12.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.12.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.12.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.12.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.12.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.13.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.13.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.13.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.13.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.13.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.13.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.13.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.13.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.14.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.14.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.14.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.14.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.14.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.14.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.14.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.14.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.15.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.15.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.15.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.15.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.15.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.15.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.15.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.15.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.16.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.16.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.16.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.16.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.16.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.16.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.16.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.16.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.17.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.17.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.17.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.17.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.17.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.17.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.17.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.17.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.18.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.18.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.18.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.18.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.18.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.18.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.18.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.18.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.19.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.19.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.19.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.19.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.19.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.19.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.19.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.19.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.20.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.20.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.20.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.20.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.20.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.20.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.20.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.20.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.21.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.21.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.21.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.21.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.21.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.21.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.21.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.21.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.22.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.22.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.22.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.22.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.22.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.22.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.22.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.22.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.23.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.23.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.23.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.23.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.23.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.23.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.23.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.23.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.final_layer_norm.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.0.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.0.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.1.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.1.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.1.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.2.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.2.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.2.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.3.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.3.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.3.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.4.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.4.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.4.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.5.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.5.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.5.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.6.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.6.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.6.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.6.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.7.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.7.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.7.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.7.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.8.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.8.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.8.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.8.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.9.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.9.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.9.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.9.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.10.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.10.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.10.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.10.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.11.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.11.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.11.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.11.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.12.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.12.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.12.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.12.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.13.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.13.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.13.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.13.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.14.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.14.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.14.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.14.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.15.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.15.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.15.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.15.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.16.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.16.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.16.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.16.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.17.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.17.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.17.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.17.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.18.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.18.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.18.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.18.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.19.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.19.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.19.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.19.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.20.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.20.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.20.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.20.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.21.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.21.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.21.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.21.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.22.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.22.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.22.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.22.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.23.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.23.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.23.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.23.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.final_layer_norm.weight: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HBVZYYihNrWi",
   "metadata": {
    "id": "HBVZYYihNrWi"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f981a4b-ef0d-46c9-8c57-8e5c47a6f8e8",
   "metadata": {
    "id": "2f981a4b-ef0d-46c9-8c57-8e5c47a6f8e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_url = \"google-t5/t5-large\"\n",
    "new_model = \"asdfasda112312/t5-large-man-pages\"\n",
    "\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_url)\n",
    "\n",
    "base_model_reload = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model_url,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084a060-af61-4e87-b9b4-5b5a181ab8f5",
   "metadata": {
    "id": "c084a060-af61-4e87-b9b4-5b5a181ab8f5",
    "outputId": "8a118273-6f2d-43fb-d3c6-c8718bb7dd2e",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target modules {'base_layer'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_reload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/peft_model.py:578\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    571\u001b[0m         model,\n\u001b[1;32m    572\u001b[0m         config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m model\u001b[38;5;241m.\u001b[39mload_adapter(\n\u001b[1;32m    587\u001b[0m     model_id,\n\u001b[1;32m    588\u001b[0m     adapter_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/peft_model.py:1831\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1829\u001b[0m     \u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1830\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1831\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_encoder_decoder_kwargs_for_generation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1834\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39m_prepare_encoder_decoder_kwargs_for_generation\n\u001b[1;32m   1835\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/peft_model.py:171\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    169\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cast_adapter_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/tuners/lora/model.py:141\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name, low_cpu_mem_usage: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:184\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:509\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Handle X-LoRA case.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(peft_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_modules\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# It's important to set the adapter here (again), because otherwise it can happen that if a 2nd adapter is\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# added, and it targets different layer(s) than the first adapter (which is active), then those different\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# layers will be activated, which we don't want.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_adapter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters)\n",
      "\u001b[0;31mValueError\u001b[0m: Target modules {'base_layer'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(base_model_reload, new_model)\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df34574-f332-48f2-9117-78f911a1019e",
   "metadata": {
    "id": "7df34574-f332-48f2-9117-78f911a1019e",
    "outputId": "79de1b33-2d2c-4845-fb58-c81d3b30f578"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|| 1.88G/1.88G [11:31<00:00, 2.72MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/asdfasda112312/t5-3b-man-pages-merged/commit/0a09219b386fac2ad7bcaca839d395383ec6be77', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='0a09219b386fac2ad7bcaca839d395383ec6be77', pr_url=None, repo_url=RepoUrl('https://huggingface.co/asdfasda112312/t5-3b-man-pages-merged', endpoint='https://huggingface.co', repo_type='model', repo_id='asdfasda112312/t5-3b-man-pages-merged'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save_pretrained(\"t5-3b-man-pages-merged\")\n",
    "# model.push_to_hub(\"t5-3b-man-pages-merged\", use_temp_dir=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6096fc6f-e876-4440-9511-d3ed76e5ece2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6096fc6f-e876-4440-9511-d3ed76e5ece2",
    "outputId": "429d0731-afeb-4fbe-a89a-fa6cd6dbbe83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <unk>SECTION>NAME<unk>/SECTION> codeshit - write a shit code <unk>SECTION>SYNOPSIS<unk>/SECTION> shit [-H] <unk>SECTION>DESCRIPTION<unk>/SECTION> shit writes code shit. On command line, shit reports the error code and gives the user instructions to write the code. Use shit to write the code. <unk>SECTION>SEE CUT STAT(1)</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"<SECTION>NAME</SECTION>codeshit - write shit code\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=1024, temperature=0.8, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb753b-1369-4cff-b015-dc607323d37e",
   "metadata": {
    "id": "7feb753b-1369-4cff-b015-dc607323d37e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a2fd6855667473ca87a48d95722f29f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ba03846819f4e3ca89dd28f4c939f70",
      "placeholder": "",
      "style": "IPY_MODEL_e3577a5d7b3d496999e9467ac6c35747",
      "value": "2029/2029[00:31&lt;00:00,61.56examples/s]"
     }
    },
    "11eb7059317c4dfabc7d4648e2307f8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d487efb02cc145b1a1b0d18e9a641354",
      "placeholder": "",
      "style": "IPY_MODEL_433c8f1e498745cc8219e7feca67645e",
      "value": "9.48M/9.48M[00:02&lt;00:00,7.62MB/s]"
     }
    },
    "12eece125e854c99b1b1a75a439fdd6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a929ec96f98240a18133cb3988c9139f",
      "max": 2029,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ee908ba9473404d9fd51bbf07b8ae98",
      "value": 2029
     }
    },
    "2ee908ba9473404d9fd51bbf07b8ae98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "33935c76b57948468b861d657fc9573f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_83a1afb24ea54eb18b30009a5be2f37f",
       "IPY_MODEL_4758f4169f7e44e2818648873f08e049",
       "IPY_MODEL_4e74976f45b441edb956a59c338c5772"
      ],
      "layout": "IPY_MODEL_a20db37b9f7341b8a256e5ce7c4e4658"
     }
    },
    "344eeba5c2cb4251a30ae4d71235cf89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36ec7f05c7f741e0a5a6afb9faf0d5a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a5de6f2752f4ed88bf9cffd3a41a83b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a92aa796fe843a78b482d0db8f1b57a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_960e4129039145ac9882cd7f0f59005d",
      "max": 9477544,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6ce6405728c4afeb7d4b8d8285ae671",
      "value": 9477544
     }
    },
    "3e5efa49cfc4412d9e8c0d9e26145e54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "433c8f1e498745cc8219e7feca67645e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4758f4169f7e44e2818648873f08e049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71980821352641788e0b9c8a90b1abfb",
      "max": 8116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8a57a71ca40a4dbc82b5b290fc68284a",
      "value": 8116
     }
    },
    "4e74976f45b441edb956a59c338c5772": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1afba226a4545f7b28c58f5bf970cd1",
      "placeholder": "",
      "style": "IPY_MODEL_3e5efa49cfc4412d9e8c0d9e26145e54",
      "value": "8116/8116[01:09&lt;00:00,102.35examples/s]"
     }
    },
    "565601837a54461abd32372cd98ca32c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dace2be826664e86906a2650aae18d6f",
      "placeholder": "",
      "style": "IPY_MODEL_3a5de6f2752f4ed88bf9cffd3a41a83b",
      "value": "adapter_model.safetensors:100%"
     }
    },
    "59e3a962819345ed887a3746704de5d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36ec7f05c7f741e0a5a6afb9faf0d5a5",
      "placeholder": "",
      "style": "IPY_MODEL_344eeba5c2cb4251a30ae4d71235cf89",
      "value": "Map:100%"
     }
    },
    "59f60b65f13f4bcb88c477598befc5b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_565601837a54461abd32372cd98ca32c",
       "IPY_MODEL_3a92aa796fe843a78b482d0db8f1b57a",
       "IPY_MODEL_11eb7059317c4dfabc7d4648e2307f8e"
      ],
      "layout": "IPY_MODEL_bf5c0a88a03c45d9830b72c768c71b1c"
     }
    },
    "6ba03846819f4e3ca89dd28f4c939f70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71980821352641788e0b9c8a90b1abfb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73e2e453d9514749a2ce6e555b8857af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_59e3a962819345ed887a3746704de5d7",
       "IPY_MODEL_12eece125e854c99b1b1a75a439fdd6b",
       "IPY_MODEL_0a2fd6855667473ca87a48d95722f29f"
      ],
      "layout": "IPY_MODEL_a59cb746f96e406380b053c9ca2ed057"
     }
    },
    "7c7aceab5c044f0eb395b649bc78df36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83a1afb24ea54eb18b30009a5be2f37f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f63784a5672f45c0bca36d5afc3e4793",
      "placeholder": "",
      "style": "IPY_MODEL_7c7aceab5c044f0eb395b649bc78df36",
      "value": "Map:100%"
     }
    },
    "8a57a71ca40a4dbc82b5b290fc68284a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "960e4129039145ac9882cd7f0f59005d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a20db37b9f7341b8a256e5ce7c4e4658": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a59cb746f96e406380b053c9ca2ed057": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a929ec96f98240a18133cb3988c9139f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf5c0a88a03c45d9830b72c768c71b1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d487efb02cc145b1a1b0d18e9a641354": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dace2be826664e86906a2650aae18d6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3577a5d7b3d496999e9467ac6c35747": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1afba226a4545f7b28c58f5bf970cd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f63784a5672f45c0bca36d5afc3e4793": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6ce6405728c4afeb7d4b8d8285ae671": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
