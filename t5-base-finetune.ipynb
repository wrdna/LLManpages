{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "VIc73r_ubp0v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIc73r_ubp0v",
    "outputId": "100c81e3-f775-4d5b-821c-3ee2674f1663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (0.44.1)\n",
      "Requirement already satisfied: accelerate in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: datasets in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: trl in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (0.12.1)\n",
      "Requirement already satisfied: torch in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from bitsandbytes) (2.5.0)\n",
      "Requirement already satisfied: numpy in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from accelerate) (0.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: rich in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from trl) (13.9.3)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from trl) (4.46.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.46.0->trl) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from transformers>=4.46.0->trl) (0.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes accelerate datasets trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce26ac9-661f-44ae-8e40-3c4840621d2d",
   "metadata": {
    "id": "4ce26ac9-661f-44ae-8e40-3c4840621d2d"
   },
   "outputs": [],
   "source": [
    "# TOKEN\n",
    "\n",
    "\n",
    "TOKEN=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403cb14c-34fa-4c5e-9d2a-26929968f2aa",
   "metadata": {
    "id": "403cb14c-34fa-4c5e-9d2a-26929968f2aa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/miniconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cdc8fe8-fb32-452f-9093-cf67ad47cb78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cdc8fe8-fb32-452f-9093-cf67ad47cb78",
    "outputId": "d83584a2-2383-4e2f-ad9d-f44561878fa3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 23:20:04.064125: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-02 23:20:04.073753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733199604.085717    6871 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733199604.089369    6871 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 23:20:04.101347: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "base_model = \"google-t5/t5-large\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "# Configuartion of model quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map = \"cuda:0\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecebf44-561b-4352-87fa-54e4412161a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ecebf44-561b-4352-87fa-54e4412161a6",
    "outputId": "7bc2815e-112b-4dd3-aab0-5fabf1129a64",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear4bit(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18d7b454-555f-4a17-ab02-d34c9cc3a484",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18d7b454-555f-4a17-ab02-d34c9cc3a484",
    "outputId": "2503b137-799c-4ef2-fe00-b4608371deed",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5TokenizerFast(name_or_path='google-t5/t5-large', vocab_size=32100, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "J0G5EEv4kvG4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0G5EEv4kvG4",
    "outputId": "053f2252-e882-46a6-a9cb-220a88f5761c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><extra_id_0>? how are you? how are you doing?</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"howdy how are we doing\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qtOsyDYVkvG8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qtOsyDYVkvG8",
    "outputId": "3ead23f3-2867-4762-8806-a1a974856f71",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><extra_id_0>. Mediterranean Roasted Chicken. Mediterranean Roasted Chicken. Mediterranean Roasted Chicken. Mediterranean Roasted Chicken.</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Mediterranean Roasted Chicken\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b943957-e9ac-4130-a92c-bbe697b2beb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b943957-e9ac-4130-a92c-bbe697b2beb1",
    "outputId": "d358fe1a-9e97-4a9a-e118-e154c8b07772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><extra_id_0> like exterior like fish</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Exterior like fish \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100, temperature=0.7, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hcCK9vwqdxHp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcCK9vwqdxHp",
    "outputId": "bf4404d5-357a-4110-a5c2-1cf986492ef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><extra_id_0> Chicken<extra_id_1>s: Mediterranean Rubbed Chicken, olive oil, lemon, rosemary; pomegranate seeds; lemon zest; olive oil; roasted spices; Greek Olive Oil; Herbs; Flavors: Mediterranean<extra_id_2> Mediterranean<extra_id_3>: Roasted Mediterranean Chicken Mediterranean Mediterranean Recipe. Roasted Mediterranean Chicken Mediterranean Roasted Chicken<extra_id_4> Mediterranean<extra_id_5>ed<extra_id_6> Mediterranean Chicken<extra_id_7>ed<extra_id_8> Chicken<extra_id_9> Chicken<extra_id_10> Chicken<extra_id_11> chicken<extra_id_12> chicken<extra_id_13> chicken<extra_id_14> chicken<extra_id_15> chicken<extra_id_16> chicken Ingredients:<extra_id_17>:<extra_id_18>:<extra_id_19>:<extra_id_22></s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Mediterranean Roasted Chicken\\n\\nIngredients:\\n\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=200, temperature=0.7, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49104eed-9477-46db-80d1-e81a3139b333",
   "metadata": {
    "id": "49104eed-9477-46db-80d1-e81a3139b333"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import peft\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77929a52-a207-40c8-ad02-7851d905fa81",
   "metadata": {
    "id": "77929a52-a207-40c8-ad02-7851d905fa81"
   },
   "source": [
    "# Datset Grabbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2104272f-3815-44fa-8994-2f873243012e",
   "metadata": {
    "id": "2104272f-3815-44fa-8994-2f873243012e"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"json\", data_files={\"train\": \"masked_man_pages.json\"}, split=\"train\")\n",
    "df = pd.DataFrame(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c937ca74-275b-44cf-9ae9-00d4c5cf5360",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "c937ca74-275b-44cf-9ae9-00d4c5cf5360",
    "outputId": "1cf3593c-1c30-49ae-d792-a793bec0031a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20419</td>\n",
       "      <td>20419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>17633</td>\n",
       "      <td>19219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>&lt;SECTION&gt;REPORTING BUGS&lt;/SECTION&gt;\\nGNU coreuti...</td>\n",
       "      <td>&lt;SECTION&gt;SYNOPSIS&lt;/SECTION&gt;\\nopenssl cmd -help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>102</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    input  \\\n",
       "count                                               20419   \n",
       "unique                                              17633   \n",
       "top     <SECTION>REPORTING BUGS</SECTION>\\nGNU coreuti...   \n",
       "freq                                                  102   \n",
       "\n",
       "                                                   output  \n",
       "count                                               20419  \n",
       "unique                                              19219  \n",
       "top     <SECTION>SYNOPSIS</SECTION>\\nopenssl cmd -help...  \n",
       "freq                                                   51  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79f579aa-d781-4fbc-9cfc-b3312c8ef2e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "79f579aa-d781-4fbc-9cfc-b3312c8ef2e4",
    "outputId": "42606bb2-0506-4d4e-e9d1-bcc310ef22cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19224</td>\n",
       "      <td>19224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>17633</td>\n",
       "      <td>19219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>&lt;SECTION&gt;REPORTING BUGS&lt;/SECTION&gt;\\nGNU coreuti...</td>\n",
       "      <td>&lt;SECTION&gt;NAME&lt;/SECTION&gt;\\nlwp-request - Simple ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>102</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    input  \\\n",
       "count                                               19224   \n",
       "unique                                              17633   \n",
       "top     <SECTION>REPORTING BUGS</SECTION>\\nGNU coreuti...   \n",
       "freq                                                  102   \n",
       "\n",
       "                                                   output  \n",
       "count                                               19224  \n",
       "unique                                              19219  \n",
       "top     <SECTION>NAME</SECTION>\\nlwp-request - Simple ...  \n",
       "freq                                                    4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85c14f2d-2007-416c-8ff9-c13939f930c5",
   "metadata": {
    "id": "85c14f2d-2007-416c-8ff9-c13939f930c5"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "df = df.drop_duplicates(subset=[\"input\"])\n",
    "df = df.drop_duplicates(subset=[\"output\"])\n",
    "\n",
    "df.describe()\n",
    "seed = 42  # Set your desired seed\n",
    "# df = df.sample(n=2000, random_state=seed)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5139340a-4675-4f4b-b0cf-6dcceb9c930a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5139340a-4675-4f4b-b0cf-6dcceb9c930a",
    "outputId": "e2e92346-be12-4893-ccb8-3ac3332ae37c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17628 entries, 0 to 17627\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   17628 non-null  object\n",
      " 1   output  17628 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 275.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "wAi9EBxqsBDZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "wAi9EBxqsBDZ",
    "outputId": "32437d23-bbb4-4caf-91cc-106db6dd3cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 1.0\n",
      "Mean: 1\n",
      "STD: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABUk0lEQVR4nO3deVwVVeM/8M8FZJVFYncB3FAURDEJUyG9CmimWYlmsTxqZfmkoVZY7j2h5lr5SOWCbaKWod9UXFBcUXPBXRMCcQFcAUEFhfP7wx/zOLII1wsXnM/79ZpXzZkzZ86ZEfk4c+ZelRBCgIiIiEhB9HTdASIiIqLaxgBEREREisMARERERIrDAERERESKwwBEREREisMARERERIrDAERERESKwwBEREREisMARERERIrDAERUw6ZOnQqVSlUrx/L394e/v7+0npiYCJVKhd9++61Wjh8WFgYXF5daOZam8vPzMWLECDg4OEClUmHs2LE1chyVSoXRo0fXSNv1XVhYGBo2bKjrbpDCMQARVUNMTAxUKpW0GBsbw8nJCQEBAfj6669x+/ZtrRznypUrmDp1KpKTk7XSnjbV5b5VxZdffomYmBiMGjUKP/30E95+++0K67q4uODll1+uxd5VXW2H2+q6c+cOpk6disTERF13hahcBrruAFF9NH36dLi6uuL+/fvIyspCYmIixo4di3nz5mH9+vXw9PSU6n7++ef49NNPq9X+lStXMG3aNLi4uMDLy6vK+23ZsqVax9FEZX374YcfUFJSUuN9eBrbt2/HCy+8gClTpui6K8+0O3fuYNq0aQAguytJVFcwABFpICgoCJ07d5bWIyMjsX37drz88st45ZVXcObMGZiYmAAADAwMYGBQsz9qd+7cgampKQwNDWv0OE/SoEEDnR6/Kq5evQp3d3ddd4OIdIyPwIi0pGfPnpg0aRIuXLiAn3/+WSovbw7Q1q1b0a1bN1hZWaFhw4Zwc3PDxIkTATx8tPH8888DAMLDw6XHbTExMQAe/mu6ffv2OHz4MHr06AFTU1Np38fnAJUqLi7GxIkT4eDgADMzM7zyyiu4ePGirI6LiwvCwsLK7Ptom0/qW3lzgAoKCjBu3Dg0bdoURkZGcHNzw5w5cyCEkNUrnTMTFxeH9u3bw8jICO3atUN8fHz5J/wxV69exfDhw2Fvbw9jY2N06NABK1askLaXPjJKS0vDhg0bpL6np6dXqf3qjqc8X3zxBfT09PDNN99IZZs2bUL37t1hZmYGc3Nz9OvXD6dOnapWnyqTk5ODsWPHSv1t2bIlZs2aJbtTl56eDpVKhTlz5uD7779HixYtYGRkhOeffx5//fVXmTbXrFkDd3d3GBsbo3379vjjjz9k1z49PR22trYAgGnTpknneurUqbJ2Ll++jIEDB6Jhw4awtbXF+PHjUVxcLKsTGxsLb29vmJubw8LCAh4eHli4cKHWzg8pF+8AEWnR22+/jYkTJ2LLli0YOXJkuXVOnTqFl19+GZ6enpg+fTqMjIyQkpKCvXv3AgDatm2L6dOnY/LkyXjnnXfQvXt3AEDXrl2lNm7cuIGgoCAMGTIEb731Fuzt7Svt13/+8x+oVCp88sknuHr1KhYsWAC1Wo3k5GTpTlVVVKVvjxJC4JVXXsGOHTswfPhweHl5YfPmzZgwYQIuX76M+fPny+rv2bMHa9euxfvvvw9zc3N8/fXXeO2115CRkYHnnnuuwn7dvXsX/v7+SElJwejRo+Hq6oo1a9YgLCwMOTk5GDNmDNq2bYuffvoJH330EZo0aYJx48YBgPSLuiqqO55Hff755/jyyy/x3XffSX82fvrpJ4SGhiIgIACzZs3CnTt3sHjxYnTr1g1Hjx596gnld+7cgZ+fHy5fvox3330XzZo1w759+xAZGYnMzEwsWLBAVv/XX3/F7du38e6770KlUmH27NkYNGgQ/vnnH+nu3oYNGxAcHAwPDw9ERUXh1q1bGD58OBo3biy1Y2tri8WLF2PUqFF49dVXMWjQIACQPRouLi5GQEAAfHx8MGfOHGzbtg1z585FixYtMGrUKAAP/6EwdOhQ9OrVC7NmzQIAnDlzBnv37sWYMWOe6twQQRBRlS1fvlwAEH/99VeFdSwtLUXHjh2l9SlTpohHf9Tmz58vAIhr165V2MZff/0lAIjly5eX2ebn5ycAiOjo6HK3+fn5Ses7duwQAETjxo1FXl6eVL569WoBQCxcuFAqc3Z2FqGhoU9ss7K+hYaGCmdnZ2k9Li5OABBffPGFrN7rr78uVCqVSElJkcoACENDQ1nZsWPHBADxzTfflDnWoxYsWCAAiJ9//lkqKyoqEr6+vqJhw4aysTs7O4t+/fpV2l5Fdas7ng8++EAIIcS4ceOEnp6eiImJkbbfvn1bWFlZiZEjR8raysrKEpaWlmXKH1d6bdesWVNhnRkzZggzMzPx999/y8o//fRToa+vLzIyMoQQQqSlpQkA4rnnnhM3b96U6q1bt04AEP/3f/8nlXl4eIgmTZqI27dvS2WJiYkCgOzaX7t2TQAQU6ZMKdOv0NBQAUBMnz5dVt6xY0fh7e0trY8ZM0ZYWFiIBw8eVHouiDTBR2BEWtawYcNK3wazsrICAKxbt07jCcNGRkYIDw+vcv2QkBCYm5tL66+//jocHR2xceNGjY5fVRs3boS+vj4+/PBDWfm4ceMghMCmTZtk5Wq1Gi1atJDWPT09YWFhgX/++eeJx3FwcMDQoUOlsgYNGuDDDz9Efn4+du7cqYXRVH88QgiMHj0aCxcuxM8//4zQ0FBp29atW5GTk4OhQ4fi+vXr0qKvrw8fHx/s2LHjqfu7Zs0adO/eHY0aNZIdQ61Wo7i4GLt27ZLVDw4ORqNGjaT10jt8pef/ypUrOHHiBEJCQmSvsfv5+cHDw6Pa/Xvvvfdk6927d5ddaysrKxQUFGDr1q3VbpvoSfgIjEjL8vPzYWdnV+H24OBgLFmyBCNGjMCnn36KXr16YdCgQXj99dehp1e1f5M0bty4WhOeW7VqJVtXqVRo2bJltee/VNeFCxfg5OQkC1/Aw0dppdsf1axZszJtNGrUCLdu3XricVq1alXm/FV0HE1Vdzw//vgj8vPzsXjxYlk4A4Dz588DeDh3rDwWFhZP3d/z58/j+PHjFT7mu3r1qmz98fNfGoZKz3/p+Fq2bFmmrZYtW+LIkSNV7puxsXGZfj1+rd9//32sXr0aQUFBaNy4Mfr06YPBgwcjMDCwyschqggDEJEWXbp0Cbm5ueX+gihlYmKCXbt2YceOHdiwYQPi4+OxatUq9OzZE1u2bIG+vv4Tj1OdeTtVVdGHNRYXF1epT9pQ0XFEFSYY10UvvvgikpOT8e2332Lw4MGwtraWtpXe/fvpp5/g4OBQZl9tvDlYUlKC3r174+OPPy53e+vWrWXrtXn+q/Jnys7ODsnJydi8eTM2bdqETZs2Yfny5QgJCZFNcCfSBAMQkRb99NNPAICAgIBK6+np6aFXr17o1asX5s2bhy+//BKfffYZduzYAbVarfVPji6921BKCIGUlBTZpNRGjRohJyenzL4XLlxA8+bNpfXq9M3Z2Rnbtm3D7du3ZXdNzp49K23XBmdnZxw/fhwlJSWyu0A1cZzqjKdly5aYPXs2/P39ERgYiISEBGm/0kd9dnZ2UKvVWunf41q0aIH8/HyttV86vpSUlDLbHi/T1p9hQ0ND9O/fH/3790dJSQnef/99fPfdd5g0aVKl/9AgehLOASLSku3bt2PGjBlwdXXFsGHDKqx38+bNMmWlHyhYWFgIADAzMwOAcgOJJn788UfZvKTffvsNmZmZCAoKkspatGiB/fv3o6ioSCr7888/y7wuX52+9e3bF8XFxfj2229l5fPnz4dKpZId/2n07dsXWVlZWLVqlVT24MEDfPPNN2jYsCH8/Py0dpzqjsfT0xMbN27EmTNn0L9/f9y9exfAw5BsYWGBL7/8Evfv3y+z37Vr1566v4MHD0ZSUhI2b95cZltOTg4ePHhQrfacnJzQvn176dFeqZ07d+LEiROyuqamptJxNHXjxg3Zup6enhTaS39WiDTFO0BEGti0aRPOnj2LBw8eIDs7G9u3b8fWrVvh7OyM9evXw9jYuMJ9p0+fjl27dqFfv35wdnbG1atX8d///hdNmjRBt27dADwMI1ZWVoiOjoa5uTnMzMzg4+MDV1dXjfprbW2Nbt26ITw8HNnZ2ViwYAFatmwpe1V/xIgR+O233xAYGIjBgwcjNTUVP//8s2xScnX71r9/f7z00kv47LPPkJ6ejg4dOmDLli1Yt24dxo4dW6ZtTb3zzjv47rvvEBYWhsOHD8PFxQW//fYb9u7diwULFpSZs6MpTcfzwgsvYN26dejbty9ef/11xMXFwcLCAosXL8bbb7+NTp06YciQIbC1tUVGRgY2bNiAF198sUzQKs/vv/8u3YF6VGhoKCZMmID169fj5ZdfRlhYGLy9vVFQUIATJ07gt99+Q3p6OmxsbKp1Dr788ksMGDAAL774IsLDw3Hr1i18++23aN++vSwUmZiYwN3dHatWrULr1q1hbW2N9u3bo3379lU+1ogRI3Dz5k307NkTTZo0wYULF/DNN9/Ay8tLmndFpDFdvoJGVN+UvgZfuhgaGgoHBwfRu3dvsXDhQtnr1qUefw0+ISFBDBgwQDg5OQlDQ0Ph5OQkhg4dWuZV5XXr1gl3d3dhYGAge+3cz89PtGvXrtz+VfQa/MqVK0VkZKSws7MTJiYmol+/fuLChQtl9p87d65o3LixMDIyEi+++KI4dOhQmTYr69vjr8EL8fB1748++kg4OTmJBg0aiFatWomvvvpKlJSUyOrhkdfGH1XR6/mPy87OFuHh4cLGxkYYGhoKDw+Pcl/Vr85r8M2aNROvvPKK1sazbt06YWBgIIKDg0VxcbEQ4uE1CggIEJaWlsLY2Fi0aNFChIWFiUOHDlXat9JrW9Gye/duqb+RkZGiZcuWwtDQUNjY2IiuXbuKOXPmiKKiIiHE/16D/+qrr8ocB+W8yh4bGyvatGkjjIyMRPv27cX69evFa6+9Jtq0aSOrt2/fPuHt7S0MDQ1l7YSGhgozM7Myx3r8Z+W3334Tffr0EXZ2dsLQ0FA0a9ZMvPvuuyIzM7PSc0NUFSoh6unsQiKiGmZtbY1+/fpJc7uoYl5eXrC1teUr61RvcA4QEVE5UlNTcevWLX5v2GPu379fZu5QYmIijh07xi89pXqFd4CIiB7xzz//YOPGjVi8eDFSUlJw9uxZjedePYvS09OhVqvx1ltvwcnJCWfPnkV0dDQsLS1x8uTJSr+yhKgu4SRoIqJH7Nq1CxEREWjXrh3WrVvH8POYRo0awdvbG0uWLMG1a9dgZmaGfv36YebMmQw/VK/wDhAREREpDucAERERkeIwABEREZHicA5QOUpKSnDlyhWYm5tr/SsJiIiIqGYIIXD79m04OTk98culGYDKceXKFTRt2lTX3SAiIiINXLx4EU2aNKm0DgNQOUo/Nv/ixYuwsLDQcW+IiIioKvLy8tC0adMqff0NA1A5Sh97WVhYMAARERHVM1WZvsJJ0ERERKQ4DEBERESkOAxAREREpDgMQERERKQ4DEBERESkOAxAREREpDgMQERERKQ4DEBERESkOAxAREREpDgMQERERKQ4Og1AUVFReP7552Fubg47OzsMHDgQ586de+J+a9asQZs2bWBsbAwPDw9s3LhRtl0IgcmTJ8PR0REmJiZQq9U4f/58TQ2DiIiI6hmdBqCdO3figw8+wP79+7F161bcv38fffr0QUFBQYX77Nu3D0OHDsXw4cNx9OhRDBw4EAMHDsTJkyelOrNnz8bXX3+N6OhoHDhwAGZmZggICMC9e/dqY1hERERUx6mEEELXnSh17do12NnZYefOnejRo0e5dYKDg1FQUIA///xTKnvhhRfg5eWF6OhoCCHg5OSEcePGYfz48QCA3Nxc2NvbIyYmBkOGDHliP/Ly8mBpaYnc3Fx+GSoREVE9UZ3f33VqDlBubi4AwNrausI6SUlJUKvVsrKAgAAkJSUBANLS0pCVlSWrY2lpCR8fH6kOERERKZuBrjtQqqSkBGPHjsWLL76I9u3bV1gvKysL9vb2sjJ7e3tkZWVJ20vLKqrzuMLCQhQWFkrreXl5Go2BiOqHjIwMXL9+XdfdIFIsGxsbNGvWTKd9qDMB6IMPPsDJkyexZ8+eWj92VFQUpk2bVuvHJaLal5GRAbc2bXHv7h1dd4VIsYxNTHHu7BmdhqA6EYBGjx6NP//8E7t27UKTJk0qrevg4IDs7GxZWXZ2NhwcHKTtpWWOjo6yOl5eXuW2GRkZiYiICGk9Ly8PTZs21WQoRFTHXb9+Hffu3sFzL49Dg+f4c05U2+7fuIgbf87F9evXlRuAhBD497//jT/++AOJiYlwdXV94j6+vr5ISEjA2LFjpbKtW7fC19cXAODq6goHBwckJCRIgScvLw8HDhzAqFGjym3TyMgIRkZGTz0eIqo/GjzXFEYOLXXdDSLSEZ0GoA8++AC//vor1q1bB3Nzc2mOjqWlJUxMTAAAISEhaNy4MaKiogAAY8aMgZ+fH+bOnYt+/fohNjYWhw4dwvfffw8AUKlUGDt2LL744gu0atUKrq6umDRpEpycnDBw4ECdjJOIiIjqFp0GoMWLFwMA/P39ZeXLly9HWFgYgIfP6/X0/veyWteuXfHrr7/i888/x8SJE9GqVSvExcXJJk5//PHHKCgowDvvvIOcnBx069YN8fHxMDY2rvExERERUd2n80dgT5KYmFim7I033sAbb7xR4T4qlQrTp0/H9OnTn6Z7RERE9IyqU58DRERERFQbGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHF0GoB27dqF/v37w8nJCSqVCnFxcZXWDwsLg0qlKrO0a9dOqjN16tQy29u0aVPDIyEiIqL6RKcBqKCgAB06dMCiRYuqVH/hwoXIzMyUlosXL8La2hpvvPGGrF67du1k9fbs2VMT3SciIqJ6ykCXBw8KCkJQUFCV61taWsLS0lJaj4uLw61btxAeHi6rZ2BgAAcHB631k4iIiJ4t9XoO0NKlS6FWq+Hs7CwrP3/+PJycnNC8eXMMGzYMGRkZOuohERER1UU6vQP0NK5cuYJNmzbh119/lZX7+PggJiYGbm5uyMzMxLRp09C9e3ecPHkS5ubm5bZVWFiIwsJCaT0vL69G+05ERES6VW8D0IoVK2BlZYWBAwfKyh99pObp6QkfHx84Oztj9erVGD58eLltRUVFYdq0aTXZXSIiIqpD6uUjMCEEli1bhrfffhuGhoaV1rWyskLr1q2RkpJSYZ3IyEjk5uZKy8WLF7XdZSIiIqpD6mUA2rlzJ1JSUiq8o/Oo/Px8pKamwtHRscI6RkZGsLCwkC1ERET07NJpAMrPz0dycjKSk5MBAGlpaUhOTpYmLUdGRiIkJKTMfkuXLoWPjw/at29fZtv48eOxc+dOpKenY9++fXj11Vehr6+PoUOH1uhYiIiIqP7Q6RygQ4cO4aWXXpLWIyIiAAChoaGIiYlBZmZmmTe4cnNz8fvvv2PhwoXltnnp0iUMHToUN27cgK2tLbp164b9+/fD1ta25gZCRERE9YpOA5C/vz+EEBVuj4mJKVNmaWmJO3fuVLhPbGysNrpGREREz7B6OQeIiIiI6GkwABEREZHiMAARERGR4jAAERERkeIwABEREZHiMAARERGR4jAAERERkeIwABEREZHiMAARERGR4jAAERERkeIwABEREZHiMAARERGR4jAAERERkeIwABEREZHiMAARERGR4jAAERERkeIwABEREZHiMAARERGR4jAAERERkeIwABEREZHiMAARERGR4jAAERERkeIwABEREZHiMAARERGR4jAAERERkeIwABEREZHiMAARERGR4jAAERERkeIwABEREZHiMAARERGR4jAAERERkeIwABEREZHiMAARERGR4ug0AO3atQv9+/eHk5MTVCoV4uLiKq2fmJgIlUpVZsnKypLVW7RoEVxcXGBsbAwfHx8cPHiwBkdBRERE9Y1OA1BBQQE6dOiARYsWVWu/c+fOITMzU1rs7OykbatWrUJERASmTJmCI0eOoEOHDggICMDVq1e13X0iIiKqpwx0efCgoCAEBQVVez87OztYWVmVu23evHkYOXIkwsPDAQDR0dHYsGEDli1bhk8//fRpuktERETPiHo5B8jLywuOjo7o3bs39u7dK5UXFRXh8OHDUKvVUpmenh7UajWSkpJ00VUiIiKqg+pVAHJ0dER0dDR+//13/P7772jatCn8/f1x5MgRAMD169dRXFwMe3t72X729vZl5gk9qrCwEHl5ebKFiIiInl06fQRWXW5ubnBzc5PWu3btitTUVMyfPx8//fSTxu1GRUVh2rRp2ugiERER1QP16g5Qebp06YKUlBQAgI2NDfT19ZGdnS2rk52dDQcHhwrbiIyMRG5urrRcvHixRvtMREREulXvA1BycjIcHR0BAIaGhvD29kZCQoK0vaSkBAkJCfD19a2wDSMjI1hYWMgWIiIienbp9BFYfn6+dPcGANLS0pCcnAxra2s0a9YMkZGRuHz5Mn788UcAwIIFC+Dq6op27drh3r17WLJkCbZv344tW7ZIbURERCA0NBSdO3dGly5dsGDBAhQUFEhvhRERERHpNAAdOnQIL730krQeEREBAAgNDUVMTAwyMzORkZEhbS8qKsK4ceNw+fJlmJqawtPTE9u2bZO1ERwcjGvXrmHy5MnIysqCl5cX4uPjy0yMJiIiIuVSCSGErjtR1+Tl5cHS0hK5ubl8HEb0jDly5Ai8vb3hELoARg4tdd0dIsUpzEpB1oqxOHz4MDp16qTVtqvz+7vezwEiIiIiqi4GICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHJ0GoF27dqF///5wcnKCSqVCXFxcpfXXrl2L3r17w9bWFhYWFvD19cXmzZtldaZOnQqVSiVb2rRpU4OjICIiovpGpwGooKAAHTp0wKJFi6pUf9euXejduzc2btyIw4cP46WXXkL//v1x9OhRWb127dohMzNTWvbs2VMT3SciIqJ6ykCXBw8KCkJQUFCV6y9YsEC2/uWXX2LdunX4v//7P3Ts2FEqNzAwgIODg7a6SURERM+Yej0HqKSkBLdv34a1tbWs/Pz583ByckLz5s0xbNgwZGRk6KiHREREVBfp9A7Q05ozZw7y8/MxePBgqczHxwcxMTFwc3NDZmYmpk2bhu7du+PkyZMwNzcvt53CwkIUFhZK63l5eTXedyIiItKdehuAfv31V0ybNg3r1q2DnZ2dVP7oIzVPT0/4+PjA2dkZq1evxvDhw8ttKyoqCtOmTavxPhMREVHdUC8fgcXGxmLEiBFYvXo11Gp1pXWtrKzQunVrpKSkVFgnMjISubm50nLx4kVtd5mIiIjqEI0C0D///KPtflTZypUrER4ejpUrV6Jfv35PrJ+fn4/U1FQ4OjpWWMfIyAgWFhayhYiIiJ5dGgWgli1b4qWXXsLPP/+Me/fuaXzw/Px8JCcnIzk5GQCQlpaG5ORkadJyZGQkQkJCpPq//vorQkJCMHfuXPj4+CArKwtZWVnIzc2V6owfPx47d+5Eeno69u3bh1dffRX6+voYOnSoxv0kIiKiZ4tGAejIkSPw9PREREQEHBwc8O677+LgwYPVbufQoUPo2LGj9Ap7REQEOnbsiMmTJwMAMjMzZW9wff/993jw4AE++OADODo6SsuYMWOkOpcuXcLQoUPh5uaGwYMH47nnnsP+/ftha2uryVCJiIjoGaQSQghNd37w4AHWr1+PmJgYxMfHo3Xr1vjXv/6Ft99+u14Hjry8PFhaWiI3N5ePw4ieMUeOHIG3tzccQhfAyKGlrrtDpDiFWSnIWjEWhw8fRqdOnbTadnV+fz/VJGgDAwMMGjQIa9aswaxZs5CSkoLx48ejadOmCAkJQWZm5tM0T0RERFQjnioAHTp0CO+//z4cHR0xb948jB8/Hqmpqdi6dSuuXLmCAQMGaKufRERERFqj0ecAzZs3D8uXL8e5c+fQt29f/Pjjj+jbty/09B7mKVdXV8TExMDFxUWbfSUiIiLSCo0C0OLFi/Gvf/0LYWFhFb5ebmdnh6VLlz5V54iIiIhqgkYB6Pz580+sY2hoiNDQUE2aJyIiIqpRGs0BWr58OdasWVOmfM2aNVixYsVTd4qIiIioJmkUgKKiomBjY1Om3M7ODl9++eVTd4qIiIioJmkUgDIyMuDq6lqm3NnZWfbBhURERER1kUYByM7ODsePHy9TfuzYMTz33HNP3SkiIiKimqRRABo6dCg+/PBD7NixA8XFxSguLsb27dsxZswYDBkyRNt9JCIiItIqjd4CmzFjBtLT09GrVy8YGDxsoqSkBCEhIZwDRERERHWeRgHI0NAQq1atwowZM3Ds2DGYmJjAw8MDzs7O2u4fERERkdZpFIBKtW7dGq1bt9ZWX4iIiIhqhUYBqLi4GDExMUhISMDVq1dRUlIi2759+3atdI6IiIioJmgUgMaMGYOYmBj069cP7du3h0ql0na/iIiIiGqMRgEoNjYWq1evRt++fbXdHyIiIqIap9Fr8IaGhmjZsqW2+0JERERUKzQKQOPGjcPChQshhNB2f4iIiIhqnEaPwPbs2YMdO3Zg06ZNaNeuHRo0aCDbvnbtWq10joiIiKgmaBSArKys8Oqrr2q7L0RERES1QqMAtHz5cm33g4iIiKjWaDQHCAAePHiAbdu24bvvvsPt27cBAFeuXEF+fr7WOkdERERUEzS6A3ThwgUEBgYiIyMDhYWF6N27N8zNzTFr1iwUFhYiOjpa2/0kIiIi0hqN7gCNGTMGnTt3xq1bt2BiYiKVv/rqq0hISNBa54iIiIhqgkZ3gHbv3o19+/bB0NBQVu7i4oLLly9rpWNERERENUWjO0AlJSUoLi4uU37p0iWYm5s/daeIiIiIapJGAahPnz5YsGCBtK5SqZCfn48pU6bw6zGIiIioztPoEdjcuXMREBAAd3d33Lt3D2+++SbOnz8PGxsbrFy5Utt9JCIiItIqjQJQkyZNcOzYMcTGxuL48ePIz8/H8OHDMWzYMNmkaCIiIqK6SKMABAAGBgZ46623tNkXIiIiolqhUQD68ccfK90eEhKiUWeIiIiIaoNGAWjMmDGy9fv37+POnTswNDSEqakpAxARERHVaRq9BXbr1i3Zkp+fj3PnzqFbt26cBE1ERER1nsbfBfa4Vq1aYebMmWXuDlVm165d6N+/P5ycnKBSqRAXF/fEfRITE9GpUycYGRmhZcuWiImJKVNn0aJFcHFxgbGxMXx8fHDw4MFqjISIiIiedVoLQMDDidFXrlypcv2CggJ06NABixYtqlL9tLQ09OvXDy+99BKSk5MxduxYjBgxAps3b5bqrFq1ChEREZgyZQqOHDmCDh06ICAgAFevXq32eIiIiOjZpNEcoPXr18vWhRDIzMzEt99+ixdffLHK7QQFBSEoKKjK9aOjo+Hq6oq5c+cCANq2bYs9e/Zg/vz5CAgIAADMmzcPI0eORHh4uLTPhg0bsGzZMnz66adVPhYRERE9uzQKQAMHDpStq1Qq2NraomfPnlI4qQlJSUlQq9WysoCAAIwdOxYAUFRUhMOHDyMyMlLarqenB7VajaSkpBrrFxEREdUvGgWgkpISbfejSrKysmBvby8rs7e3R15eHu7evYtbt26huLi43Dpnz56tsN3CwkIUFhZK63l5edrtOBEREdUpWp0DVF9FRUXB0tJSWpo2barrLhEREVEN0ugOUERERJXrzps3T5NDlMvBwQHZ2dmysuzsbFhYWMDExAT6+vrQ19cvt46Dg0OF7UZGRsrGlJeXxxBERET0DNMoAB09ehRHjx7F/fv34ebmBgD4+++/oa+vj06dOkn1VCqVdnr5//n6+mLjxo2ysq1bt8LX1xcAYGhoCG9vbyQkJEjzlEpKSpCQkIDRo0dX2K6RkRGMjIy02lciIiKquzQKQP3794e5uTlWrFiBRo0aAXj44Yjh4eHo3r07xo0bV6V28vPzkZKSIq2npaUhOTkZ1tbWaNasGSIjI3H58mXpqzfee+89fPvtt/j444/xr3/9C9u3b8fq1auxYcMGqY2IiAiEhoaic+fO6NKlCxYsWICCggLprTAiIiIijQLQ3LlzsWXLFin8AECjRo3wxRdfoE+fPlUOQIcOHcJLL70krZc+hgoNDUVMTAwyMzORkZEhbXd1dcWGDRvw0UcfYeHChWjSpAmWLFkivQIPAMHBwbh27RomT56MrKwseHl5IT4+vszEaCIiIlIujQJQXl4erl27Vqb82rVruH37dpXb8ff3hxCiwu3lfcqzv78/jh49Wmm7o0ePrvSRFxERESmbRm+BvfrqqwgPD8fatWtx6dIlXLp0Cb///juGDx+OQYMGabuPRERERFql0R2g6OhojB8/Hm+++Sbu37//sCEDAwwfPhxfffWVVjtIREREpG0aBSBTU1P897//xVdffYXU1FQAQIsWLWBmZqbVzhERERHVhKf6IMTMzExkZmaiVatWMDMzq3Q+DxEREVFdoVEAunHjBnr16oXWrVujb9++yMzMBAAMHz68ym+AEREREemKRgHoo48+QoMGDZCRkQFTU1OpPDg4GPHx8VrrHBEREVFN0GgO0JYtW7B582Y0adJEVt6qVStcuHBBKx0jIiIiqika3QEqKCiQ3fkpdfPmTX6lBBEREdV5GgWg7t27S19PATz8zq+SkhLMnj1b9snORERERHWRRo/AZs+ejV69euHQoUMoKirCxx9/jFOnTuHmzZvYu3evtvtIREREpFUa3QFq3749/v77b3Tr1g0DBgxAQUEBBg0ahKNHj6JFixba7iMRERGRVlX7DtD9+/cRGBiI6OhofPbZZzXRJyIiIqIaVe07QA0aNMDx48droi9EREREtUKjR2BvvfUWli5dqu2+EBEREdUKjSZBP3jwAMuWLcO2bdvg7e1d5jvA5s2bp5XOEREREdWEagWgf/75By4uLjh58iQ6deoEAPj7779ldVQqlfZ6R0RERFQDqhWAWrVqhczMTOzYsQPAw6+++Prrr2Fvb18jnSMiIiKqCdWaA/T4t71v2rQJBQUFWu0QERERUU3TaBJ0qccDEREREVF9UK0ApFKpyszx4ZwfIiIiqm+qNQdICIGwsDDpC0/v3buH9957r8xbYGvXrtVeD4mIiIi0rFoBKDQ0VLb+1ltvabUzRERERLWhWgFo+fLlNdUPIiIiolrzVJOgiYiIiOojBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlKcOhGAFi1aBBcXFxgbG8PHxwcHDx6ssK6/v7/0rfSPLv369ZPqhIWFldkeGBhYG0MhIiKieqBa3wVWE1atWoWIiAhER0fDx8cHCxYsQEBAAM6dOwc7O7sy9deuXYuioiJp/caNG+jQoQPeeOMNWb3AwEDZd5eVfoM9ERERkc7vAM2bNw8jR45EeHg43N3dER0dDVNTUyxbtqzc+tbW1nBwcJCWrVu3wtTUtEwAMjIyktVr1KhRbQyHiIiI6gGdBqCioiIcPnwYarVaKtPT04NarUZSUlKV2li6dCmGDBkCMzMzWXliYiLs7Ozg5uaGUaNG4caNG1rtOxEREdVfOn0Edv36dRQXF8Pe3l5Wbm9vj7Nnzz5x/4MHD+LkyZNYunSprDwwMBCDBg2Cq6srUlNTMXHiRAQFBSEpKQn6+vpl2iksLERhYaG0npeXp+GIiIiIqD7Q+Rygp7F06VJ4eHigS5cusvIhQ4ZI/+/h4QFPT0+0aNECiYmJ6NWrV5l2oqKiMG3atBrvLxEREdUNOn0EZmNjA319fWRnZ8vKs7Oz4eDgUOm+BQUFiI2NxfDhw594nObNm8PGxgYpKSnlbo+MjERubq60XLx4seqDICIionpHpwHI0NAQ3t7eSEhIkMpKSkqQkJAAX1/fSvdds2YNCgsL8dZbbz3xOJcuXcKNGzfg6OhY7nYjIyNYWFjIFiIiInp26fwtsIiICPzwww9YsWIFzpw5g1GjRqGgoADh4eEAgJCQEERGRpbZb+nSpRg4cCCee+45WXl+fj4mTJiA/fv3Iz09HQkJCRgwYABatmyJgICAWhkTERER1W06nwMUHByMa9euYfLkycjKyoKXlxfi4+OlidEZGRnQ05PntHPnzmHPnj3YsmVLmfb09fVx/PhxrFixAjk5OXByckKfPn0wY8YMfhYQERERAagDAQgARo8ejdGjR5e7LTExsUyZm5sbhBDl1jcxMcHmzZu12T0iIiJ6xuj8ERgRERFRbWMAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFqRMBaNGiRXBxcYGxsTF8fHxw8ODBCuvGxMRApVLJFmNjY1kdIQQmT54MR0dHmJiYQK1W4/z58zU9DCIiIqondB6AVq1ahYiICEyZMgVHjhxBhw4dEBAQgKtXr1a4j4WFBTIzM6XlwoULsu2zZ8/G119/jejoaBw4cABmZmYICAjAvXv3ano4REREVA/oPADNmzcPI0eORHh4ONzd3REdHQ1TU1MsW7aswn1UKhUcHBykxd7eXtomhMCCBQvw+eefY8CAAfD09MSPP/6IK1euIC4urhZGRERERHWdTgNQUVERDh8+DLVaLZXp6elBrVYjKSmpwv3y8/Ph7OyMpk2bYsCAATh16pS0LS0tDVlZWbI2LS0t4ePjU2mbREREpBw6DUDXr19HcXGx7A4OANjb2yMrK6vcfdzc3LBs2TKsW7cOP//8M0pKStC1a1dcunQJAKT9qtNmYWEh8vLyZAsRERE9u3T+CKy6fH19ERISAi8vL/j5+WHt2rWwtbXFd999p3GbUVFRsLS0lJamTZtqscdERERU1+g0ANnY2EBfXx/Z2dmy8uzsbDg4OFSpjQYNGqBjx45ISUkBAGm/6rQZGRmJ3Nxcabl48WJ1h0JERET1iE4DkKGhIby9vZGQkCCVlZSUICEhAb6+vlVqo7i4GCdOnICjoyMAwNXVFQ4ODrI28/LycODAgQrbNDIygoWFhWwhIiKiZ5eBrjsQERGB0NBQdO7cGV26dMGCBQtQUFCA8PBwAEBISAgaN26MqKgoAMD06dPxwgsvoGXLlsjJycFXX32FCxcuYMSIEQAeviE2duxYfPHFF2jVqhVcXV0xadIkODk5YeDAgboaJhEREdUhOg9AwcHBuHbtGiZPnoysrCx4eXkhPj5emsSckZEBPb3/3ai6desWRo4ciaysLDRq1Aje3t7Yt28f3N3dpToff/wxCgoK8M477yAnJwfdunVDfHx8mQ9MJCIiImVSCSGErjtR1+Tl5cHS0hK5ubl8HEb0jDly5Ai8vb3hELoARg4tdd0dIsUpzEpB1oqxOHz4MDp16qTVtqvz+7vevQVGRERE9LQYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcepEAFq0aBFcXFxgbGwMHx8fHDx4sMK6P/zwA7p3745GjRqhUaNGUKvVZeqHhYVBpVLJlsDAwJoeBhEREdUTOg9Aq1atQkREBKZMmYIjR46gQ4cOCAgIwNWrV8utn5iYiKFDh2LHjh1ISkpC06ZN0adPH1y+fFlWLzAwEJmZmdKycuXK2hgOERER1QM6D0Dz5s3DyJEjER4eDnd3d0RHR8PU1BTLli0rt/4vv/yC999/H15eXmjTpg2WLFmCkpISJCQkyOoZGRnBwcFBWho1alQbwyEiIqJ6QKcBqKioCIcPH4ZarZbK9PT0oFarkZSUVKU27ty5g/v378Pa2lpWnpiYCDs7O7i5uWHUqFG4ceOGVvtORERE9ZeBLg9+/fp1FBcXw97eXlZub2+Ps2fPVqmNTz75BE5OTrIQFRgYiEGDBsHV1RWpqamYOHEigoKCkJSUBH19/TJtFBYWorCwUFrPy8vTcERERERUH+g0AD2tmTNnIjY2FomJiTA2NpbKhwwZIv2/h4cHPD090aJFCyQmJqJXr15l2omKisK0adNqpc9ERESkezp9BGZjYwN9fX1kZ2fLyrOzs+Hg4FDpvnPmzMHMmTOxZcsWeHp6Vlq3efPmsLGxQUpKSrnbIyMjkZubKy0XL16s3kCIiIioXtFpADI0NIS3t7dsAnPphGZfX98K95s9ezZmzJiB+Ph4dO7c+YnHuXTpEm7cuAFHR8dytxsZGcHCwkK2EBER0bNL52+BRURE4IcffsCKFStw5swZjBo1CgUFBQgPDwcAhISEIDIyUqo/a9YsTJo0CcuWLYOLiwuysrKQlZWF/Px8AEB+fj4mTJiA/fv3Iz09HQkJCRgwYABatmyJgIAAnYyRiIiI6hadzwEKDg7GtWvXMHnyZGRlZcHLywvx8fHSxOiMjAzo6f0vpy1evBhFRUV4/fXXZe1MmTIFU6dOhb6+Po4fP44VK1YgJycHTk5O6NOnD2bMmAEjI6NaHRsRERHVTToPQAAwevRojB49utxtiYmJsvX09PRK2zIxMcHmzZu11DMiIiJ6Fun8ERgRERFRbWMAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFYQAiIiIixWEAIiIiIsVhACIiIiLFqRMBaNGiRXBxcYGxsTF8fHxw8ODBSuuvWbMGbdq0gbGxMTw8PLBx40bZdiEEJk+eDEdHR5iYmECtVuP8+fM1OQQiIiKqR3QegFatWoWIiAhMmTIFR44cQYcOHRAQEICrV6+WW3/fvn0YOnQohg8fjqNHj2LgwIEYOHAgTp48KdWZPXs2vv76a0RHR+PAgQMwMzNDQEAA7t27V1vDIiIiojpM5wFo3rx5GDlyJMLDw+Hu7o7o6GiYmppi2bJl5dZfuHAhAgMDMWHCBLRt2xYzZsxAp06d8O233wJ4ePdnwYIF+PzzzzFgwAB4enrixx9/xJUrVxAXF1eLIyMiIqK6SqcBqKioCIcPH4ZarZbK9PT0oFarkZSUVO4+SUlJsvoAEBAQINVPS0tDVlaWrI6lpSV8fHwqbJOIiIiUxUCXB79+/TqKi4thb28vK7e3t8fZs2fL3ScrK6vc+llZWdL20rKK6jyusLAQhYWF0npubi4AIC8vrxqjqbqsrKwK+0JENevcuXMAgMKsFJQU8bE4UW27f/MSACA/P1/rv2dL2xNCPLGuTgNQXREVFYVp06aVKW/atKkOekNEteHW5m913QUiRfPz86uxtm/fvg1LS8tK6+g0ANnY2EBfXx/Z2dmy8uzsbDg4OJS7j4ODQ6X1S/+bnZ0NR0dHWR0vL69y24yMjERERIS0XlJSgps3b+K5556DSqWq9rgqk5eXh6ZNm+LixYuwsLDQatt1AcdX/z3rY+T46r9nfYwcn+aEELh9+zacnJyeWFenAcjQ0BDe3t5ISEjAwIEDATwMHwkJCRg9enS5+/j6+iIhIQFjx46VyrZu3QpfX18AgKurKxwcHJCQkCAFnry8PBw4cACjRo0qt00jIyMYGRnJyqysrJ5qbE9iYWHxTP7BLsXx1X/P+hg5vvrvWR8jx6eZJ935KaXzR2AREREIDQ1F586d0aVLFyxYsAAFBQUIDw8HAISEhKBx48aIiooCAIwZMwZ+fn6YO3cu+vXrh9jYWBw6dAjff/89AEClUmHs2LH44osv0KpVK7i6umLSpElwcnKSQhYREREpm84DUHBwMK5du4bJkycjKysLXl5eiI+PlyYxZ2RkQE/vfy+rde3aFb/++is+//xzTJw4Ea1atUJcXBzat28v1fn4449RUFCAd955Bzk5OejWrRvi4+NhbGxc6+MjIiKiukfnAQgARo8eXeEjr8TExDJlb7zxBt54440K21OpVJg+fTqmT5+urS5qjZGREaZMmVLmkduzguOr/571MXJ89d+zPkaOr3aoRFXeFSMiIiJ6huj8k6CJiIiIahsDEBERESkOAxAREREpDgMQERERKQ4DkJbdvHkTw4YNg4WFBaysrDB8+HDk5+dXuo+/vz9UKpVsee+992R1MjIy0K9fP5iamsLOzg4TJkzAgwcPanIo5aru+G7evIl///vfcHNzg4mJCZo1a4YPP/xQ+r61Uo+PX6VSITY2tqaHAwBYtGgRXFxcYGxsDB8fHxw8eLDS+mvWrEGbNm1gbGwMDw8PbNy4UbZdCIHJkyfD0dERJiYmUKvVOH/+fE0OoVLVGd8PP/yA7t27o1GjRmjUqBHUanWZ+mFhYWWuVWBgYE0Po1LVGWNMTEyZ/j/+ERn1+RqW9/eJSqVCv379pDp16Rru2rUL/fv3h5OTE1QqFeLi4p64T2JiIjp16gQjIyO0bNkSMTExZepU9+e6plR3fGvXrkXv3r1ha2sLCwsL+Pr6YvPmzbI6U6dOLXP92rRpU4OjqFx1x5iYmFjun9HHvyOzxq+hIK0KDAwUHTp0EPv37xe7d+8WLVu2FEOHDq10Hz8/PzFy5EiRmZkpLbm5udL2Bw8eiPbt2wu1Wi2OHj0qNm7cKGxsbERkZGRND6eM6o7vxIkTYtCgQWL9+vUiJSVFJCQkiFatWonXXntNVg+AWL58uewc3L17t6aHI2JjY4WhoaFYtmyZOHXqlBg5cqSwsrIS2dnZ5dbfu3ev0NfXF7NnzxanT58Wn3/+uWjQoIE4ceKEVGfmzJnC0tJSxMXFiWPHjolXXnlFuLq61sp4Hlfd8b355pti0aJF4ujRo+LMmTMiLCxMWFpaikuXLkl1QkNDRWBgoOxa3bx5s7aGVEZ1x7h8+XJhYWEh639WVpasTn2+hjdu3JCN7eTJk0JfX18sX75cqlOXruHGjRvFZ599JtauXSsAiD/++KPS+v/8848wNTUVERER4vTp0+Kbb74R+vr6Ij4+XqpT3XNWk6o7vjFjxohZs2aJgwcPir///ltERkaKBg0aiCNHjkh1pkyZItq1aye7fteuXavhkVSsumPcsWOHACDOnTsnG0NxcbFUpzauIQOQFp0+fVoAEH/99ZdUtmnTJqFSqcTly5cr3M/Pz0+MGTOmwu0bN24Uenp6sr+kFy9eLCwsLERhYaFW+l4Vmo7vcatXrxaGhobi/v37UllVfmhqQpcuXcQHH3wgrRcXFwsnJycRFRVVbv3BgweLfv36ycp8fHzEu+++K4QQoqSkRDg4OIivvvpK2p6TkyOMjIzEypUra2AElavu+B734MEDYW5uLlasWCGVhYaGigEDBmi7qxqr7hiXL18uLC0tK2zvWbuG8+fPF+bm5iI/P18qq2vXsFRV/h74+OOPRbt27WRlwcHBIiAgQFp/2nNWUzT9e87d3V1MmzZNWp8yZYro0KGD9jqmRdUJQLdu3aqwTm1cQz4C06KkpCRYWVmhc+fOUplarYaenh4OHDhQ6b6//PILbGxs0L59e0RGRuLOnTuydj08PKRPxwaAgIAA5OXl4dSpU9ofSAWeZnyPys3NhYWFBQwM5J/D+cEHH8DGxgZdunTBsmXLIGr4I6qKiopw+PBhqNVqqUxPTw9qtRpJSUnl7pOUlCSrDzy8FqX109LSkJWVJatjaWkJHx+fCtusKZqM73F37tzB/fv3YW1tLStPTEyEnZ0d3NzcMGrUKNy4cUOrfa8qTceYn58PZ2dnNG3aFAMGDJD9HD1r13Dp0qUYMmQIzMzMZOV15RpW15N+BrVxzuqSkpIS3L59u8zP4Pnz5+Hk5ITmzZtj2LBhyMjI0FEPNefl5QVHR0f07t0be/fulcpr6xrWiU+CflZkZWXBzs5OVmZgYABra+syzzYf9eabb8LZ2RlOTk44fvw4PvnkE5w7dw5r166V2n00/ACQ1itrV9s0Hd+jrl+/jhkzZuCdd96RlU+fPh09e/aEqakptmzZgvfffx/5+fn48MMPtdb/8vpSXFxc7rk9e/ZsuftUdC1Kx1/638rq1BZNxve4Tz75BE5OTrK/iAIDAzFo0CC4uroiNTUVEydORFBQEJKSkqCvr6/VMTyJJmN0c3PDsmXL4OnpidzcXMyZMwddu3bFqVOn0KRJk2fqGh48eBAnT57E0qVLZeV16RpWV0U/g3l5ebh79y5u3br11H/u65I5c+YgPz8fgwcPlsp8fHwQExMDNzc3ZGZmYtq0aejevTtOnjwJc3NzHfa2ahwdHREdHY3OnTujsLAQS5Ysgb+/Pw4cOIBOnTpp5e+uqmAAqoJPP/0Us2bNqrTOmTNnNG7/0TDg4eEBR0dH9OrVC6mpqWjRooXG7VZVTY+vVF5eHvr16wd3d3dMnTpVtm3SpEnS/3fs2BEFBQX46quvajQAUeVmzpyJ2NhYJCYmyiYJDxkyRPp/Dw8PeHp6okWLFkhMTESvXr100dVq8fX1ha+vr7TetWtXtG3bFt999x1mzJihw55p39KlS+Hh4YEuXbrIyuv7NVSKX3/9FdOmTcO6detk//gMCgqS/t/T0xM+Pj5wdnbG6tWrMXz4cF10tVrc3Nzg5uYmrXft2hWpqamYP38+fvrpp1rrBwNQFYwbNw5hYWGV1mnevDkcHBxw9epVWfmDBw9w8+ZNODg4VPl4Pj4+AICUlBS0aNECDg4OZWa/Z2dnA0C12q1IbYzv9u3bCAwMhLm5Of744w80aNCg0vo+Pj6YMWMGCgsLa+z7YmxsbKCvry+dy1LZ2dkVjsfBwaHS+qX/zc7OhqOjo6yOl5eXFnv/ZJqMr9ScOXMwc+ZMbNu2DZ6enpXWbd68OWxsbJCSklLrvzyfZoylGjRogI4dOyIlJQXAs3MNCwoKEBsbW6XvRNTlNayuin4GLSwsYGJiAn19/af+M1EXxMbGYsSIEVizZk2ZR36Ps7KyQuvWraU/w/VRly5dsGfPHgDa+bmuCs4BqgJbW1u0adOm0sXQ0BC+vr7IycnB4cOHpX23b9+OkpISKdRURXJyMgBIf/n6+vrixIkTsvCxdetWWFhYwN3dvc6PLy8vD3369IGhoSHWr19f5pXj8iQnJ6NRo0Y1+mV5hoaG8Pb2RkJCglRWUlKChIQE2R2CR/n6+srqAw+vRWl9V1dXODg4yOrk5eXhwIEDFbZZUzQZHwDMnj0bM2bMQHx8vGy+V0UuXbqEGzduyMJCbdF0jI8qLi7GiRMnpP4/C9cQePhxDYWFhXjrrbeeeBxdXsPqetLPoDb+TOjaypUrER4ejpUrV8o+vqAi+fn5SE1NrRfXryLJyclS/2vtGmptOjUJIR6+Jt6xY0dx4MABsWfPHtGqVSvZa+KXLl0Sbm5u4sCBA0IIIVJSUsT06dPFoUOHRFpamli3bp1o3ry56NGjh7RP6Wvwffr0EcnJySI+Pl7Y2trq7DX46owvNzdX+Pj4CA8PD5GSkiJ75fHBgwdCCCHWr18vfvjhB3HixAlx/vx58d///leYmpqKyZMn1/h4YmNjhZGRkYiJiRGnT58W77zzjrCyspLeuHv77bfFp59+KtXfu3evMDAwEHPmzBFnzpwRU6ZMKfc1eCsrK7Fu3Tpx/PhxMWDAAJ2+Ql2d8c2cOVMYGhqK3377TXatbt++LYQQ4vbt22L8+PEiKSlJpKWliW3btolOnTqJVq1aiXv37tX6+DQZ47Rp08TmzZtFamqqOHz4sBgyZIgwNjYWp06dkurU52tYqlu3biI4OLhMeV27hrdv3xZHjx4VR48eFQDEvHnzxNGjR8WFCxeEEEJ8+umn4u2335bql74GP2HCBHHmzBmxaNGicl+Dr+yc1eXx/fLLL8LAwEAsWrRI9jOYk5Mj1Rk3bpxITEwUaWlpYu/evUKtVgsbGxtx9erVWh+fENUf4/z580VcXJw4f/68OHHihBgzZozQ09MT27Ztk+rUxjVkANKyGzduiKFDh4qGDRsKCwsLER4eLv3yEEKItLQ0AUDs2LFDCCFERkaG6NGjh7C2thZGRkaiZcuWYsKECbLPARJCiPT0dBEUFCRMTEyEjY2NGDdunOw18tpS3fGVvu5Y3pKWliaEePgqvZeXl2jYsKEwMzMTHTp0ENHR0bLPhKhJ33zzjWjWrJkwNDQUXbp0Efv375e2+fn5idDQUFn91atXi9atWwtDQ0PRrl07sWHDBtn2kpISMWnSJGFvby+MjIxEr169xLlz52pjKOWqzvicnZ3LvVZTpkwRQghx584d0adPH2FraysaNGggnJ2dxciRI3Xyi+VR1Rnj2LFjpbr29vaib9++ss9YEaJ+X0MhhDh79qwAILZs2VKmrbp2DSv6O6J0TKGhocLPz6/MPl5eXsLQ0FA0b95c9hlHpSo7Z7WpuuPz8/OrtL4QD1/7d3R0FIaGhqJx48YiODhYpKSk1O7AHlHdMc6aNUu0aNFCGBsbC2tra+Hv7y+2b99ept2avoYqIWr4XWMiIiKiOoZzgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIqN6JiYmBlZWVRvtOmjRJ9gXEuuTv74+xY8fquhsSIQTeeecdWFtbQ6VSSV/LU1eoVCrExcU9sV5RURFcXFxw6NChmu8U1VsMQEQArl27hlGjRqFZs2YwMjKCg4MDAgICsHfvXq0ep679wqvM04QMbXJxccGCBQu00lZWVhYWLlyIzz77TCvtPWvi4+MRExODP//8E5mZmWjfvr2uu6QRQ0NDjB8/Hp988omuu0J1GL8NngjAa6+9hqKiIqxYsQLNmzdHdnY2EhIScOPGDV13jbRoyZIl6Nq1K5ydnXXdlRpTXFwMlUoFPb3q//u29As1u3btWgM9q5qioiIYGho+dTvDhg3DuHHjcOrUKbRr104LPaNnDe8AkeLl5ORg9+7dmDVrFl566SU4OzujS5cuiIyMxCuvvCKrN2LECNja2sLCwgI9e/bEsWPHpO1Tp06Fl5cXfvrpJ7i4uMDS0hJDhgzB7du3AQBhYWHYuXMnFi5cCJVKBZVKhfT0dADAyZMnERQUhIYNG8Le3h5vv/02rl+/LrXt7++PDz/8EB9//DGsra3h4OCAqVOnlhnHu+++C3t7exgbG6N9+/b4888/pe179uxB9+7dYWJigqZNm+LDDz9EQUHBU523pzkfAHD79m0MGzYMZmZmcHR0xPz582V3yfz9/XHhwgV89NFH0jl71ObNm9G2bVs0bNgQgYGByMzMrLTPsbGx6N+/v6zsSec2PT29zOOgnJwcqFQqJCYmAgASExOhUqmwefNmdOzYESYmJujZsyeuXr2KTZs2oW3btrCwsMCbb76JO3fuyI7/4MEDjB49GpaWlrCxscGkSZPw6DcUFRYWYvz48WjcuDHMzMzg4+MjHRf435269evXw93dHUZGRsjIyCh3/Dt37kSXLl1gZGQER0dHfPrpp3jw4AGAh38+//3vfyMjIwMqlQouLi5l9hdCwNbWFr/99ptU5uXlJfsW8j179sDIyEgaZ0ZGBgYMGICGDRvCwsICgwcPRnZ2tlS/9M/JkiVL4OrqCmNjYwDA+fPn0aNHDxgbG8Pd3R1bt26V9aWoqAijR4+Go6MjjI2N4ezsjKioKGl7o0aN8OKLLyI2Nrbcc0HEAESK17BhQzRs2BBxcXEoLCyssN4bb7wh/UI7fPgwOnXqhF69euHmzZtSndTUVMTFxeHPP//En3/+iZ07d2LmzJkAgIULF8LX1xcjR45EZmYmMjMz0bRpU+Tk5KBnz57o2LEjDh06hPj4eGRnZ2Pw4MGy469YsQJmZmY4cOAAZs+ejenTp0u/FEpKShAUFIS9e/fi559/xunTpzFz5kzo6+tL/QoMDMRrr72G48ePY9WqVdizZw9Gjx6t8Xl72vMBABEREdi7dy/Wr1+PrVu3Yvfu3Thy5Ii0fe3atWjSpAmmT58unbNSd+7cwZw5c/DTTz9h165dyMjIwPjx4yvs782bN3H69Gl07ty5zLbKzm11TJ06Fd9++y327duHixcvYvDgwViwYAF+/fVXbNiwAVu2bME333xT5tgGBgY4ePAgFi5ciHnz5mHJkiXS9tGjRyMpKQmxsbE4fvw43njjDQQGBuL8+fOyczFr1iwsWbIEp06dgp2dXZm+Xb58GX379sXzzz+PY8eOYfHixVi6dCm++OILAA//fE6fPh1NmjRBZmYm/vrrrzJtqFQq9OjRQwpgt27dwpkzZ3D37l2cPXsWwMOQ9fzzz8PU1BQlJSUYMGAAbt68iZ07d2Lr1q34559/EBwcLGs3JSUFv//+O9auXYvk5GSUlJRg0KBBMDQ0xIEDBxAdHV3mcdbXX3+N9evXY/Xq1Th37hx++eWXMqGtS5cu2L179xOuGimWVr9alaie+u2330SjRo2EsbGx6Nq1q4iMjBTHjh2Ttu/evVtYWFiIe/fuyfZr0aKF+O6774QQQkyZMkWYmpqKvLw8afuECROEj4+PtO7n5yfGjBkja2PGjBmiT58+srKLFy8KANI3kPv5+Ylu3brJ6jz//PPik08+EUIIsXnzZqGnp1fhN5YPHz5cvPPOO7Ky3bt3Cz09PXH37t1y91m+fLmwtLQsd5s2zkdeXp5o0KCBWLNmjbQ9JydHmJqays6Rs7OzmD9/fpm+AZB9A/aiRYuEvb19uf0VQoijR48KACIjI0NW/qRzm5aWJgCIo0ePSttv3bolAIgdO3YIIf73bdjbtm2T6kRFRQkAIjU1VSp79913RUBAgOzYbdu2FSUlJVLZJ598Itq2bSuEEOLChQtCX19fXL58Wda/Xr16icjISNm5SE5OrnDsQggxceJE4ebmJjvWokWLRMOGDUVxcbEQQoj58+cLZ2fnStv5+uuvRbt27YQQQsTFxQkfHx8xYMAAsXjxYiGEEGq1WkycOFEIIcSWLVuEvr6+7JyfOnVKABAHDx4UQjz8c9KgQQNx9epVqc7mzZuFgYGBbNybNm0SAMQff/whhBDi3//+t+jZs6dsPI9buHChcHFxqXQ8pFy8A0SEh3OArly5gvXr1yMwMBCJiYno1KkTYmJiAADHjh1Dfn4+nnvuOemOUcOGDZGWlobU1FSpHRcXF5ibm0vrjo6OuHr1aqXHPnbsGHbs2CFrt02bNgAga9vT01O236NtJycno0mTJmjdunWFx4iJiZEdIyAgACUlJUhLS6v6iXqkvac9H//88w/u37+PLl26SNstLS3h5uZWpT6YmpqiRYsW5bZdnrt37wKA9IjlUZWd2+p4tB17e3uYmpqiefPmsrLH233hhRdkj/Z8fX1x/vx5FBcX48SJEyguLkbr1q1l53nnzp2y82xoaFhmDI87c+YMfH19Zcd68cUXkZ+fj0uXLlV5jH5+fjh9+jSuXbuGnTt3wt/fH/7+/khMTMT9+/exb98++Pv7S8ds2rQpmjZtKu3v7u4OKysrnDlzRipzdnaGra2trK9NmzaFk5OT7Lw8KiwsDMnJyXBzc8OHH36ILVu2lOmriYlJmUeORKU4CZro/zM2Nkbv3r3Ru3dvTJo0CSNGjMCUKVMQFhaG/Px8ODo6yuZelHr0TakGDRrItqlUKpSUlFR63Pz8fPTv3x+zZs0qs+3RuRWVtW1iYvLEY7z77rv48MMPy2xr1qxZpftW1F5NnY+qKq9t8cjcmcfZ2NgAePjY5tFftk/qZ+lk4kfbvn///hP7pFKpnnr8+fn50NfXx+HDh6XHmaUaNmwo/b+JiUmZ+VE1xcPDA9bW1ti5cyd27tyJ//znP3BwcMCsWbPw119/4f79+9WeRG1mZlbtfnTq1AlpaWnYtGkTtm3bhsGDB0OtVsvmJ928ebPMtSYqxQBEVAF3d3fpM0c6deqErKwsGBgYlDs5tKoMDQ1RXFwsK+vUqRN+//13uLi4wMBAsx9JT09PXLp0CX///Xe5d4E6deqE06dPo2XLlhq1X157T3s+mjdvjgYNGuCvv/6SQlhubi7+/vtv9OjRQ6pX3jnTRIsWLWBhYYHTp09XeKesPKW/QDMzM9GxY0cA0Orn4xw4cEC2vn//frRq1Qr6+vro2LEjiouLcfXqVXTv3v2pjtO2bVv8/vvvEEJIYWnv3r0wNzdHkyZNqtyOSqVC9+7dsW7dOpw6dQrdunWDqakpCgsL8d1336Fz585SoGnbti0uXryIixcvSneBTp8+jZycHLi7u1fa14sXLyIzM1P6R8D+/fvL1LOwsEBwcDCCg4Px+uuvIzAwEDdv3oS1tTWAhy8XlF4zosfxERgp3o0bN9CzZ0/8/PPPOH78ONLS0rBmzRrMnj0bAwYMAACo1Wr4+vpi4MCB2LJlC9LT07Fv3z589tln1fqwNRcXFxw4cADp6em4fv06SkpK8MEHH+DmzZsYOnQo/vrrL6SmpmLz5s0IDw+v8i9+Pz8/9OjRA6+99hq2bt0q/cs4Pj4eAPDJJ59g3759GD16NJKTk3H+/HmsW7fuiZOgi4uLkZycLFvOnDmjlfNhbm6O0NBQTJgwATt27MCpU6cwfPhw6Onpye5muLi4YNeuXbh8+bLszbjq0tPTg1qtxp49e6q1n4mJCV544QXMnDkTZ86cwc6dO/H5559r3I/HZWRkICIiAufOncPKlSvxzTffYMyYMQCA1q1bY9iwYQgJCcHatWuRlpaGgwcPIioqChs2bKjWcd5//31cvHgR//73v3H27FmsW7cOU6ZMQURERLVfmff398fKlSvh5eWFhg0bQk9PDz169MAvv/wCPz8/qZ5arYaHhweGDRuGI0eO4ODBgwgJCYGfn1+5k9Ef3a9169YIDQ3FsWPHsHv37jKf3TRv3jysXLkSZ8+exd9//401a9bAwcFBdgdy9+7d6NOnT7XGRsrBAESK17BhQ/j4+GD+/Pno0aMH2rdvj0mTJmHkyJH49ttvATz8V+/GjRvRo0cPhIeHo3Xr1hgyZAguXLgAe3v7Kh9r/Pjx0NfXh7u7O2xtbZGRkQEnJyfs3bsXxcXF6NOnDzw8PDB27FhYWVlV6xfT77//jueffx5Dhw6Fu7s7Pv74YylAeXp6YufOnfj777/RvXt3dOzYEZMnT5bNsShPfn4+OnbsKFv69++vtfMxb948+Pr64uWXX4ZarcaLL76Itm3byubpTJ8+Henp6WjRosVTP84YMWIEYmNjq/0YbtmyZXjw4AG8vb0xduxY6c0pbQgJCcHdu3fRpUsXfPDBBxgzZozsk6qXL1+OkJAQjBs3Dm5ubhg4cKDsrllVNW7cGBs3bsTBgwfRoUMHvPfeexg+fLhGYc7Pzw/FxcXSXB/gYSh6vEylUmHdunVo1KgRevToAbVajebNm2PVqlWVtq+np4c//vhDOi8jRozAf/7zH1kdc3NzzJ49G507d8bzzz+P9PR0bNy4UfqZSUpKQm5uLl5//fVqj4+UQSUqe2hORFSLCgoK0LhxY8ydOxfDhw/XevtCCPj4+OCjjz7C0KFDtd4+1R3BwcHo0KEDJk6cqOuuUB3FO0BEpDNHjx7FypUrkZqaiiNHjmDYsGEAID161DaVSoXvv/9e+vA/ejYVFRXBw8MDH330ka67QnUY7wARkc4cPXoUI0aMwLlz52BoaAhvb2/MmzcPHh4euu4aET3jGICIiIhIcfgIjIiIiBSHAYiIiIgUhwGIiIiIFIcBiIiIiBSHAYiIiIgUhwGIiIiIFIcBiIiIiBSHAYiIiIgUhwGIiIiIFOf/Aa6j91zbptgrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statistics as stats\n",
    "\n",
    "lengths = [len(str(item).split()) for item in df]\n",
    "lengths.sort(reverse=True)\n",
    "\n",
    "print(f\"Median: {stats.median(lengths)}\")\n",
    "print(f\"Mean: {stats.mean(lengths)}\")\n",
    "print(f\"STD: {stats.stdev(lengths)}\")\n",
    "\n",
    "plt.hist(lengths, bins=range(max(lengths) + 2), align='left', edgecolor='black')\n",
    "plt.xlabel('Sentence Length (number of words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Joke Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "776eddc0-093e-45f7-a6d2-97e37713c6ec",
   "metadata": {
    "id": "776eddc0-093e-45f7-a6d2-97e37713c6ec"
   },
   "outputs": [],
   "source": [
    "final_ds = dataset.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81e08a74-8b23-4088-b4de-4cef0d3cc6f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81e08a74-8b23-4088-b4de-4cef0d3cc6f3",
    "outputId": "b5d753e5-5551-43d4-d708-eec3b9af9dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 14102\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 3526\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(final_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c3d5ce2-f5ea-4e8e-a6d5-6e305dfc9de6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c3d5ce2-f5ea-4e8e-a6d5-6e305dfc9de6",
    "outputId": "f6360542-4591-41d5-decd-17187a8b5df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input', 'output']\n"
     ]
    }
   ],
   "source": [
    "print(final_ds[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13234f4c-f64c-448c-8553-2612011a49b2",
   "metadata": {
    "id": "13234f4c-f64c-448c-8553-2612011a49b2"
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    # Minimal formatting, directly using input and output fields\n",
    "    return [f\"{example['input']}\\n{example['output']}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f55eef0-152e-41a9-90c5-6f61b25fa041",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "33935c76b57948468b861d657fc9573f",
      "83a1afb24ea54eb18b30009a5be2f37f",
      "4758f4169f7e44e2818648873f08e049",
      "4e74976f45b441edb956a59c338c5772",
      "a20db37b9f7341b8a256e5ce7c4e4658",
      "f63784a5672f45c0bca36d5afc3e4793",
      "7c7aceab5c044f0eb395b649bc78df36",
      "71980821352641788e0b9c8a90b1abfb",
      "8a57a71ca40a4dbc82b5b290fc68284a",
      "f1afba226a4545f7b28c58f5bf970cd1",
      "3e5efa49cfc4412d9e8c0d9e26145e54",
      "73e2e453d9514749a2ce6e555b8857af",
      "59e3a962819345ed887a3746704de5d7",
      "12eece125e854c99b1b1a75a439fdd6b",
      "0a2fd6855667473ca87a48d95722f29f",
      "a59cb746f96e406380b053c9ca2ed057",
      "36ec7f05c7f741e0a5a6afb9faf0d5a5",
      "344eeba5c2cb4251a30ae4d71235cf89",
      "a929ec96f98240a18133cb3988c9139f",
      "2ee908ba9473404d9fd51bbf07b8ae98",
      "6ba03846819f4e3ca89dd28f4c939f70",
      "e3577a5d7b3d496999e9467ac6c35747"
     ]
    },
    "id": "2f55eef0-152e-41a9-90c5-6f61b25fa041",
    "outputId": "4a4ab7b5-77f7-4053-802a-2d6f81371032"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14102/14102 [01:16<00:00, 184.29 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3526/3526 [00:20<00:00, 169.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_and_tokenize_batch(examples):\n",
    "    input_texts = [f\"manpage: {inp}\" for inp in examples[\"input\"]]\n",
    "    target_texts = examples[\"output\"]\n",
    "\n",
    "    input_encodings = tokenizer(\n",
    "        input_texts,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    target_encodings = tokenizer(\n",
    "        target_texts,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Apply label masking\n",
    "    labels = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in target]\n",
    "        for target in target_encodings[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# Apply the batch-friendly function\n",
    "tokenized_ds = final_ds.map(\n",
    "    format_and_tokenize_batch,\n",
    "    batched=True,  # Process examples in batches\n",
    "    remove_columns=[\"input\", \"output\"],  # Remove original columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc7c580c-b69a-47a2-b7d7-0a8a60afe7cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc7c580c-b69a-47a2-b7d7-0a8a60afe7cc",
    "outputId": "eda7ec6f-2b3d-4f2b-e8d3-84eee35dae85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '<SECTION>HANDLING OF SVN BRANCHES</SECTION>\\nIf git svn is configured to fetch branches (and --follow-branches is in\\n       effect), it sometimes creates multiple Git branches for one SVN branch,\\n       where the additional branches have names of the form branchname@nnn\\n       (with nnn an SVN revision number). These additional branches are\\n       created if git svn cannot find a parent commit for the first commit in\\n       an SVN branch, to connect the branch to the history of the other\\n       branches.\\n\\n       Normally, the first commit in an SVN branch consists of a copy\\n       operation. git svn will read this commit to get the SVN revision the\\n       branch was created from. It will then try to find the Git commit that\\n       corresponds to this SVN revision, and use that as the parent of the\\n       branch. However, it is possible that there is no suitable Git commit to\\n       serve as parent. This will happen, among other reasons, if the SVN\\n       branch is a copy of a revision that was not fetched by git svn (e.g.\\n       because it is an old revision that was skipped with --revision), or if\\n       in SVN a directory was copied that is not tracked by git svn (such as a\\n       branch that is not tracked at all, or a subdirectory of a tracked\\n       branch). In these cases, git svn will still create a Git branch, but\\n       instead of using an existing Git commit as the parent of the branch, it\\n       will read the SVN history of the directory the branch was copied from\\n       and create appropriate Git commits. This is indicated by the message\\n       \"Initializing parent: <branchname>\".\\n\\n       Additionally, it will create a special branch named\\n       <branchname>@<SVN-Revision>, where <SVN-Revision> is the SVN revision\\n       number the branch was copied from. This branch will point to the newly\\n       created parent commit of the branch. If in SVN the branch was deleted\\n       and later recreated from a different version, there will be multiple\\n       such branches with an @.\\n\\n       Note that this may mean that multiple Git commits are created for a\\n       single SVN revision.\\n\\n       An example: in an SVN repository with a standard trunk/tags/branches\\n       layout, a directory trunk/sub is created in r.100. In r.200, trunk/sub\\n       is branched by copying it to branches/. git svn clone -s will then\\n       create a branch sub. It will also create new Git commits for r.100\\n       through r.199 and use these as the history of branch sub. Thus there\\n       will be two Git commits for each revision from r.100 to r.199 (one\\n       containing trunk/, one containing trunk/sub/). Finally, it will create\\n       a branch sub@200 pointing to the new parent commit of branch sub (i.e.\\n       the commit for r.200 and trunk/sub/).',\n",
       " 'output': '<SECTION>NAME</SECTION>\\ngit-svn - Bidirectional operation between a Subversion repository and\\n       Git\\n<SECTION>SYNOPSIS</SECTION>\\ngit svn <command> [<options>] [<arguments>]\\n<SECTION>DESCRIPTION</SECTION>\\ngit svn is a simple conduit for changesets between Subversion and Git.\\n       It provides a bidirectional flow of changes between a Subversion and a\\n       Git repository.\\n\\n       git svn can track a standard Subversion repository, following the\\n       common \"trunk/branches/tags\" layout, with the --stdlayout option. It\\n       can also follow branches and tags in any layout with the -T/-t/-b\\n       options (see options to init below, and also the clone command).\\n\\n       Once tracking a Subversion repository (with any of the above methods),\\n       the Git repository can be updated from Subversion by the fetch command\\n       and Subversion updated from Git by the dcommit command.\\n<SECTION>COMMANDS</SECTION>\\ninit\\n           Initializes an empty Git repository with additional metadata\\n           directories for git svn. The Subversion URL may be specified as a\\n           command-line argument, or as full URL arguments to -T/-t/-b.\\n           Optionally, the target directory to operate on can be specified as\\n           a second argument. Normally this command initializes the current\\n           directory.\\n\\n           -T<trunk-subdir>, --trunk=<trunk-subdir>, -t<tags-subdir>,\\n           --tags=<tags-subdir>, -b<branches-subdir>,\\n           --branches=<branches-subdir>, -s, --stdlayout\\n               These are optional command-line options for init. Each of these\\n               flags can point to a relative repository path\\n               (--tags=project/tags) or a full url\\n               (--tags=https://foo.org/project/tags). You can specify more\\n               than one --tags and/or --branches options, in case your\\n               Subversion repository places tags or branches under multiple\\n               paths. The option --stdlayout is a shorthand way of setting\\n               trunk,tags,branches as the relative paths, which is the\\n               Subversion default. If any of the other options are given as\\n               well, they take precedence.\\n\\n           --no-metadata\\n               Set the noMetadata option in the [svn-remote] config. This\\n               option is not recommended, please read the svn.noMetadata\\n               section of this manpage before using this option.\\n\\n           --use-svm-props\\n               Set the useSvmProps option in the [svn-remote] config.\\n\\n           --use-svnsync-props\\n               Set the useSvnsyncProps option in the [svn-remote] config.\\n\\n           --rewrite-root=<URL>\\n               Set the rewriteRoot option in the [svn-remote] config.\\n\\n           --rewrite-uuid=<UUID>\\n               Set the rewriteUUID option in the [svn-remote] config.\\n\\n           --username=<user>\\n               For transports that SVN handles authentication for (http,\\n               https, and plain svn), specify the username. For other\\n               transports (e.g.  svn+ssh://), you must include the username in\\n               the URL, e.g.  svn+ssh://foo@svn.bar.com/project\\n\\n           --prefix=<prefix>\\n               This allows one to specify a prefix which is prepended to the\\n               names of remotes if trunk/branches/tags are specified. The\\n               prefix does not automatically include a trailing slash, so be\\n               sure you include one in the argument if that is what you want.\\n               If --branches/-b is specified, the prefix must include a\\n               trailing slash. Setting a prefix (with a trailing slash) is\\n               strongly encouraged in any case, as your SVN-tracking refs will\\n               then be located at \"refs/remotes/$prefix/\", which is compatible\\n               with Git’s own remote-tracking ref layout\\n               (refs/remotes/$remote/). Setting a prefix is also useful if you\\n               wish to track multiple projects that share a common repository.\\n               By default, the prefix is set to origin/.\\n\\n                   Note\\n                   Before Git v2.0, the default prefix was \"\" (no prefix).\\n                   This meant that SVN-tracking refs were put at\\n                   \"refs/remotes/*\", which is incompatible with how Git’s own\\n                   remote-tracking refs are organized. If you still want the\\n                   old default, you can get it by passing --prefix \"\" on the\\n                   command line (--prefix=\"\" may not work if your Perl’s\\n                   Getopt::Long is < v2.37).\\n\\n           --ignore-refs=<regex>\\n               When passed to init or clone this regular expression will be\\n               preserved as a config key. See fetch for a description of\\n               --ignore-refs.\\n\\n           --ignore-paths=<regex>\\n               When passed to init or clone this regular expression will be\\n               preserved as a config key. See fetch for a description of\\n               --ignore-paths.\\n\\n           --include-paths=<regex>\\n               When passed to init or clone this regular expression will be\\n               preserved as a config key. See fetch for a description of\\n               --include-paths.\\n\\n           --no-minimize-url\\n               When tracking multiple directories (using --stdlayout,\\n               --branches, or --tags options), git svn will attempt to connect\\n               to the root (or highest allowed level) of the Subversion\\n               repository. This default allows better tracking of history if\\n               entire projects are moved within a repository, but may cause\\n               issues on repositories where read access restrictions are in\\n               place. Passing --no-minimize-url will allow git svn to accept\\n               URLs as-is without attempting to connect to a higher level\\n               directory. This option is off by default when only one\\n               URL/branch is tracked (it would do little good).\\n\\n       fetch\\n           Fetch unfetched revisions from the Subversion remote we are\\n           tracking. The name of the [svn-remote \"...\"] section in the\\n           $GIT_DIR/config file may be specified as an optional command-line\\n           argument.\\n\\n           This automatically updates the rev_map if needed (see\\n           $GIT_DIR/svn/**/.rev_map.*  in the FILES section below for\\n           details).\\n\\n           --localtime\\n               Store Git commit times in the local time zone instead of UTC.\\n               This makes git log (even without --date=local) show the same\\n               times that svn log would in the local time zone.\\n\\n               This doesn’t interfere with interoperating with the Subversion\\n               repository you cloned from, but if you wish for your local Git\\n               repository to be able to interoperate with someone else’s local\\n               Git repository, either don’t use this option or you should both\\n               use it in the same local time zone.\\n\\n           --parent\\n               Fetch only from the SVN parent of the current HEAD.\\n\\n           --ignore-refs=<regex>\\n               Ignore refs for branches or tags matching the Perl regular\\n               expression. A \"negative look-ahead assertion\" like\\n               ^refs/remotes/origin/(?!tags/wanted-tag|wanted-branch).*$ can\\n               be used to allow only certain refs.\\n\\n                   config key: svn-remote.<name>.ignore-refs\\n\\n               If the ignore-refs configuration key is set, and the\\n               command-line option is also given, both regular expressions\\n               will be used.\\n\\n           --ignore-paths=<regex>\\n               This allows one to specify a Perl regular expression that will\\n               cause skipping of all matching paths from checkout from SVN.\\n               The --ignore-paths option should match for every fetch\\n               (including automatic fetches due to clone, dcommit, rebase,\\n               etc) on a given repository.\\n\\n                   config key: svn-remote.<name>.ignore-paths\\n\\n               If the ignore-paths configuration key is set, and the\\n               command-line option is also given, both regular expressions\\n               will be used.\\n\\n               Examples:\\n\\n               Skip \"doc*\" directory for every fetch\\n\\n                       --ignore-paths=\"^doc\"\\n\\n\\n               Skip \"branches\" and \"tags\" of first level directories\\n\\n                       --ignore-paths=\"^[^/]+/(?:branches|tags)\"\\n\\n\\n           --include-paths=<regex>\\n               This allows one to specify a Perl regular expression that will\\n               cause the inclusion of only matching paths from checkout from\\n               SVN. The --include-paths option should match for every fetch\\n               (including automatic fetches due to clone, dcommit, rebase,\\n               etc) on a given repository.  --ignore-paths takes precedence\\n               over --include-paths.\\n\\n                   config key: svn-remote.<name>.include-paths\\n\\n\\n           --log-window-size=<n>\\n               Fetch <n> log entries per request when scanning Subversion\\n               history. The default is 100. For very large Subversion\\n               repositories, larger values may be needed for clone/fetch to\\n               complete in reasonable time. But overly large values may lead\\n               to higher memory usage and request timeouts.\\n\\n       clone\\n           Runs init and fetch. It will automatically create a directory based\\n           on the basename of the URL passed to it; or if a second argument is\\n           passed; it will create a directory and work within that. It accepts\\n           all arguments that the init and fetch commands accept; with the\\n           exception of --fetch-all and --parent. After a repository is\\n           cloned, the fetch command will be able to update revisions without\\n           affecting the working tree; and the rebase command will be able to\\n           update the working tree with the latest changes.\\n\\n           --preserve-empty-dirs\\n               Create a placeholder file in the local Git repository for each\\n               empty directory fetched from Subversion. This includes\\n               directories that become empty by removing all entries in the\\n               Subversion repository (but not the directory itself). The\\n               placeholder files are also tracked and removed when no longer\\n               necessary.\\n\\n           --placeholder-filename=<filename>\\n               Set the name of placeholder files created by\\n               --preserve-empty-dirs. Default: \".gitignore\"\\n\\n       rebase\\n           This fetches revisions from the SVN parent of the current HEAD and\\n           rebases the current (uncommitted to SVN) work against it.\\n\\n           This works similarly to svn update or git pull except that it\\n           preserves linear history with git rebase instead of git merge for\\n           ease of dcommitting with git svn.\\n\\n           This accepts all options that git svn fetch and git rebase accept.\\n           However, --fetch-all only fetches from the current [svn-remote],\\n           and not all [svn-remote] definitions.\\n\\n           Like git rebase; this requires that the working tree be clean and\\n           have no uncommitted changes.\\n\\n           This automatically updates the rev_map if needed (see\\n           $GIT_DIR/svn/**/.rev_map.*  in the FILES section below for\\n           details).\\n\\n           -l, --local\\n               Do not fetch remotely; only run git rebase against the last\\n               fetched commit from the upstream SVN.\\n\\n       dcommit\\n           Commit each diff from the current branch directly to the SVN\\n           repository, and then rebase or reset (depending on whether or not\\n           there is a diff between SVN and head). This will create a revision\\n           in SVN for each commit in Git.\\n\\n           When an optional Git branch name (or a Git commit object name) is\\n           specified as an argument, the subcommand works on the specified\\n           branch, not on the current branch.\\n\\n           Use of dcommit is preferred to set-tree (below).\\n\\n           --no-rebase\\n               After committing, do not rebase or reset.\\n\\n           --commit-url <URL>\\n               Commit to this SVN URL (the full path). This is intended to\\n               allow existing git svn repositories created with one transport\\n               method (e.g.  svn:// or http:// for anonymous read) to be\\n               reused if a user is later given access to an alternate\\n               transport method (e.g.  svn+ssh:// or https://) for commit.\\n\\n                   config key: svn-remote.<name>.commiturl\\n                   config key: svn.commiturl (overwrites all svn-remote.<name>.commiturl options)\\n\\n               Note that the SVN URL of the commiturl config key includes the\\n               SVN branch. If you rather want to set the commit URL for an\\n               entire SVN repository use svn-remote.<name>.pushurl instead.\\n\\n               Using this option for any other purpose (don’t ask) is very\\n               strongly discouraged.\\n\\n           --mergeinfo=<mergeinfo>\\n               Add the given merge information during the dcommit (e.g.\\n               --mergeinfo=\"/branches/foo:1-10\"). All svn server versions can\\n               store this information (as a property), and svn clients\\n               starting from version 1.5 can make use of it. To specify merge\\n               information from multiple branches, use a single space\\n               character between the branches (--mergeinfo=\"/branches/foo:1-10\\n               /branches/bar:3,5-6,8\")\\n\\n                   config key: svn.pushmergeinfo\\n\\n               This option will cause git-svn to attempt to automatically\\n               populate the svn:mergeinfo property in the SVN repository when\\n               possible. Currently, this can only be done when dcommitting\\n               non-fast-forward merges where all parents but the first have\\n               already been pushed into SVN.\\n\\n           --interactive\\n               Ask the user to confirm that a patch set should actually be\\n               sent to SVN. For each patch, one may answer \"yes\" (accept this\\n               patch), \"no\" (discard this patch), \"all\" (accept all patches),\\n               or \"quit\".\\n\\n               git svn dcommit returns immediately if answer is \"no\" or\\n               \"quit\", without committing anything to SVN.\\n\\n       branch\\n           Create a branch in the SVN repository.\\n\\n           -m, --message\\n               Allows to specify the commit message.\\n\\n           -t, --tag\\n               Create a tag by using the tags_subdir instead of the\\n               branches_subdir specified during git svn init.\\n\\n           -d<path>, --destination=<path>\\n               If more than one --branches (or --tags) option was given to the\\n               init or clone command, you must provide the location of the\\n               branch (or tag) you wish to create in the SVN repository.\\n               <path> specifies which path to use to create the branch or tag\\n               and should match the pattern on the left-hand side of one of\\n               the configured branches or tags refspecs. You can see these\\n               refspecs with the commands\\n\\n                   git config --get-all svn-remote.<name>.branches\\n                   git config --get-all svn-remote.<name>.tags\\n\\n               where <name> is the name of the SVN repository as specified by\\n               the -R option to init (or \"svn\" by default).\\n\\n           --username\\n               Specify the SVN username to perform the commit as. This option\\n               overrides the username configuration property.\\n\\n           --commit-url\\n               Use the specified URL to connect to the destination Subversion\\n               repository. This is useful in cases where the source SVN\\n               repository is read-only. This option overrides configuration\\n               property commiturl.\\n\\n                   git config --get-all svn-remote.<name>.commiturl\\n\\n           --parents\\n               Create parent folders. This parameter is equivalent to the\\n               parameter --parents on svn cp commands and is useful for\\n               non-standard repository layouts.\\n\\n       tag\\n           Create a tag in the SVN repository. This is a shorthand for branch\\n           -t.\\n\\n       log\\n           This should make it easy to look up svn log messages when svn users\\n           refer to -r/--revision numbers.\\n\\n           The following features from ‘svn log’ are supported:\\n\\n           -r <n>[:<n>], --revision=<n>[:<n>]\\n               is supported, non-numeric args are not: HEAD, NEXT, BASE, PREV,\\n               etc ...\\n\\n           -v, --verbose\\n               it’s not completely compatible with the --verbose output in svn\\n               log, but reasonably close.\\n\\n           --limit=<n>\\n               is NOT the same as --max-count, doesn’t count merged/excluded\\n               commits\\n\\n           --incremental\\n               supported\\n\\n           New features:\\n\\n           --show-commit\\n               shows the Git commit sha1, as well\\n\\n           --oneline\\n               our version of --pretty=oneline\\n\\n\\n               Note\\n               SVN itself only stores times in UTC and nothing else. The\\n               regular svn client converts the UTC time to the local time (or\\n               based on the TZ= environment). This command has the same\\n               behaviour.\\n           Any other arguments are passed directly to git log\\n\\n       blame\\n           Show what revision and author last modified each line of a file.\\n           The output of this mode is format-compatible with the output of\\n           ‘svn blame’ by default. Like the SVN blame command, local\\n           uncommitted changes in the working tree are ignored; the version of\\n           the file in the HEAD revision is annotated. Unknown arguments are\\n           passed directly to git blame.\\n\\n           --git-format\\n               Produce output in the same format as git blame, but with SVN\\n               revision numbers instead of Git commit hashes. In this mode,\\n               changes that haven’t been committed to SVN (including local\\n               working-copy edits) are shown as revision 0.\\n\\n       find-rev\\n           When given an SVN revision number of the form rN, returns the\\n           corresponding Git commit hash (this can optionally be followed by a\\n           tree-ish to specify which branch should be searched). When given a\\n           tree-ish, returns the corresponding SVN revision number.\\n\\n           -B, --before\\n               Don’t require an exact match if given an SVN revision, instead\\n               find the commit corresponding to the state of the SVN\\n               repository (on the current branch) at the specified revision.\\n\\n           -A, --after\\n               Don’t require an exact match if given an SVN revision; if there\\n               is not an exact match return the closest match searching\\n               forward in the history.\\n\\n       set-tree\\n           You should consider using dcommit instead of this command. Commit\\n           specified commit or tree objects to SVN. This relies on your\\n           imported fetch data being up to date. This makes absolutely no\\n           attempts to do patching when committing to SVN, it simply\\n           overwrites files with those specified in the tree or commit. All\\n           merging is assumed to have taken place independently of git svn\\n           functions.\\n\\n       create-ignore\\n           Recursively finds the svn:ignore property on directories and\\n           creates matching .gitignore files. The resulting files are staged\\n           to be committed, but are not committed. Use -r/--revision to refer\\n           to a specific revision.\\n\\n       show-ignore\\n           Recursively finds and lists the svn:ignore property on directories.\\n           The output is suitable for appending to the $GIT_DIR/info/exclude\\n           file.\\n\\n       mkdirs\\n           Attempts to recreate empty directories that core Git cannot track\\n           based on information in $GIT_DIR/svn/<refname>/unhandled.log files.\\n           Empty directories are automatically recreated when using \"git svn\\n           clone\" and \"git svn rebase\", so \"mkdirs\" is intended for use after\\n           commands like \"git checkout\" or \"git reset\". (See the\\n           svn-remote.<name>.automkdirs config file option for more\\n           information.)\\n\\n       commit-diff\\n           Commits the diff of two tree-ish arguments from the command-line.\\n           This command does not rely on being inside a git svn init-ed\\n           repository. This command takes three arguments, (a) the original\\n           tree to diff against, (b) the new tree result, (c) the URL of the\\n           target Subversion repository. The final argument (URL) may be\\n           omitted if you are working from a git svn-aware repository (that\\n           has been init-ed with git svn). The -r<revision> option is required\\n           for this.\\n\\n           The commit message is supplied either directly with the -m or -F\\n           option, or indirectly from the tag or commit when the second\\n           tree-ish denotes such an object, or it is requested by invoking an\\n           editor (see --edit option below).\\n\\n           -m <msg>, --message=<msg>\\n               Use the given msg as the commit message. This option disables\\n               the --edit option.\\n\\n           -F <filename>, --file=<filename>\\n               Take the commit message from the given file. This option\\n               disables the --edit option.\\n\\n       info\\n           Shows information about a file or directory similar to what ‘svn\\n           info’ provides. Does not currently support a -r/--revision\\n           argument. Use the --url option to output only the value of the URL:\\n           field.\\n\\n       proplist\\n           Lists the properties stored in the Subversion repository about a\\n           given file or directory. Use -r/--revision to refer to a specific\\n           Subversion revision.\\n\\n       propget\\n           Gets the Subversion property given as the first argument, for a\\n           file. A specific revision can be specified with -r/--revision.\\n\\n       propset\\n           Sets the Subversion property given as the first argument, to the\\n           value given as the second argument for the file given as the third\\n           argument.\\n\\n           Example:\\n\\n               git svn propset svn:keywords \"FreeBSD=%H\" devel/py-tipper/Makefile\\n\\n           This will set the property svn:keywords to FreeBSD=%H for the file\\n           devel/py-tipper/Makefile.\\n\\n       show-externals\\n           Shows the Subversion externals. Use -r/--revision to specify a\\n           specific revision.\\n\\n       gc\\n           Compress $GIT_DIR/svn/<refname>/unhandled.log files and remove\\n           $GIT_DIR/svn/<refname>/index files.\\n\\n       reset\\n           Undoes the effects of fetch back to the specified revision. This\\n           allows you to re-fetch an SVN revision. Normally the contents of an\\n           SVN revision should never change and reset should not be necessary.\\n           However, if SVN permissions change, or if you alter your\\n           --ignore-paths option, a fetch may fail with \"not found in commit\"\\n           (file not previously visible) or \"checksum mismatch\" (missed a\\n           modification). If the problem file cannot be ignored forever (with\\n           --ignore-paths) the only way to repair the repo is to use reset.\\n\\n           Only the rev_map and refs/remotes/git-svn are changed (see\\n           $GIT_DIR/svn/**/.rev_map.*  in the FILES section below for\\n           details). Follow reset with a fetch and then git reset or git\\n           rebase to move local branches onto the new tree.\\n\\n           -r <n>, --revision=<n>\\n               Specify the most recent revision to keep. All later revisions\\n               are discarded.\\n\\n           -p, --parent\\n               Discard the specified revision as well, keeping the nearest\\n               parent instead.\\n\\n           Example:\\n               Assume you have local changes in \"master\", but you need to\\n               refetch \"r2\".\\n\\n                       r1---r2---r3 remotes/git-svn\\n                                   \\\\\\n                                    A---B master\\n\\n               Fix the ignore-paths or SVN permissions problem that caused\\n               \"r2\" to be incomplete in the first place. Then:\\n\\n                   git svn reset -r2 -p\\n                   git svn fetch\\n\\n\\n\\n                       r1---r2\\'--r3\\' remotes/git-svn\\n                         \\\\\\n                          r2---r3---A---B master\\n\\n               Then fixup \"master\" with git rebase. Do NOT use git merge or\\n               your history will not be compatible with a future dcommit!\\n\\n                   git rebase --onto remotes/git-svn A^ master\\n\\n\\n\\n                       r1---r2\\'--r3\\' remotes/git-svn\\n                                   \\\\\\n                                    A\\'--B\\' master\\n<SECTION>OPTIONS</SECTION>\\n--shared[=(false|true|umask|group|all|world|everybody)],\\n       --template=<template-directory>\\n           Only used with the init command. These are passed directly to git\\n           init.\\n\\n       -r <arg>, --revision <arg>\\n           Used with the fetch command.\\n\\n           This allows revision ranges for partial/cauterized history to be\\n           supported. $NUMBER, $NUMBER1:$NUMBER2 (numeric ranges),\\n           $NUMBER:HEAD, and BASE:$NUMBER are all supported.\\n\\n           This can allow you to make partial mirrors when running fetch; but\\n           is generally not recommended because history will be skipped and\\n           lost.\\n\\n       -, --stdin\\n           Only used with the set-tree command.\\n\\n           Read a list of commits from stdin and commit them in reverse order.\\n           Only the leading sha1 is read from each line, so git rev-list\\n           --pretty=oneline output can be used.\\n\\n       --rmdir\\n           Only used with the dcommit, set-tree and commit-diff commands.\\n\\n           Remove directories from the SVN tree if there are no files left\\n           behind. SVN can version empty directories, and they are not removed\\n           by default if there are no files left in them. Git cannot version\\n           empty directories. Enabling this flag will make the commit to SVN\\n           act like Git.\\n\\n               config key: svn.rmdir\\n\\n\\n       -e, --edit\\n           Only used with the dcommit, set-tree and commit-diff commands.\\n\\n           Edit the commit message before committing to SVN. This is off by\\n           default for objects that are commits, and forced on when committing\\n           tree objects.\\n\\n               config key: svn.edit\\n\\n\\n       -l<num>, --find-copies-harder\\n           Only used with the dcommit, set-tree and commit-diff commands.\\n\\n           They are both passed directly to git diff-tree; see git-diff-\\n           tree(1) for more information.\\n\\n               config key: svn.l\\n               config key: svn.findcopiesharder\\n\\n\\n       -A<filename>, --authors-file=<filename>\\n           Syntax is compatible with the file used by git cvsimport but an\\n           empty email address can be supplied with <>:\\n\\n                       loginname = Joe User <user@example.com>\\n\\n           If this option is specified and git svn encounters an SVN committer\\n           name that does not exist in the authors-file, git svn will abort\\n           operation. The user will then have to add the appropriate entry.\\n           Re-running the previous git svn command after the authors-file is\\n           modified should continue operation.\\n\\n               config key: svn.authorsfile\\n\\n\\n       --authors-prog=<filename>\\n           If this option is specified, for each SVN committer name that does\\n           not exist in the authors file, the given file is executed with the\\n           committer name as the first argument. The program is expected to\\n           return a single line of the form \"Name <email>\" or \"Name <>\", which\\n           will be treated as if included in the authors file.\\n\\n           Due to historical reasons a relative filename is first searched\\n           relative to the current directory for init and clone and relative\\n           to the root of the working tree for fetch. If filename is not\\n           found, it is searched like any other command in $PATH.\\n\\n               config key: svn.authorsProg\\n\\n\\n       -q, --quiet\\n           Make git svn less verbose. Specify a second time to make it even\\n           less verbose.\\n\\n       -m, --merge, -s<strategy>, --strategy=<strategy>, -p, --rebase-merges\\n           These are only used with the dcommit and rebase commands.\\n\\n           Passed directly to git rebase when using dcommit if a git reset\\n           cannot be used (see dcommit).\\n\\n       -n, --dry-run\\n           This can be used with the dcommit, rebase, branch and tag commands.\\n\\n           For dcommit, print out the series of Git arguments that would show\\n           which diffs would be committed to SVN.\\n\\n           For rebase, display the local branch associated with the upstream\\n           svn repository associated with the current branch and the URL of\\n           svn repository that will be fetched from.\\n\\n           For branch and tag, display the urls that will be used for copying\\n           when creating the branch or tag.\\n\\n       --use-log-author\\n           When retrieving svn commits into Git (as part of fetch, rebase, or\\n           dcommit operations), look for the first From: line or Signed-off-by\\n           trailer in the log message and use that as the author string.\\n\\n               config key: svn.useLogAuthor\\n\\n\\n       --add-author-from\\n           When committing to svn from Git (as part of set-tree or dcommit\\n           operations), if the existing log message doesn’t already have a\\n           From: or Signed-off-by trailer, append a From: line based on the\\n           Git commit’s author string. If you use this, then --use-log-author\\n           will retrieve a valid author string for all commits.\\n\\n               config key: svn.addAuthorFrom\\n<SECTION>ADVANCED OPTIONS</SECTION>\\n-i<GIT_SVN_ID>, --id <GIT_SVN_ID>\\n           This sets GIT_SVN_ID (instead of using the environment). This\\n           allows the user to override the default refname to fetch from when\\n           tracking a single URL. The log and dcommit commands no longer\\n           require this switch as an argument.\\n\\n       -R<remote-name>, --svn-remote <remote-name>\\n           Specify the [svn-remote \"<remote-name>\"] section to use, this\\n           allows SVN multiple repositories to be tracked. Default: \"svn\"\\n\\n       --follow-parent\\n           This option is only relevant if we are tracking branches (using one\\n           of the repository layout options --trunk, --tags, --branches,\\n           --stdlayout). For each tracked branch, try to find out where its\\n           revision was copied from, and set a suitable parent in the first\\n           Git commit for the branch. This is especially helpful when we’re\\n           tracking a directory that has been moved around within the\\n           repository. If this feature is disabled, the branches created by\\n           git svn will all be linear and not share any history, meaning that\\n           there will be no information on where branches were branched off or\\n           merged. However, following long/convoluted histories can take a\\n           long time, so disabling this feature may speed up the cloning\\n           process. This feature is enabled by default, use --no-follow-parent\\n           to disable it.\\n\\n               config key: svn.followparent\\n\\n\\nCONFIG FILE-ONLY OPTIONS\\n       svn.noMetadata, svn-remote.<name>.noMetadata\\n           This gets rid of the git-svn-id: lines at the end of every commit.\\n\\n           This option can only be used for one-shot imports as git svn will\\n           not be able to fetch again without metadata. Additionally, if you\\n           lose your $GIT_DIR/svn/**/.rev_map.*  files, git svn will not be\\n           able to rebuild them.\\n\\n           The git svn log command will not work on repositories using this,\\n           either. Using this conflicts with the useSvmProps option for\\n           (hopefully) obvious reasons.\\n\\n           This option is NOT recommended as it makes it difficult to track\\n           down old references to SVN revision numbers in existing\\n           documentation, bug reports, and archives. If you plan to eventually\\n           migrate from SVN to Git and are certain about dropping SVN history,\\n           consider git-filter-repo[1] instead. filter-repo also allows\\n           reformatting of metadata for ease-of-reading and rewriting\\n           authorship info for non-\"svn.authorsFile\" users.\\n\\n       svn.useSvmProps, svn-remote.<name>.useSvmProps\\n           This allows git svn to re-map repository URLs and UUIDs from\\n           mirrors created using SVN::Mirror (or svk) for metadata.\\n\\n           If an SVN revision has a property, \"svm:headrev\", it is likely that\\n           the revision was created by SVN::Mirror (also used by SVK). The\\n           property contains a repository UUID and a revision. We want to make\\n           it look like we are mirroring the original URL, so introduce a\\n           helper function that returns the original identity URL and UUID,\\n           and use it when generating metadata in commit messages.\\n\\n       svn.useSvnsyncProps, svn-remote.<name>.useSvnsyncprops\\n           Similar to the useSvmProps option; this is for users of the\\n           svnsync(1) command distributed with SVN 1.4.x and later.\\n\\n       svn-remote.<name>.rewriteRoot\\n           This allows users to create repositories from alternate URLs. For\\n           example, an administrator could run git svn on the server locally\\n           (accessing via file://) but wish to distribute the repository with\\n           a public http:// or svn:// URL in the metadata so users of it will\\n           see the public URL.\\n\\n       svn-remote.<name>.rewriteUUID\\n           Similar to the useSvmProps option; this is for users who need to\\n           remap the UUID manually. This may be useful in situations where the\\n           original UUID is not available via either useSvmProps or\\n           useSvnsyncProps.\\n\\n       svn-remote.<name>.pushurl\\n           Similar to Git’s remote.<name>.pushurl, this key is designed to be\\n           used in cases where url points to an SVN repository via a read-only\\n           transport, to provide an alternate read/write transport. It is\\n           assumed that both keys point to the same repository. Unlike\\n           commiturl, pushurl is a base path. If either commiturl or pushurl\\n           could be used, commiturl takes precedence.\\n\\n       svn.brokenSymlinkWorkaround\\n           This disables potentially expensive checks to workaround broken\\n           symlinks checked into SVN by broken clients. Set this option to\\n           \"false\" if you track a SVN repository with many empty blobs that\\n           are not symlinks. This option may be changed while git svn is\\n           running and take effect on the next revision fetched. If unset, git\\n           svn assumes this option to be \"true\".\\n\\n       svn.pathnameencoding\\n           This instructs git svn to recode pathnames to a given encoding. It\\n           can be used by windows users and by those who work in non-utf8\\n           locales to avoid corrupted file names with non-ASCII characters.\\n           Valid encodings are the ones supported by Perl’s Encode module.\\n\\n       svn-remote.<name>.automkdirs\\n           Normally, the \"git svn clone\" and \"git svn rebase\" commands attempt\\n           to recreate empty directories that are in the Subversion\\n           repository. If this option is set to \"false\", then empty\\n           directories will only be created if the \"git svn mkdirs\" command is\\n           run explicitly. If unset, git svn assumes this option to be \"true\".\\n\\n       Since the noMetadata, rewriteRoot, rewriteUUID, useSvnsyncProps and\\n       useSvmProps options all affect the metadata generated and used by git\\n       svn; they must be set in the configuration file before any history is\\n       imported and these settings should never be changed once they are set.\\n\\n       Additionally, only one of these options can be used per svn-remote\\n       section because they affect the git-svn-id: metadata line, except for\\n       rewriteRoot and rewriteUUID which can be used together.\\n\\nBASIC EXAMPLES\\n       Tracking and contributing to the trunk of a Subversion-managed project\\n       (ignoring tags and branches):\\n\\n           # Clone a repo (like git clone):\\n                   git svn clone http://svn.example.com/project/trunk\\n           # Enter the newly cloned directory:\\n                   cd trunk\\n           # You should be on master branch, double-check with \\'git branch\\'\\n                   git branch\\n           # Do some work and commit locally to Git:\\n                   git commit ...\\n           # Something is committed to SVN, rebase your local changes against the\\n           # latest changes in SVN:\\n                   git svn rebase\\n           # Now commit your changes (that were committed previously using Git) to SVN,\\n           # as well as automatically updating your working HEAD:\\n                   git svn dcommit\\n           # Append svn:ignore settings to the default Git exclude file:\\n                   git svn show-ignore >> .git/info/exclude\\n\\n\\n       Tracking and contributing to an entire Subversion-managed project\\n       (complete with a trunk, tags and branches):\\n\\n           # Clone a repo with standard SVN directory layout (like git clone):\\n                   git svn clone http://svn.example.com/project --stdlayout --prefix svn/\\n           # Or, if the repo uses a non-standard directory layout:\\n                   git svn clone http://svn.example.com/project -T tr -b branch -t tag --prefix svn/\\n           # View all branches and tags you have cloned:\\n                   git branch -r\\n           # Create a new branch in SVN\\n                   git svn branch waldo\\n           # Reset your master to trunk (or any other branch, replacing \\'trunk\\'\\n           # with the appropriate name):\\n                   git reset --hard svn/trunk\\n           # You may only dcommit to one branch/tag/trunk at a time.  The usage\\n           # of dcommit/rebase/show-ignore should be the same as above.\\n\\n\\n       The initial git svn clone can be quite time-consuming (especially for\\n       large Subversion repositories). If multiple people (or one person with\\n       multiple machines) want to use git svn to interact with the same\\n       Subversion repository, you can do the initial git svn clone to a\\n       repository on a server and have each person clone that repository with\\n       git clone:\\n\\n           # Do the initial import on a server\\n                   ssh server \"cd /pub && git svn clone http://svn.example.com/project [options...]\"\\n           # Clone locally - make sure the refs/remotes/ space matches the server\\n                   mkdir project\\n                   cd project\\n                   git init\\n                   git remote add origin server:/pub/project\\n                   git config --replace-all remote.origin.fetch \\'+refs/remotes/*:refs/remotes/*\\'\\n                   git fetch\\n           # Prevent fetch/pull from remote Git server in the future,\\n           # we only want to use git svn for future updates\\n                   git config --remove-section remote.origin\\n           # Create a local branch from one of the branches just fetched\\n                   git checkout -b master FETCH_HEAD\\n           # Initialize \\'git svn\\' locally (be sure to use the same URL and\\n           # --stdlayout/-T/-b/-t/--prefix options as were used on server)\\n                   git svn init http://svn.example.com/project [options...]\\n           # Pull the latest changes from Subversion\\n                   git svn rebase\\n\\n\\nREBASE VS. PULL/MERGE\\n       Prefer to use git svn rebase or git rebase, rather than git pull or git\\n       merge to synchronize unintegrated commits with a git svn branch. Doing\\n       so will keep the history of unintegrated commits linear with respect to\\n       the upstream SVN repository and allow the use of the preferred git svn\\n       dcommit subcommand to push unintegrated commits back into SVN.\\n\\n       Originally, git svn recommended that developers pulled or merged from\\n       the git svn branch. This was because the author favored git svn\\n       set-tree B to commit a single head rather than the git svn set-tree\\n       A..B notation to commit multiple commits. Use of git pull or git merge\\n       with git svn set-tree A..B will cause non-linear history to be\\n       flattened when committing into SVN and this can lead to merge commits\\n       unexpectedly reversing previous commits in SVN.\\n\\nMERGE TRACKING\\n       While git svn can track copy history (including branches and tags) for\\n       repositories adopting a standard layout, it cannot yet represent merge\\n       history that happened inside git back upstream to SVN users. Therefore\\n       it is advised that users keep history as linear as possible inside Git\\n       to ease compatibility with SVN (see the CAVEATS section below).\\n\\nHANDLING OF SVN BRANCHES\\n       If git svn is configured to fetch branches (and --follow-branches is in\\n       effect), it sometimes creates multiple Git branches for one SVN branch,\\n       where the additional branches have names of the form branchname@nnn\\n       (with nnn an SVN revision number). These additional branches are\\n       created if git svn cannot find a parent commit for the first commit in\\n       an SVN branch, to connect the branch to the history of the other\\n       branches.\\n\\n       Normally, the first commit in an SVN branch consists of a copy\\n       operation. git svn will read this commit to get the SVN revision the\\n       branch was created from. It will then try to find the Git commit that\\n       corresponds to this SVN revision, and use that as the parent of the\\n       branch. However, it is possible that there is no suitable Git commit to\\n       serve as parent. This will happen, among other reasons, if the SVN\\n       branch is a copy of a revision that was not fetched by git svn (e.g.\\n       because it is an old revision that was skipped with --revision), or if\\n       in SVN a directory was copied that is not tracked by git svn (such as a\\n       branch that is not tracked at all, or a subdirectory of a tracked\\n       branch). In these cases, git svn will still create a Git branch, but\\n       instead of using an existing Git commit as the parent of the branch, it\\n       will read the SVN history of the directory the branch was copied from\\n       and create appropriate Git commits. This is indicated by the message\\n       \"Initializing parent: <branchname>\".\\n\\n       Additionally, it will create a special branch named\\n       <branchname>@<SVN-Revision>, where <SVN-Revision> is the SVN revision\\n       number the branch was copied from. This branch will point to the newly\\n       created parent commit of the branch. If in SVN the branch was deleted\\n       and later recreated from a different version, there will be multiple\\n       such branches with an @.\\n\\n       Note that this may mean that multiple Git commits are created for a\\n       single SVN revision.\\n\\n       An example: in an SVN repository with a standard trunk/tags/branches\\n       layout, a directory trunk/sub is created in r.100. In r.200, trunk/sub\\n       is branched by copying it to branches/. git svn clone -s will then\\n       create a branch sub. It will also create new Git commits for r.100\\n       through r.199 and use these as the history of branch sub. Thus there\\n       will be two Git commits for each revision from r.100 to r.199 (one\\n       containing trunk/, one containing trunk/sub/). Finally, it will create\\n       a branch sub@200 pointing to the new parent commit of branch sub (i.e.\\n       the commit for r.200 and trunk/sub/).\\n\\nCAVEATS\\n       For the sake of simplicity and interoperating with Subversion, it is\\n       recommended that all git svn users clone, fetch and dcommit directly\\n       from the SVN server, and avoid all git clone/pull/merge/push operations\\n       between Git repositories and branches. The recommended method of\\n       exchanging code between Git branches and users is git format-patch and\\n       git am, or just \\'dcommit’ing to the SVN repository.\\n\\n       Running git merge or git pull is NOT recommended on a branch you plan\\n       to dcommit from because Subversion users cannot see any merges you’ve\\n       made. Furthermore, if you merge or pull from a Git branch that is a\\n       mirror of an SVN branch, dcommit may commit to the wrong branch.\\n\\n       If you do merge, note the following rule: git svn dcommit will attempt\\n       to commit on top of the SVN commit named in\\n\\n           git log --grep=^git-svn-id: --first-parent -1\\n\\n\\n       You must therefore ensure that the most recent commit of the branch you\\n       want to dcommit to is the first parent of the merge. Chaos will ensue\\n       otherwise, especially if the first parent is an older commit on the\\n       same SVN branch.\\n\\n       git clone does not clone branches under the refs/remotes/ hierarchy or\\n       any git svn metadata, or config. So repositories created and managed\\n       with using git svn should use rsync for cloning, if cloning is to be\\n       done at all.\\n\\n       Since dcommit uses rebase internally, any Git branches you git push to\\n       before dcommit on will require forcing an overwrite of the existing ref\\n       on the remote repository. This is generally considered bad practice,\\n       see the git-push(1) documentation for details.\\n\\n       Do not use the --amend option of git-commit(1) on a change you’ve\\n       already dcommitted. It is considered bad practice to --amend commits\\n       you’ve already pushed to a remote repository for other users, and\\n       dcommit with SVN is analogous to that.\\n\\n       When cloning an SVN repository, if none of the options for describing\\n       the repository layout is used (--trunk, --tags, --branches,\\n       --stdlayout), git svn clone will create a Git repository with\\n       completely linear history, where branches and tags appear as separate\\n       directories in the working copy. While this is the easiest way to get a\\n       copy of a complete repository, for projects with many branches it will\\n       lead to a working copy many times larger than just the trunk. Thus for\\n       projects using the standard directory structure (trunk/branches/tags),\\n       it is recommended to clone with option --stdlayout. If the project uses\\n       a non-standard structure, and/or if branches and tags are not required,\\n       it is easiest to only clone one directory (typically trunk), without\\n       giving any repository layout options. If the full history with branches\\n       and tags is required, the options --trunk / --branches / --tags must be\\n       used.\\n\\n       When using multiple --branches or --tags, git svn does not\\n       automatically handle name collisions (for example, if two branches from\\n       different paths have the same name, or if a branch and a tag have the\\n       same name). In these cases, use init to set up your Git repository\\n       then, before your first fetch, edit the $GIT_DIR/config file so that\\n       the branches and tags are associated with different name spaces. For\\n       example:\\n\\n           branches = stable/*:refs/remotes/svn/stable/*\\n           branches = debug/*:refs/remotes/svn/debug/*\\n\\nCONFIGURATION\\n       git svn stores [svn-remote] configuration information in the repository\\n       $GIT_DIR/config file. It is similar the core Git [remote] sections\\n       except fetch keys do not accept glob arguments; but they are instead\\n       handled by the branches and tags keys. Since some SVN repositories are\\n       oddly configured with multiple projects glob expansions such those\\n       listed below are allowed:\\n\\n           [svn-remote \"project-a\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/project-a:refs/remotes/project-a/trunk\\n                   branches = branches/*/project-a:refs/remotes/project-a/branches/*\\n                   branches = branches/release_*:refs/remotes/project-a/branches/release_*\\n                   branches = branches/re*se:refs/remotes/project-a/branches/*\\n                   tags = tags/*/project-a:refs/remotes/project-a/tags/*\\n\\n\\n       Keep in mind that the * (asterisk) wildcard of the local ref (right of\\n       the :) must be the farthest right path component; however the remote\\n       wildcard may be anywhere as long as it’s an independent path component\\n       (surrounded by / or EOL). This type of configuration is not\\n       automatically created by init and should be manually entered with a\\n       text-editor or using git config.\\n\\n       Also note that only one asterisk is allowed per word. For example:\\n\\n           branches = branches/re*se:refs/remotes/project-a/branches/*\\n\\n       will match branches release, rese, re123se, however\\n\\n           branches = branches/re*s*e:refs/remotes/project-a/branches/*\\n\\n       will produce an error.\\n\\n       It is also possible to fetch a subset of branches or tags by using a\\n       comma-separated list of names within braces. For example:\\n\\n           [svn-remote \"huge-project\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/src:refs/remotes/trunk\\n                   branches = branches/{red,green}/src:refs/remotes/project-a/branches/*\\n                   tags = tags/{1.0,2.0}/src:refs/remotes/project-a/tags/*\\n\\n\\n       Multiple fetch, branches, and tags keys are supported:\\n\\n           [svn-remote \"messy-repo\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/project-a:refs/remotes/project-a/trunk\\n                   fetch = branches/demos/june-project-a-demo:refs/remotes/project-a/demos/june-demo\\n                   branches = branches/server/*:refs/remotes/project-a/branches/*\\n                   branches = branches/demos/2011/*:refs/remotes/project-a/2011-demos/*\\n                   tags = tags/server/*:refs/remotes/project-a/tags/*\\n\\n\\n       Creating a branch in such a configuration requires disambiguating which\\n       location to use using the -d or --destination flag:\\n\\n           $ git svn branch -d branches/server release-2-3-0\\n\\n\\n       Note that git-svn keeps track of the highest revision in which a branch\\n       or tag has appeared. If the subset of branches or tags is changed after\\n       fetching, then $GIT_DIR/svn/.metadata must be manually edited to remove\\n       (or reset) branches-maxRev and/or tags-maxRev as appropriate.\\n\\nFILES\\n       $GIT_DIR/svn/**/.rev_map.*\\n           Mapping between Subversion revision numbers and Git commit names.\\n           In a repository where the noMetadata option is not set, this can be\\n           rebuilt from the git-svn-id: lines that are at the end of every\\n           commit (see the svn.noMetadata section above for details).\\n\\n           git svn fetch and git svn rebase automatically update the rev_map\\n           if it is missing or not up to date.  git svn reset automatically\\n           rewinds it.\\n\\nBUGS\\n       We ignore all SVN properties except svn:executable. Any unhandled\\n       properties are logged to $GIT_DIR/svn/<refname>/unhandled.log\\n\\n       Renamed and copied directories are not detected by Git and hence not\\n       tracked when committing to SVN. I do not plan on adding support for\\n       this as it’s quite difficult and time-consuming to get working for all\\n       the possible corner cases (Git doesn’t do it, either). Committing\\n       renamed and copied files is fully supported if they’re similar enough\\n       for Git to detect them.\\n\\n       In SVN, it is possible (though discouraged) to commit changes to a tag\\n       (because a tag is just a directory copy, thus technically the same as a\\n       branch). When cloning an SVN repository, git svn cannot know if such a\\n       commit to a tag will happen in the future. Thus it acts conservatively\\n       and imports all SVN tags as branches, prefixing the tag name with\\n       tags/.\\n\\nSEE ALSO\\n       git-rebase(1)\\n\\nGIT\\n       Part of the git(1) suite\\n\\nNOTES\\n        1. git-filter-repo\\n           https://github.com/newren/git-filter-repo\\n\\nGit 2.46.1                        09/14/2024                        GIT-SVN(1)\\n<SECTION>BASIC EXAMPLES</SECTION>\\nTracking and contributing to the trunk of a Subversion-managed project\\n       (ignoring tags and branches):\\n\\n           # Clone a repo (like git clone):\\n                   git svn clone http://svn.example.com/project/trunk\\n           # Enter the newly cloned directory:\\n                   cd trunk\\n           # You should be on master branch, double-check with \\'git branch\\'\\n                   git branch\\n           # Do some work and commit locally to Git:\\n                   git commit ...\\n           # Something is committed to SVN, rebase your local changes against the\\n           # latest changes in SVN:\\n                   git svn rebase\\n           # Now commit your changes (that were committed previously using Git) to SVN,\\n           # as well as automatically updating your working HEAD:\\n                   git svn dcommit\\n           # Append svn:ignore settings to the default Git exclude file:\\n                   git svn show-ignore >> .git/info/exclude\\n\\n\\n       Tracking and contributing to an entire Subversion-managed project\\n       (complete with a trunk, tags and branches):\\n\\n           # Clone a repo with standard SVN directory layout (like git clone):\\n                   git svn clone http://svn.example.com/project --stdlayout --prefix svn/\\n           # Or, if the repo uses a non-standard directory layout:\\n                   git svn clone http://svn.example.com/project -T tr -b branch -t tag --prefix svn/\\n           # View all branches and tags you have cloned:\\n                   git branch -r\\n           # Create a new branch in SVN\\n                   git svn branch waldo\\n           # Reset your master to trunk (or any other branch, replacing \\'trunk\\'\\n           # with the appropriate name):\\n                   git reset --hard svn/trunk\\n           # You may only dcommit to one branch/tag/trunk at a time.  The usage\\n           # of dcommit/rebase/show-ignore should be the same as above.\\n\\n\\n       The initial git svn clone can be quite time-consuming (especially for\\n       large Subversion repositories). If multiple people (or one person with\\n       multiple machines) want to use git svn to interact with the same\\n       Subversion repository, you can do the initial git svn clone to a\\n       repository on a server and have each person clone that repository with\\n       git clone:\\n\\n           # Do the initial import on a server\\n                   ssh server \"cd /pub && git svn clone http://svn.example.com/project [options...]\"\\n           # Clone locally - make sure the refs/remotes/ space matches the server\\n                   mkdir project\\n                   cd project\\n                   git init\\n                   git remote add origin server:/pub/project\\n                   git config --replace-all remote.origin.fetch \\'+refs/remotes/*:refs/remotes/*\\'\\n                   git fetch\\n           # Prevent fetch/pull from remote Git server in the future,\\n           # we only want to use git svn for future updates\\n                   git config --remove-section remote.origin\\n           # Create a local branch from one of the branches just fetched\\n                   git checkout -b master FETCH_HEAD\\n           # Initialize \\'git svn\\' locally (be sure to use the same URL and\\n           # --stdlayout/-T/-b/-t/--prefix options as were used on server)\\n                   git svn init http://svn.example.com/project [options...]\\n           # Pull the latest changes from Subversion\\n                   git svn rebase\\n\\n\\nREBASE VS. PULL/MERGE\\n       Prefer to use git svn rebase or git rebase, rather than git pull or git\\n       merge to synchronize unintegrated commits with a git svn branch. Doing\\n       so will keep the history of unintegrated commits linear with respect to\\n       the upstream SVN repository and allow the use of the preferred git svn\\n       dcommit subcommand to push unintegrated commits back into SVN.\\n\\n       Originally, git svn recommended that developers pulled or merged from\\n       the git svn branch. This was because the author favored git svn\\n       set-tree B to commit a single head rather than the git svn set-tree\\n       A..B notation to commit multiple commits. Use of git pull or git merge\\n       with git svn set-tree A..B will cause non-linear history to be\\n       flattened when committing into SVN and this can lead to merge commits\\n       unexpectedly reversing previous commits in SVN.\\n\\nMERGE TRACKING\\n       While git svn can track copy history (including branches and tags) for\\n       repositories adopting a standard layout, it cannot yet represent merge\\n       history that happened inside git back upstream to SVN users. Therefore\\n       it is advised that users keep history as linear as possible inside Git\\n       to ease compatibility with SVN (see the CAVEATS section below).\\n\\nHANDLING OF SVN BRANCHES\\n       If git svn is configured to fetch branches (and --follow-branches is in\\n       effect), it sometimes creates multiple Git branches for one SVN branch,\\n       where the additional branches have names of the form branchname@nnn\\n       (with nnn an SVN revision number). These additional branches are\\n       created if git svn cannot find a parent commit for the first commit in\\n       an SVN branch, to connect the branch to the history of the other\\n       branches.\\n\\n       Normally, the first commit in an SVN branch consists of a copy\\n       operation. git svn will read this commit to get the SVN revision the\\n       branch was created from. It will then try to find the Git commit that\\n       corresponds to this SVN revision, and use that as the parent of the\\n       branch. However, it is possible that there is no suitable Git commit to\\n       serve as parent. This will happen, among other reasons, if the SVN\\n       branch is a copy of a revision that was not fetched by git svn (e.g.\\n       because it is an old revision that was skipped with --revision), or if\\n       in SVN a directory was copied that is not tracked by git svn (such as a\\n       branch that is not tracked at all, or a subdirectory of a tracked\\n       branch). In these cases, git svn will still create a Git branch, but\\n       instead of using an existing Git commit as the parent of the branch, it\\n       will read the SVN history of the directory the branch was copied from\\n       and create appropriate Git commits. This is indicated by the message\\n       \"Initializing parent: <branchname>\".\\n\\n       Additionally, it will create a special branch named\\n       <branchname>@<SVN-Revision>, where <SVN-Revision> is the SVN revision\\n       number the branch was copied from. This branch will point to the newly\\n       created parent commit of the branch. If in SVN the branch was deleted\\n       and later recreated from a different version, there will be multiple\\n       such branches with an @.\\n\\n       Note that this may mean that multiple Git commits are created for a\\n       single SVN revision.\\n\\n       An example: in an SVN repository with a standard trunk/tags/branches\\n       layout, a directory trunk/sub is created in r.100. In r.200, trunk/sub\\n       is branched by copying it to branches/. git svn clone -s will then\\n       create a branch sub. It will also create new Git commits for r.100\\n       through r.199 and use these as the history of branch sub. Thus there\\n       will be two Git commits for each revision from r.100 to r.199 (one\\n       containing trunk/, one containing trunk/sub/). Finally, it will create\\n       a branch sub@200 pointing to the new parent commit of branch sub (i.e.\\n       the commit for r.200 and trunk/sub/).\\n\\nCAVEATS\\n       For the sake of simplicity and interoperating with Subversion, it is\\n       recommended that all git svn users clone, fetch and dcommit directly\\n       from the SVN server, and avoid all git clone/pull/merge/push operations\\n       between Git repositories and branches. The recommended method of\\n       exchanging code between Git branches and users is git format-patch and\\n       git am, or just \\'dcommit’ing to the SVN repository.\\n\\n       Running git merge or git pull is NOT recommended on a branch you plan\\n       to dcommit from because Subversion users cannot see any merges you’ve\\n       made. Furthermore, if you merge or pull from a Git branch that is a\\n       mirror of an SVN branch, dcommit may commit to the wrong branch.\\n\\n       If you do merge, note the following rule: git svn dcommit will attempt\\n       to commit on top of the SVN commit named in\\n\\n           git log --grep=^git-svn-id: --first-parent -1\\n\\n\\n       You must therefore ensure that the most recent commit of the branch you\\n       want to dcommit to is the first parent of the merge. Chaos will ensue\\n       otherwise, especially if the first parent is an older commit on the\\n       same SVN branch.\\n\\n       git clone does not clone branches under the refs/remotes/ hierarchy or\\n       any git svn metadata, or config. So repositories created and managed\\n       with using git svn should use rsync for cloning, if cloning is to be\\n       done at all.\\n\\n       Since dcommit uses rebase internally, any Git branches you git push to\\n       before dcommit on will require forcing an overwrite of the existing ref\\n       on the remote repository. This is generally considered bad practice,\\n       see the git-push(1) documentation for details.\\n\\n       Do not use the --amend option of git-commit(1) on a change you’ve\\n       already dcommitted. It is considered bad practice to --amend commits\\n       you’ve already pushed to a remote repository for other users, and\\n       dcommit with SVN is analogous to that.\\n\\n       When cloning an SVN repository, if none of the options for describing\\n       the repository layout is used (--trunk, --tags, --branches,\\n       --stdlayout), git svn clone will create a Git repository with\\n       completely linear history, where branches and tags appear as separate\\n       directories in the working copy. While this is the easiest way to get a\\n       copy of a complete repository, for projects with many branches it will\\n       lead to a working copy many times larger than just the trunk. Thus for\\n       projects using the standard directory structure (trunk/branches/tags),\\n       it is recommended to clone with option --stdlayout. If the project uses\\n       a non-standard structure, and/or if branches and tags are not required,\\n       it is easiest to only clone one directory (typically trunk), without\\n       giving any repository layout options. If the full history with branches\\n       and tags is required, the options --trunk / --branches / --tags must be\\n       used.\\n\\n       When using multiple --branches or --tags, git svn does not\\n       automatically handle name collisions (for example, if two branches from\\n       different paths have the same name, or if a branch and a tag have the\\n       same name). In these cases, use init to set up your Git repository\\n       then, before your first fetch, edit the $GIT_DIR/config file so that\\n       the branches and tags are associated with different name spaces. For\\n       example:\\n\\n           branches = stable/*:refs/remotes/svn/stable/*\\n           branches = debug/*:refs/remotes/svn/debug/*\\n\\nCONFIGURATION\\n       git svn stores [svn-remote] configuration information in the repository\\n       $GIT_DIR/config file. It is similar the core Git [remote] sections\\n       except fetch keys do not accept glob arguments; but they are instead\\n       handled by the branches and tags keys. Since some SVN repositories are\\n       oddly configured with multiple projects glob expansions such those\\n       listed below are allowed:\\n\\n           [svn-remote \"project-a\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/project-a:refs/remotes/project-a/trunk\\n                   branches = branches/*/project-a:refs/remotes/project-a/branches/*\\n                   branches = branches/release_*:refs/remotes/project-a/branches/release_*\\n                   branches = branches/re*se:refs/remotes/project-a/branches/*\\n                   tags = tags/*/project-a:refs/remotes/project-a/tags/*\\n\\n\\n       Keep in mind that the * (asterisk) wildcard of the local ref (right of\\n       the :) must be the farthest right path component; however the remote\\n       wildcard may be anywhere as long as it’s an independent path component\\n       (surrounded by / or EOL). This type of configuration is not\\n       automatically created by init and should be manually entered with a\\n       text-editor or using git config.\\n\\n       Also note that only one asterisk is allowed per word. For example:\\n\\n           branches = branches/re*se:refs/remotes/project-a/branches/*\\n\\n       will match branches release, rese, re123se, however\\n\\n           branches = branches/re*s*e:refs/remotes/project-a/branches/*\\n\\n       will produce an error.\\n\\n       It is also possible to fetch a subset of branches or tags by using a\\n       comma-separated list of names within braces. For example:\\n\\n           [svn-remote \"huge-project\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/src:refs/remotes/trunk\\n                   branches = branches/{red,green}/src:refs/remotes/project-a/branches/*\\n                   tags = tags/{1.0,2.0}/src:refs/remotes/project-a/tags/*\\n\\n\\n       Multiple fetch, branches, and tags keys are supported:\\n\\n           [svn-remote \"messy-repo\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/project-a:refs/remotes/project-a/trunk\\n                   fetch = branches/demos/june-project-a-demo:refs/remotes/project-a/demos/june-demo\\n                   branches = branches/server/*:refs/remotes/project-a/branches/*\\n                   branches = branches/demos/2011/*:refs/remotes/project-a/2011-demos/*\\n                   tags = tags/server/*:refs/remotes/project-a/tags/*\\n\\n\\n       Creating a branch in such a configuration requires disambiguating which\\n       location to use using the -d or --destination flag:\\n\\n           $ git svn branch -d branches/server release-2-3-0\\n\\n\\n       Note that git-svn keeps track of the highest revision in which a branch\\n       or tag has appeared. If the subset of branches or tags is changed after\\n       fetching, then $GIT_DIR/svn/.metadata must be manually edited to remove\\n       (or reset) branches-maxRev and/or tags-maxRev as appropriate.\\n\\nFILES\\n       $GIT_DIR/svn/**/.rev_map.*\\n           Mapping between Subversion revision numbers and Git commit names.\\n           In a repository where the noMetadata option is not set, this can be\\n           rebuilt from the git-svn-id: lines that are at the end of every\\n           commit (see the svn.noMetadata section above for details).\\n\\n           git svn fetch and git svn rebase automatically update the rev_map\\n           if it is missing or not up to date.  git svn reset automatically\\n           rewinds it.\\n\\nBUGS\\n       We ignore all SVN properties except svn:executable. Any unhandled\\n       properties are logged to $GIT_DIR/svn/<refname>/unhandled.log\\n\\n       Renamed and copied directories are not detected by Git and hence not\\n       tracked when committing to SVN. I do not plan on adding support for\\n       this as it’s quite difficult and time-consuming to get working for all\\n       the possible corner cases (Git doesn’t do it, either). Committing\\n       renamed and copied files is fully supported if they’re similar enough\\n       for Git to detect them.\\n\\n       In SVN, it is possible (though discouraged) to commit changes to a tag\\n       (because a tag is just a directory copy, thus technically the same as a\\n       branch). When cloning an SVN repository, git svn cannot know if such a\\n       commit to a tag will happen in the future. Thus it acts conservatively\\n       and imports all SVN tags as branches, prefixing the tag name with\\n       tags/.\\n\\nSEE ALSO\\n       git-rebase(1)\\n\\nGIT\\n       Part of the git(1) suite\\n\\nNOTES\\n        1. git-filter-repo\\n           https://github.com/newren/git-filter-repo\\n\\nGit 2.46.1                        09/14/2024                        GIT-SVN(1)\\n<SECTION>MERGE TRACKING</SECTION>\\nWhile git svn can track copy history (including branches and tags) for\\n       repositories adopting a standard layout, it cannot yet represent merge\\n       history that happened inside git back upstream to SVN users. Therefore\\n       it is advised that users keep history as linear as possible inside Git\\n       to ease compatibility with SVN (see the CAVEATS section below).\\n<SECTION>CAVEATS</SECTION>\\nsection below).\\n\\nHANDLING OF SVN BRANCHES\\n       If git svn is configured to fetch branches (and --follow-branches is in\\n       effect), it sometimes creates multiple Git branches for one SVN branch,\\n       where the additional branches have names of the form branchname@nnn\\n       (with nnn an SVN revision number). These additional branches are\\n       created if git svn cannot find a parent commit for the first commit in\\n       an SVN branch, to connect the branch to the history of the other\\n       branches.\\n\\n       Normally, the first commit in an SVN branch consists of a copy\\n       operation. git svn will read this commit to get the SVN revision the\\n       branch was created from. It will then try to find the Git commit that\\n       corresponds to this SVN revision, and use that as the parent of the\\n       branch. However, it is possible that there is no suitable Git commit to\\n       serve as parent. This will happen, among other reasons, if the SVN\\n       branch is a copy of a revision that was not fetched by git svn (e.g.\\n       because it is an old revision that was skipped with --revision), or if\\n       in SVN a directory was copied that is not tracked by git svn (such as a\\n       branch that is not tracked at all, or a subdirectory of a tracked\\n       branch). In these cases, git svn will still create a Git branch, but\\n       instead of using an existing Git commit as the parent of the branch, it\\n       will read the SVN history of the directory the branch was copied from\\n       and create appropriate Git commits. This is indicated by the message\\n       \"Initializing parent: <branchname>\".\\n\\n       Additionally, it will create a special branch named\\n       <branchname>@<SVN-Revision>, where <SVN-Revision> is the SVN revision\\n       number the branch was copied from. This branch will point to the newly\\n       created parent commit of the branch. If in SVN the branch was deleted\\n       and later recreated from a different version, there will be multiple\\n       such branches with an @.\\n\\n       Note that this may mean that multiple Git commits are created for a\\n       single SVN revision.\\n\\n       An example: in an SVN repository with a standard trunk/tags/branches\\n       layout, a directory trunk/sub is created in r.100. In r.200, trunk/sub\\n       is branched by copying it to branches/. git svn clone -s will then\\n       create a branch sub. It will also create new Git commits for r.100\\n       through r.199 and use these as the history of branch sub. Thus there\\n       will be two Git commits for each revision from r.100 to r.199 (one\\n       containing trunk/, one containing trunk/sub/). Finally, it will create\\n       a branch sub@200 pointing to the new parent commit of branch sub (i.e.\\n       the commit for r.200 and trunk/sub/).\\n\\nCAVEATS\\n       For the sake of simplicity and interoperating with Subversion, it is\\n       recommended that all git svn users clone, fetch and dcommit directly\\n       from the SVN server, and avoid all git clone/pull/merge/push operations\\n       between Git repositories and branches. The recommended method of\\n       exchanging code between Git branches and users is git format-patch and\\n       git am, or just \\'dcommit’ing to the SVN repository.\\n\\n       Running git merge or git pull is NOT recommended on a branch you plan\\n       to dcommit from because Subversion users cannot see any merges you’ve\\n       made. Furthermore, if you merge or pull from a Git branch that is a\\n       mirror of an SVN branch, dcommit may commit to the wrong branch.\\n\\n       If you do merge, note the following rule: git svn dcommit will attempt\\n       to commit on top of the SVN commit named in\\n\\n           git log --grep=^git-svn-id: --first-parent -1\\n\\n\\n       You must therefore ensure that the most recent commit of the branch you\\n       want to dcommit to is the first parent of the merge. Chaos will ensue\\n       otherwise, especially if the first parent is an older commit on the\\n       same SVN branch.\\n\\n       git clone does not clone branches under the refs/remotes/ hierarchy or\\n       any git svn metadata, or config. So repositories created and managed\\n       with using git svn should use rsync for cloning, if cloning is to be\\n       done at all.\\n\\n       Since dcommit uses rebase internally, any Git branches you git push to\\n       before dcommit on will require forcing an overwrite of the existing ref\\n       on the remote repository. This is generally considered bad practice,\\n       see the git-push(1) documentation for details.\\n\\n       Do not use the --amend option of git-commit(1) on a change you’ve\\n       already dcommitted. It is considered bad practice to --amend commits\\n       you’ve already pushed to a remote repository for other users, and\\n       dcommit with SVN is analogous to that.\\n\\n       When cloning an SVN repository, if none of the options for describing\\n       the repository layout is used (--trunk, --tags, --branches,\\n       --stdlayout), git svn clone will create a Git repository with\\n       completely linear history, where branches and tags appear as separate\\n       directories in the working copy. While this is the easiest way to get a\\n       copy of a complete repository, for projects with many branches it will\\n       lead to a working copy many times larger than just the trunk. Thus for\\n       projects using the standard directory structure (trunk/branches/tags),\\n       it is recommended to clone with option --stdlayout. If the project uses\\n       a non-standard structure, and/or if branches and tags are not required,\\n       it is easiest to only clone one directory (typically trunk), without\\n       giving any repository layout options. If the full history with branches\\n       and tags is required, the options --trunk / --branches / --tags must be\\n       used.\\n\\n       When using multiple --branches or --tags, git svn does not\\n       automatically handle name collisions (for example, if two branches from\\n       different paths have the same name, or if a branch and a tag have the\\n       same name). In these cases, use init to set up your Git repository\\n       then, before your first fetch, edit the $GIT_DIR/config file so that\\n       the branches and tags are associated with different name spaces. For\\n       example:\\n\\n           branches = stable/*:refs/remotes/svn/stable/*\\n           branches = debug/*:refs/remotes/svn/debug/*\\n<SECTION>CONFIGURATION</SECTION>\\ngit svn stores [svn-remote] configuration information in the repository\\n       $GIT_DIR/config file. It is similar the core Git [remote] sections\\n       except fetch keys do not accept glob arguments; but they are instead\\n       handled by the branches and tags keys. Since some SVN repositories are\\n       oddly configured with multiple projects glob expansions such those\\n       listed below are allowed:\\n\\n           [svn-remote \"project-a\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/project-a:refs/remotes/project-a/trunk\\n                   branches = branches/*/project-a:refs/remotes/project-a/branches/*\\n                   branches = branches/release_*:refs/remotes/project-a/branches/release_*\\n                   branches = branches/re*se:refs/remotes/project-a/branches/*\\n                   tags = tags/*/project-a:refs/remotes/project-a/tags/*\\n\\n\\n       Keep in mind that the * (asterisk) wildcard of the local ref (right of\\n       the :) must be the farthest right path component; however the remote\\n       wildcard may be anywhere as long as it’s an independent path component\\n       (surrounded by / or EOL). This type of configuration is not\\n       automatically created by init and should be manually entered with a\\n       text-editor or using git config.\\n\\n       Also note that only one asterisk is allowed per word. For example:\\n\\n           branches = branches/re*se:refs/remotes/project-a/branches/*\\n\\n       will match branches release, rese, re123se, however\\n\\n           branches = branches/re*s*e:refs/remotes/project-a/branches/*\\n\\n       will produce an error.\\n\\n       It is also possible to fetch a subset of branches or tags by using a\\n       comma-separated list of names within braces. For example:\\n\\n           [svn-remote \"huge-project\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/src:refs/remotes/trunk\\n                   branches = branches/{red,green}/src:refs/remotes/project-a/branches/*\\n                   tags = tags/{1.0,2.0}/src:refs/remotes/project-a/tags/*\\n\\n\\n       Multiple fetch, branches, and tags keys are supported:\\n\\n           [svn-remote \"messy-repo\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/project-a:refs/remotes/project-a/trunk\\n                   fetch = branches/demos/june-project-a-demo:refs/remotes/project-a/demos/june-demo\\n                   branches = branches/server/*:refs/remotes/project-a/branches/*\\n                   branches = branches/demos/2011/*:refs/remotes/project-a/2011-demos/*\\n                   tags = tags/server/*:refs/remotes/project-a/tags/*\\n\\n\\n       Creating a branch in such a configuration requires disambiguating which\\n       location to use using the -d or --destination flag:\\n\\n           $ git svn branch -d branches/server release-2-3-0\\n\\n\\n       Note that git-svn keeps track of the highest revision in which a branch\\n       or tag has appeared. If the subset of branches or tags is changed after\\n       fetching, then $GIT_DIR/svn/.metadata must be manually edited to remove\\n       (or reset) branches-maxRev and/or tags-maxRev as appropriate.\\n<SECTION>FILES</SECTION>\\nsection below for\\n           details).\\n\\n           --localtime\\n               Store Git commit times in the local time zone instead of UTC.\\n               This makes git log (even without --date=local) show the same\\n               times that svn log would in the local time zone.\\n\\n               This doesn’t interfere with interoperating with the Subversion\\n               repository you cloned from, but if you wish for your local Git\\n               repository to be able to interoperate with someone else’s local\\n               Git repository, either don’t use this option or you should both\\n               use it in the same local time zone.\\n\\n           --parent\\n               Fetch only from the SVN parent of the current HEAD.\\n\\n           --ignore-refs=<regex>\\n               Ignore refs for branches or tags matching the Perl regular\\n               expression. A \"negative look-ahead assertion\" like\\n               ^refs/remotes/origin/(?!tags/wanted-tag|wanted-branch).*$ can\\n               be used to allow only certain refs.\\n\\n                   config key: svn-remote.<name>.ignore-refs\\n\\n               If the ignore-refs configuration key is set, and the\\n               command-line option is also given, both regular expressions\\n               will be used.\\n\\n           --ignore-paths=<regex>\\n               This allows one to specify a Perl regular expression that will\\n               cause skipping of all matching paths from checkout from SVN.\\n               The --ignore-paths option should match for every fetch\\n               (including automatic fetches due to clone, dcommit, rebase,\\n               etc) on a given repository.\\n\\n                   config key: svn-remote.<name>.ignore-paths\\n\\n               If the ignore-paths configuration key is set, and the\\n               command-line option is also given, both regular expressions\\n               will be used.\\n\\n               Examples:\\n\\n               Skip \"doc*\" directory for every fetch\\n\\n                       --ignore-paths=\"^doc\"\\n\\n\\n               Skip \"branches\" and \"tags\" of first level directories\\n\\n                       --ignore-paths=\"^[^/]+/(?:branches|tags)\"\\n\\n\\n           --include-paths=<regex>\\n               This allows one to specify a Perl regular expression that will\\n               cause the inclusion of only matching paths from checkout from\\n               SVN. The --include-paths option should match for every fetch\\n               (including automatic fetches due to clone, dcommit, rebase,\\n               etc) on a given repository.  --ignore-paths takes precedence\\n               over --include-paths.\\n\\n                   config key: svn-remote.<name>.include-paths\\n\\n\\n           --log-window-size=<n>\\n               Fetch <n> log entries per request when scanning Subversion\\n               history. The default is 100. For very large Subversion\\n               repositories, larger values may be needed for clone/fetch to\\n               complete in reasonable time. But overly large values may lead\\n               to higher memory usage and request timeouts.\\n\\n       clone\\n           Runs init and fetch. It will automatically create a directory based\\n           on the basename of the URL passed to it; or if a second argument is\\n           passed; it will create a directory and work within that. It accepts\\n           all arguments that the init and fetch commands accept; with the\\n           exception of --fetch-all and --parent. After a repository is\\n           cloned, the fetch command will be able to update revisions without\\n           affecting the working tree; and the rebase command will be able to\\n           update the working tree with the latest changes.\\n\\n           --preserve-empty-dirs\\n               Create a placeholder file in the local Git repository for each\\n               empty directory fetched from Subversion. This includes\\n               directories that become empty by removing all entries in the\\n               Subversion repository (but not the directory itself). The\\n               placeholder files are also tracked and removed when no longer\\n               necessary.\\n\\n           --placeholder-filename=<filename>\\n               Set the name of placeholder files created by\\n               --preserve-empty-dirs. Default: \".gitignore\"\\n\\n       rebase\\n           This fetches revisions from the SVN parent of the current HEAD and\\n           rebases the current (uncommitted to SVN) work against it.\\n\\n           This works similarly to svn update or git pull except that it\\n           preserves linear history with git rebase instead of git merge for\\n           ease of dcommitting with git svn.\\n\\n           This accepts all options that git svn fetch and git rebase accept.\\n           However, --fetch-all only fetches from the current [svn-remote],\\n           and not all [svn-remote] definitions.\\n\\n           Like git rebase; this requires that the working tree be clean and\\n           have no uncommitted changes.\\n\\n           This automatically updates the rev_map if needed (see\\n           $GIT_DIR/svn/**/.rev_map.*  in the FILES section below for\\n           details).\\n\\n           -l, --local\\n               Do not fetch remotely; only run git rebase against the last\\n               fetched commit from the upstream SVN.\\n\\n       dcommit\\n           Commit each diff from the current branch directly to the SVN\\n           repository, and then rebase or reset (depending on whether or not\\n           there is a diff between SVN and head). This will create a revision\\n           in SVN for each commit in Git.\\n\\n           When an optional Git branch name (or a Git commit object name) is\\n           specified as an argument, the subcommand works on the specified\\n           branch, not on the current branch.\\n\\n           Use of dcommit is preferred to set-tree (below).\\n\\n           --no-rebase\\n               After committing, do not rebase or reset.\\n\\n           --commit-url <URL>\\n               Commit to this SVN URL (the full path). This is intended to\\n               allow existing git svn repositories created with one transport\\n               method (e.g.  svn:// or http:// for anonymous read) to be\\n               reused if a user is later given access to an alternate\\n               transport method (e.g.  svn+ssh:// or https://) for commit.\\n\\n                   config key: svn-remote.<name>.commiturl\\n                   config key: svn.commiturl (overwrites all svn-remote.<name>.commiturl options)\\n\\n               Note that the SVN URL of the commiturl config key includes the\\n               SVN branch. If you rather want to set the commit URL for an\\n               entire SVN repository use svn-remote.<name>.pushurl instead.\\n\\n               Using this option for any other purpose (don’t ask) is very\\n               strongly discouraged.\\n\\n           --mergeinfo=<mergeinfo>\\n               Add the given merge information during the dcommit (e.g.\\n               --mergeinfo=\"/branches/foo:1-10\"). All svn server versions can\\n               store this information (as a property), and svn clients\\n               starting from version 1.5 can make use of it. To specify merge\\n               information from multiple branches, use a single space\\n               character between the branches (--mergeinfo=\"/branches/foo:1-10\\n               /branches/bar:3,5-6,8\")\\n\\n                   config key: svn.pushmergeinfo\\n\\n               This option will cause git-svn to attempt to automatically\\n               populate the svn:mergeinfo property in the SVN repository when\\n               possible. Currently, this can only be done when dcommitting\\n               non-fast-forward merges where all parents but the first have\\n               already been pushed into SVN.\\n\\n           --interactive\\n               Ask the user to confirm that a patch set should actually be\\n               sent to SVN. For each patch, one may answer \"yes\" (accept this\\n               patch), \"no\" (discard this patch), \"all\" (accept all patches),\\n               or \"quit\".\\n\\n               git svn dcommit returns immediately if answer is \"no\" or\\n               \"quit\", without committing anything to SVN.\\n\\n       branch\\n           Create a branch in the SVN repository.\\n\\n           -m, --message\\n               Allows to specify the commit message.\\n\\n           -t, --tag\\n               Create a tag by using the tags_subdir instead of the\\n               branches_subdir specified during git svn init.\\n\\n           -d<path>, --destination=<path>\\n               If more than one --branches (or --tags) option was given to the\\n               init or clone command, you must provide the location of the\\n               branch (or tag) you wish to create in the SVN repository.\\n               <path> specifies which path to use to create the branch or tag\\n               and should match the pattern on the left-hand side of one of\\n               the configured branches or tags refspecs. You can see these\\n               refspecs with the commands\\n\\n                   git config --get-all svn-remote.<name>.branches\\n                   git config --get-all svn-remote.<name>.tags\\n\\n               where <name> is the name of the SVN repository as specified by\\n               the -R option to init (or \"svn\" by default).\\n\\n           --username\\n               Specify the SVN username to perform the commit as. This option\\n               overrides the username configuration property.\\n\\n           --commit-url\\n               Use the specified URL to connect to the destination Subversion\\n               repository. This is useful in cases where the source SVN\\n               repository is read-only. This option overrides configuration\\n               property commiturl.\\n\\n                   git config --get-all svn-remote.<name>.commiturl\\n\\n           --parents\\n               Create parent folders. This parameter is equivalent to the\\n               parameter --parents on svn cp commands and is useful for\\n               non-standard repository layouts.\\n\\n       tag\\n           Create a tag in the SVN repository. This is a shorthand for branch\\n           -t.\\n\\n       log\\n           This should make it easy to look up svn log messages when svn users\\n           refer to -r/--revision numbers.\\n\\n           The following features from ‘svn log’ are supported:\\n\\n           -r <n>[:<n>], --revision=<n>[:<n>]\\n               is supported, non-numeric args are not: HEAD, NEXT, BASE, PREV,\\n               etc ...\\n\\n           -v, --verbose\\n               it’s not completely compatible with the --verbose output in svn\\n               log, but reasonably close.\\n\\n           --limit=<n>\\n               is NOT the same as --max-count, doesn’t count merged/excluded\\n               commits\\n\\n           --incremental\\n               supported\\n\\n           New features:\\n\\n           --show-commit\\n               shows the Git commit sha1, as well\\n\\n           --oneline\\n               our version of --pretty=oneline\\n\\n\\n               Note\\n               SVN itself only stores times in UTC and nothing else. The\\n               regular svn client converts the UTC time to the local time (or\\n               based on the TZ= environment). This command has the same\\n               behaviour.\\n           Any other arguments are passed directly to git log\\n\\n       blame\\n           Show what revision and author last modified each line of a file.\\n           The output of this mode is format-compatible with the output of\\n           ‘svn blame’ by default. Like the SVN blame command, local\\n           uncommitted changes in the working tree are ignored; the version of\\n           the file in the HEAD revision is annotated. Unknown arguments are\\n           passed directly to git blame.\\n\\n           --git-format\\n               Produce output in the same format as git blame, but with SVN\\n               revision numbers instead of Git commit hashes. In this mode,\\n               changes that haven’t been committed to SVN (including local\\n               working-copy edits) are shown as revision 0.\\n\\n       find-rev\\n           When given an SVN revision number of the form rN, returns the\\n           corresponding Git commit hash (this can optionally be followed by a\\n           tree-ish to specify which branch should be searched). When given a\\n           tree-ish, returns the corresponding SVN revision number.\\n\\n           -B, --before\\n               Don’t require an exact match if given an SVN revision, instead\\n               find the commit corresponding to the state of the SVN\\n               repository (on the current branch) at the specified revision.\\n\\n           -A, --after\\n               Don’t require an exact match if given an SVN revision; if there\\n               is not an exact match return the closest match searching\\n               forward in the history.\\n\\n       set-tree\\n           You should consider using dcommit instead of this command. Commit\\n           specified commit or tree objects to SVN. This relies on your\\n           imported fetch data being up to date. This makes absolutely no\\n           attempts to do patching when committing to SVN, it simply\\n           overwrites files with those specified in the tree or commit. All\\n           merging is assumed to have taken place independently of git svn\\n           functions.\\n\\n       create-ignore\\n           Recursively finds the svn:ignore property on directories and\\n           creates matching .gitignore files. The resulting files are staged\\n           to be committed, but are not committed. Use -r/--revision to refer\\n           to a specific revision.\\n\\n       show-ignore\\n           Recursively finds and lists the svn:ignore property on directories.\\n           The output is suitable for appending to the $GIT_DIR/info/exclude\\n           file.\\n\\n       mkdirs\\n           Attempts to recreate empty directories that core Git cannot track\\n           based on information in $GIT_DIR/svn/<refname>/unhandled.log files.\\n           Empty directories are automatically recreated when using \"git svn\\n           clone\" and \"git svn rebase\", so \"mkdirs\" is intended for use after\\n           commands like \"git checkout\" or \"git reset\". (See the\\n           svn-remote.<name>.automkdirs config file option for more\\n           information.)\\n\\n       commit-diff\\n           Commits the diff of two tree-ish arguments from the command-line.\\n           This command does not rely on being inside a git svn init-ed\\n           repository. This command takes three arguments, (a) the original\\n           tree to diff against, (b) the new tree result, (c) the URL of the\\n           target Subversion repository. The final argument (URL) may be\\n           omitted if you are working from a git svn-aware repository (that\\n           has been init-ed with git svn). The -r<revision> option is required\\n           for this.\\n\\n           The commit message is supplied either directly with the -m or -F\\n           option, or indirectly from the tag or commit when the second\\n           tree-ish denotes such an object, or it is requested by invoking an\\n           editor (see --edit option below).\\n\\n           -m <msg>, --message=<msg>\\n               Use the given msg as the commit message. This option disables\\n               the --edit option.\\n\\n           -F <filename>, --file=<filename>\\n               Take the commit message from the given file. This option\\n               disables the --edit option.\\n\\n       info\\n           Shows information about a file or directory similar to what ‘svn\\n           info’ provides. Does not currently support a -r/--revision\\n           argument. Use the --url option to output only the value of the URL:\\n           field.\\n\\n       proplist\\n           Lists the properties stored in the Subversion repository about a\\n           given file or directory. Use -r/--revision to refer to a specific\\n           Subversion revision.\\n\\n       propget\\n           Gets the Subversion property given as the first argument, for a\\n           file. A specific revision can be specified with -r/--revision.\\n\\n       propset\\n           Sets the Subversion property given as the first argument, to the\\n           value given as the second argument for the file given as the third\\n           argument.\\n\\n           Example:\\n\\n               git svn propset svn:keywords \"FreeBSD=%H\" devel/py-tipper/Makefile\\n\\n           This will set the property svn:keywords to FreeBSD=%H for the file\\n           devel/py-tipper/Makefile.\\n\\n       show-externals\\n           Shows the Subversion externals. Use -r/--revision to specify a\\n           specific revision.\\n\\n       gc\\n           Compress $GIT_DIR/svn/<refname>/unhandled.log files and remove\\n           $GIT_DIR/svn/<refname>/index files.\\n\\n       reset\\n           Undoes the effects of fetch back to the specified revision. This\\n           allows you to re-fetch an SVN revision. Normally the contents of an\\n           SVN revision should never change and reset should not be necessary.\\n           However, if SVN permissions change, or if you alter your\\n           --ignore-paths option, a fetch may fail with \"not found in commit\"\\n           (file not previously visible) or \"checksum mismatch\" (missed a\\n           modification). If the problem file cannot be ignored forever (with\\n           --ignore-paths) the only way to repair the repo is to use reset.\\n\\n           Only the rev_map and refs/remotes/git-svn are changed (see\\n           $GIT_DIR/svn/**/.rev_map.*  in the FILES section below for\\n           details). Follow reset with a fetch and then git reset or git\\n           rebase to move local branches onto the new tree.\\n\\n           -r <n>, --revision=<n>\\n               Specify the most recent revision to keep. All later revisions\\n               are discarded.\\n\\n           -p, --parent\\n               Discard the specified revision as well, keeping the nearest\\n               parent instead.\\n\\n           Example:\\n               Assume you have local changes in \"master\", but you need to\\n               refetch \"r2\".\\n\\n                       r1---r2---r3 remotes/git-svn\\n                                   \\\\\\n                                    A---B master\\n\\n               Fix the ignore-paths or SVN permissions problem that caused\\n               \"r2\" to be incomplete in the first place. Then:\\n\\n                   git svn reset -r2 -p\\n                   git svn fetch\\n\\n\\n\\n                       r1---r2\\'--r3\\' remotes/git-svn\\n                         \\\\\\n                          r2---r3---A---B master\\n\\n               Then fixup \"master\" with git rebase. Do NOT use git merge or\\n               your history will not be compatible with a future dcommit!\\n\\n                   git rebase --onto remotes/git-svn A^ master\\n\\n\\n\\n                       r1---r2\\'--r3\\' remotes/git-svn\\n                                   \\\\\\n                                    A\\'--B\\' master\\n\\n\\nOPTIONS\\n       --shared[=(false|true|umask|group|all|world|everybody)],\\n       --template=<template-directory>\\n           Only used with the init command. These are passed directly to git\\n           init.\\n\\n       -r <arg>, --revision <arg>\\n           Used with the fetch command.\\n\\n           This allows revision ranges for partial/cauterized history to be\\n           supported. $NUMBER, $NUMBER1:$NUMBER2 (numeric ranges),\\n           $NUMBER:HEAD, and BASE:$NUMBER are all supported.\\n\\n           This can allow you to make partial mirrors when running fetch; but\\n           is generally not recommended because history will be skipped and\\n           lost.\\n\\n       -, --stdin\\n           Only used with the set-tree command.\\n\\n           Read a list of commits from stdin and commit them in reverse order.\\n           Only the leading sha1 is read from each line, so git rev-list\\n           --pretty=oneline output can be used.\\n\\n       --rmdir\\n           Only used with the dcommit, set-tree and commit-diff commands.\\n\\n           Remove directories from the SVN tree if there are no files left\\n           behind. SVN can version empty directories, and they are not removed\\n           by default if there are no files left in them. Git cannot version\\n           empty directories. Enabling this flag will make the commit to SVN\\n           act like Git.\\n\\n               config key: svn.rmdir\\n\\n\\n       -e, --edit\\n           Only used with the dcommit, set-tree and commit-diff commands.\\n\\n           Edit the commit message before committing to SVN. This is off by\\n           default for objects that are commits, and forced on when committing\\n           tree objects.\\n\\n               config key: svn.edit\\n\\n\\n       -l<num>, --find-copies-harder\\n           Only used with the dcommit, set-tree and commit-diff commands.\\n\\n           They are both passed directly to git diff-tree; see git-diff-\\n           tree(1) for more information.\\n\\n               config key: svn.l\\n               config key: svn.findcopiesharder\\n\\n\\n       -A<filename>, --authors-file=<filename>\\n           Syntax is compatible with the file used by git cvsimport but an\\n           empty email address can be supplied with <>:\\n\\n                       loginname = Joe User <user@example.com>\\n\\n           If this option is specified and git svn encounters an SVN committer\\n           name that does not exist in the authors-file, git svn will abort\\n           operation. The user will then have to add the appropriate entry.\\n           Re-running the previous git svn command after the authors-file is\\n           modified should continue operation.\\n\\n               config key: svn.authorsfile\\n\\n\\n       --authors-prog=<filename>\\n           If this option is specified, for each SVN committer name that does\\n           not exist in the authors file, the given file is executed with the\\n           committer name as the first argument. The program is expected to\\n           return a single line of the form \"Name <email>\" or \"Name <>\", which\\n           will be treated as if included in the authors file.\\n\\n           Due to historical reasons a relative filename is first searched\\n           relative to the current directory for init and clone and relative\\n           to the root of the working tree for fetch. If filename is not\\n           found, it is searched like any other command in $PATH.\\n\\n               config key: svn.authorsProg\\n\\n\\n       -q, --quiet\\n           Make git svn less verbose. Specify a second time to make it even\\n           less verbose.\\n\\n       -m, --merge, -s<strategy>, --strategy=<strategy>, -p, --rebase-merges\\n           These are only used with the dcommit and rebase commands.\\n\\n           Passed directly to git rebase when using dcommit if a git reset\\n           cannot be used (see dcommit).\\n\\n       -n, --dry-run\\n           This can be used with the dcommit, rebase, branch and tag commands.\\n\\n           For dcommit, print out the series of Git arguments that would show\\n           which diffs would be committed to SVN.\\n\\n           For rebase, display the local branch associated with the upstream\\n           svn repository associated with the current branch and the URL of\\n           svn repository that will be fetched from.\\n\\n           For branch and tag, display the urls that will be used for copying\\n           when creating the branch or tag.\\n\\n       --use-log-author\\n           When retrieving svn commits into Git (as part of fetch, rebase, or\\n           dcommit operations), look for the first From: line or Signed-off-by\\n           trailer in the log message and use that as the author string.\\n\\n               config key: svn.useLogAuthor\\n\\n\\n       --add-author-from\\n           When committing to svn from Git (as part of set-tree or dcommit\\n           operations), if the existing log message doesn’t already have a\\n           From: or Signed-off-by trailer, append a From: line based on the\\n           Git commit’s author string. If you use this, then --use-log-author\\n           will retrieve a valid author string for all commits.\\n\\n               config key: svn.addAuthorFrom\\n\\n\\nADVANCED OPTIONS\\n       -i<GIT_SVN_ID>, --id <GIT_SVN_ID>\\n           This sets GIT_SVN_ID (instead of using the environment). This\\n           allows the user to override the default refname to fetch from when\\n           tracking a single URL. The log and dcommit commands no longer\\n           require this switch as an argument.\\n\\n       -R<remote-name>, --svn-remote <remote-name>\\n           Specify the [svn-remote \"<remote-name>\"] section to use, this\\n           allows SVN multiple repositories to be tracked. Default: \"svn\"\\n\\n       --follow-parent\\n           This option is only relevant if we are tracking branches (using one\\n           of the repository layout options --trunk, --tags, --branches,\\n           --stdlayout). For each tracked branch, try to find out where its\\n           revision was copied from, and set a suitable parent in the first\\n           Git commit for the branch. This is especially helpful when we’re\\n           tracking a directory that has been moved around within the\\n           repository. If this feature is disabled, the branches created by\\n           git svn will all be linear and not share any history, meaning that\\n           there will be no information on where branches were branched off or\\n           merged. However, following long/convoluted histories can take a\\n           long time, so disabling this feature may speed up the cloning\\n           process. This feature is enabled by default, use --no-follow-parent\\n           to disable it.\\n\\n               config key: svn.followparent\\n\\n\\nCONFIG FILE-ONLY OPTIONS\\n       svn.noMetadata, svn-remote.<name>.noMetadata\\n           This gets rid of the git-svn-id: lines at the end of every commit.\\n\\n           This option can only be used for one-shot imports as git svn will\\n           not be able to fetch again without metadata. Additionally, if you\\n           lose your $GIT_DIR/svn/**/.rev_map.*  files, git svn will not be\\n           able to rebuild them.\\n\\n           The git svn log command will not work on repositories using this,\\n           either. Using this conflicts with the useSvmProps option for\\n           (hopefully) obvious reasons.\\n\\n           This option is NOT recommended as it makes it difficult to track\\n           down old references to SVN revision numbers in existing\\n           documentation, bug reports, and archives. If you plan to eventually\\n           migrate from SVN to Git and are certain about dropping SVN history,\\n           consider git-filter-repo[1] instead. filter-repo also allows\\n           reformatting of metadata for ease-of-reading and rewriting\\n           authorship info for non-\"svn.authorsFile\" users.\\n\\n       svn.useSvmProps, svn-remote.<name>.useSvmProps\\n           This allows git svn to re-map repository URLs and UUIDs from\\n           mirrors created using SVN::Mirror (or svk) for metadata.\\n\\n           If an SVN revision has a property, \"svm:headrev\", it is likely that\\n           the revision was created by SVN::Mirror (also used by SVK). The\\n           property contains a repository UUID and a revision. We want to make\\n           it look like we are mirroring the original URL, so introduce a\\n           helper function that returns the original identity URL and UUID,\\n           and use it when generating metadata in commit messages.\\n\\n       svn.useSvnsyncProps, svn-remote.<name>.useSvnsyncprops\\n           Similar to the useSvmProps option; this is for users of the\\n           svnsync(1) command distributed with SVN 1.4.x and later.\\n\\n       svn-remote.<name>.rewriteRoot\\n           This allows users to create repositories from alternate URLs. For\\n           example, an administrator could run git svn on the server locally\\n           (accessing via file://) but wish to distribute the repository with\\n           a public http:// or svn:// URL in the metadata so users of it will\\n           see the public URL.\\n\\n       svn-remote.<name>.rewriteUUID\\n           Similar to the useSvmProps option; this is for users who need to\\n           remap the UUID manually. This may be useful in situations where the\\n           original UUID is not available via either useSvmProps or\\n           useSvnsyncProps.\\n\\n       svn-remote.<name>.pushurl\\n           Similar to Git’s remote.<name>.pushurl, this key is designed to be\\n           used in cases where url points to an SVN repository via a read-only\\n           transport, to provide an alternate read/write transport. It is\\n           assumed that both keys point to the same repository. Unlike\\n           commiturl, pushurl is a base path. If either commiturl or pushurl\\n           could be used, commiturl takes precedence.\\n\\n       svn.brokenSymlinkWorkaround\\n           This disables potentially expensive checks to workaround broken\\n           symlinks checked into SVN by broken clients. Set this option to\\n           \"false\" if you track a SVN repository with many empty blobs that\\n           are not symlinks. This option may be changed while git svn is\\n           running and take effect on the next revision fetched. If unset, git\\n           svn assumes this option to be \"true\".\\n\\n       svn.pathnameencoding\\n           This instructs git svn to recode pathnames to a given encoding. It\\n           can be used by windows users and by those who work in non-utf8\\n           locales to avoid corrupted file names with non-ASCII characters.\\n           Valid encodings are the ones supported by Perl’s Encode module.\\n\\n       svn-remote.<name>.automkdirs\\n           Normally, the \"git svn clone\" and \"git svn rebase\" commands attempt\\n           to recreate empty directories that are in the Subversion\\n           repository. If this option is set to \"false\", then empty\\n           directories will only be created if the \"git svn mkdirs\" command is\\n           run explicitly. If unset, git svn assumes this option to be \"true\".\\n\\n       Since the noMetadata, rewriteRoot, rewriteUUID, useSvnsyncProps and\\n       useSvmProps options all affect the metadata generated and used by git\\n       svn; they must be set in the configuration file before any history is\\n       imported and these settings should never be changed once they are set.\\n\\n       Additionally, only one of these options can be used per svn-remote\\n       section because they affect the git-svn-id: metadata line, except for\\n       rewriteRoot and rewriteUUID which can be used together.\\n\\nBASIC EXAMPLES\\n       Tracking and contributing to the trunk of a Subversion-managed project\\n       (ignoring tags and branches):\\n\\n           # Clone a repo (like git clone):\\n                   git svn clone http://svn.example.com/project/trunk\\n           # Enter the newly cloned directory:\\n                   cd trunk\\n           # You should be on master branch, double-check with \\'git branch\\'\\n                   git branch\\n           # Do some work and commit locally to Git:\\n                   git commit ...\\n           # Something is committed to SVN, rebase your local changes against the\\n           # latest changes in SVN:\\n                   git svn rebase\\n           # Now commit your changes (that were committed previously using Git) to SVN,\\n           # as well as automatically updating your working HEAD:\\n                   git svn dcommit\\n           # Append svn:ignore settings to the default Git exclude file:\\n                   git svn show-ignore >> .git/info/exclude\\n\\n\\n       Tracking and contributing to an entire Subversion-managed project\\n       (complete with a trunk, tags and branches):\\n\\n           # Clone a repo with standard SVN directory layout (like git clone):\\n                   git svn clone http://svn.example.com/project --stdlayout --prefix svn/\\n           # Or, if the repo uses a non-standard directory layout:\\n                   git svn clone http://svn.example.com/project -T tr -b branch -t tag --prefix svn/\\n           # View all branches and tags you have cloned:\\n                   git branch -r\\n           # Create a new branch in SVN\\n                   git svn branch waldo\\n           # Reset your master to trunk (or any other branch, replacing \\'trunk\\'\\n           # with the appropriate name):\\n                   git reset --hard svn/trunk\\n           # You may only dcommit to one branch/tag/trunk at a time.  The usage\\n           # of dcommit/rebase/show-ignore should be the same as above.\\n\\n\\n       The initial git svn clone can be quite time-consuming (especially for\\n       large Subversion repositories). If multiple people (or one person with\\n       multiple machines) want to use git svn to interact with the same\\n       Subversion repository, you can do the initial git svn clone to a\\n       repository on a server and have each person clone that repository with\\n       git clone:\\n\\n           # Do the initial import on a server\\n                   ssh server \"cd /pub && git svn clone http://svn.example.com/project [options...]\"\\n           # Clone locally - make sure the refs/remotes/ space matches the server\\n                   mkdir project\\n                   cd project\\n                   git init\\n                   git remote add origin server:/pub/project\\n                   git config --replace-all remote.origin.fetch \\'+refs/remotes/*:refs/remotes/*\\'\\n                   git fetch\\n           # Prevent fetch/pull from remote Git server in the future,\\n           # we only want to use git svn for future updates\\n                   git config --remove-section remote.origin\\n           # Create a local branch from one of the branches just fetched\\n                   git checkout -b master FETCH_HEAD\\n           # Initialize \\'git svn\\' locally (be sure to use the same URL and\\n           # --stdlayout/-T/-b/-t/--prefix options as were used on server)\\n                   git svn init http://svn.example.com/project [options...]\\n           # Pull the latest changes from Subversion\\n                   git svn rebase\\n\\n\\nREBASE VS. PULL/MERGE\\n       Prefer to use git svn rebase or git rebase, rather than git pull or git\\n       merge to synchronize unintegrated commits with a git svn branch. Doing\\n       so will keep the history of unintegrated commits linear with respect to\\n       the upstream SVN repository and allow the use of the preferred git svn\\n       dcommit subcommand to push unintegrated commits back into SVN.\\n\\n       Originally, git svn recommended that developers pulled or merged from\\n       the git svn branch. This was because the author favored git svn\\n       set-tree B to commit a single head rather than the git svn set-tree\\n       A..B notation to commit multiple commits. Use of git pull or git merge\\n       with git svn set-tree A..B will cause non-linear history to be\\n       flattened when committing into SVN and this can lead to merge commits\\n       unexpectedly reversing previous commits in SVN.\\n\\nMERGE TRACKING\\n       While git svn can track copy history (including branches and tags) for\\n       repositories adopting a standard layout, it cannot yet represent merge\\n       history that happened inside git back upstream to SVN users. Therefore\\n       it is advised that users keep history as linear as possible inside Git\\n       to ease compatibility with SVN (see the CAVEATS section below).\\n\\nHANDLING OF SVN BRANCHES\\n       If git svn is configured to fetch branches (and --follow-branches is in\\n       effect), it sometimes creates multiple Git branches for one SVN branch,\\n       where the additional branches have names of the form branchname@nnn\\n       (with nnn an SVN revision number). These additional branches are\\n       created if git svn cannot find a parent commit for the first commit in\\n       an SVN branch, to connect the branch to the history of the other\\n       branches.\\n\\n       Normally, the first commit in an SVN branch consists of a copy\\n       operation. git svn will read this commit to get the SVN revision the\\n       branch was created from. It will then try to find the Git commit that\\n       corresponds to this SVN revision, and use that as the parent of the\\n       branch. However, it is possible that there is no suitable Git commit to\\n       serve as parent. This will happen, among other reasons, if the SVN\\n       branch is a copy of a revision that was not fetched by git svn (e.g.\\n       because it is an old revision that was skipped with --revision), or if\\n       in SVN a directory was copied that is not tracked by git svn (such as a\\n       branch that is not tracked at all, or a subdirectory of a tracked\\n       branch). In these cases, git svn will still create a Git branch, but\\n       instead of using an existing Git commit as the parent of the branch, it\\n       will read the SVN history of the directory the branch was copied from\\n       and create appropriate Git commits. This is indicated by the message\\n       \"Initializing parent: <branchname>\".\\n\\n       Additionally, it will create a special branch named\\n       <branchname>@<SVN-Revision>, where <SVN-Revision> is the SVN revision\\n       number the branch was copied from. This branch will point to the newly\\n       created parent commit of the branch. If in SVN the branch was deleted\\n       and later recreated from a different version, there will be multiple\\n       such branches with an @.\\n\\n       Note that this may mean that multiple Git commits are created for a\\n       single SVN revision.\\n\\n       An example: in an SVN repository with a standard trunk/tags/branches\\n       layout, a directory trunk/sub is created in r.100. In r.200, trunk/sub\\n       is branched by copying it to branches/. git svn clone -s will then\\n       create a branch sub. It will also create new Git commits for r.100\\n       through r.199 and use these as the history of branch sub. Thus there\\n       will be two Git commits for each revision from r.100 to r.199 (one\\n       containing trunk/, one containing trunk/sub/). Finally, it will create\\n       a branch sub@200 pointing to the new parent commit of branch sub (i.e.\\n       the commit for r.200 and trunk/sub/).\\n\\nCAVEATS\\n       For the sake of simplicity and interoperating with Subversion, it is\\n       recommended that all git svn users clone, fetch and dcommit directly\\n       from the SVN server, and avoid all git clone/pull/merge/push operations\\n       between Git repositories and branches. The recommended method of\\n       exchanging code between Git branches and users is git format-patch and\\n       git am, or just \\'dcommit’ing to the SVN repository.\\n\\n       Running git merge or git pull is NOT recommended on a branch you plan\\n       to dcommit from because Subversion users cannot see any merges you’ve\\n       made. Furthermore, if you merge or pull from a Git branch that is a\\n       mirror of an SVN branch, dcommit may commit to the wrong branch.\\n\\n       If you do merge, note the following rule: git svn dcommit will attempt\\n       to commit on top of the SVN commit named in\\n\\n           git log --grep=^git-svn-id: --first-parent -1\\n\\n\\n       You must therefore ensure that the most recent commit of the branch you\\n       want to dcommit to is the first parent of the merge. Chaos will ensue\\n       otherwise, especially if the first parent is an older commit on the\\n       same SVN branch.\\n\\n       git clone does not clone branches under the refs/remotes/ hierarchy or\\n       any git svn metadata, or config. So repositories created and managed\\n       with using git svn should use rsync for cloning, if cloning is to be\\n       done at all.\\n\\n       Since dcommit uses rebase internally, any Git branches you git push to\\n       before dcommit on will require forcing an overwrite of the existing ref\\n       on the remote repository. This is generally considered bad practice,\\n       see the git-push(1) documentation for details.\\n\\n       Do not use the --amend option of git-commit(1) on a change you’ve\\n       already dcommitted. It is considered bad practice to --amend commits\\n       you’ve already pushed to a remote repository for other users, and\\n       dcommit with SVN is analogous to that.\\n\\n       When cloning an SVN repository, if none of the options for describing\\n       the repository layout is used (--trunk, --tags, --branches,\\n       --stdlayout), git svn clone will create a Git repository with\\n       completely linear history, where branches and tags appear as separate\\n       directories in the working copy. While this is the easiest way to get a\\n       copy of a complete repository, for projects with many branches it will\\n       lead to a working copy many times larger than just the trunk. Thus for\\n       projects using the standard directory structure (trunk/branches/tags),\\n       it is recommended to clone with option --stdlayout. If the project uses\\n       a non-standard structure, and/or if branches and tags are not required,\\n       it is easiest to only clone one directory (typically trunk), without\\n       giving any repository layout options. If the full history with branches\\n       and tags is required, the options --trunk / --branches / --tags must be\\n       used.\\n\\n       When using multiple --branches or --tags, git svn does not\\n       automatically handle name collisions (for example, if two branches from\\n       different paths have the same name, or if a branch and a tag have the\\n       same name). In these cases, use init to set up your Git repository\\n       then, before your first fetch, edit the $GIT_DIR/config file so that\\n       the branches and tags are associated with different name spaces. For\\n       example:\\n\\n           branches = stable/*:refs/remotes/svn/stable/*\\n           branches = debug/*:refs/remotes/svn/debug/*\\n\\nCONFIGURATION\\n       git svn stores [svn-remote] configuration information in the repository\\n       $GIT_DIR/config file. It is similar the core Git [remote] sections\\n       except fetch keys do not accept glob arguments; but they are instead\\n       handled by the branches and tags keys. Since some SVN repositories are\\n       oddly configured with multiple projects glob expansions such those\\n       listed below are allowed:\\n\\n           [svn-remote \"project-a\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/project-a:refs/remotes/project-a/trunk\\n                   branches = branches/*/project-a:refs/remotes/project-a/branches/*\\n                   branches = branches/release_*:refs/remotes/project-a/branches/release_*\\n                   branches = branches/re*se:refs/remotes/project-a/branches/*\\n                   tags = tags/*/project-a:refs/remotes/project-a/tags/*\\n\\n\\n       Keep in mind that the * (asterisk) wildcard of the local ref (right of\\n       the :) must be the farthest right path component; however the remote\\n       wildcard may be anywhere as long as it’s an independent path component\\n       (surrounded by / or EOL). This type of configuration is not\\n       automatically created by init and should be manually entered with a\\n       text-editor or using git config.\\n\\n       Also note that only one asterisk is allowed per word. For example:\\n\\n           branches = branches/re*se:refs/remotes/project-a/branches/*\\n\\n       will match branches release, rese, re123se, however\\n\\n           branches = branches/re*s*e:refs/remotes/project-a/branches/*\\n\\n       will produce an error.\\n\\n       It is also possible to fetch a subset of branches or tags by using a\\n       comma-separated list of names within braces. For example:\\n\\n           [svn-remote \"huge-project\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/src:refs/remotes/trunk\\n                   branches = branches/{red,green}/src:refs/remotes/project-a/branches/*\\n                   tags = tags/{1.0,2.0}/src:refs/remotes/project-a/tags/*\\n\\n\\n       Multiple fetch, branches, and tags keys are supported:\\n\\n           [svn-remote \"messy-repo\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/project-a:refs/remotes/project-a/trunk\\n                   fetch = branches/demos/june-project-a-demo:refs/remotes/project-a/demos/june-demo\\n                   branches = branches/server/*:refs/remotes/project-a/branches/*\\n                   branches = branches/demos/2011/*:refs/remotes/project-a/2011-demos/*\\n                   tags = tags/server/*:refs/remotes/project-a/tags/*\\n\\n\\n       Creating a branch in such a configuration requires disambiguating which\\n       location to use using the -d or --destination flag:\\n\\n           $ git svn branch -d branches/server release-2-3-0\\n\\n\\n       Note that git-svn keeps track of the highest revision in which a branch\\n       or tag has appeared. If the subset of branches or tags is changed after\\n       fetching, then $GIT_DIR/svn/.metadata must be manually edited to remove\\n       (or reset) branches-maxRev and/or tags-maxRev as appropriate.\\n\\nFILES\\n       $GIT_DIR/svn/**/.rev_map.*\\n           Mapping between Subversion revision numbers and Git commit names.\\n           In a repository where the noMetadata option is not set, this can be\\n           rebuilt from the git-svn-id: lines that are at the end of every\\n           commit (see the svn.noMetadata section above for details).\\n\\n           git svn fetch and git svn rebase automatically update the rev_map\\n           if it is missing or not up to date.  git svn reset automatically\\n           rewinds it.\\n<SECTION>BUGS</SECTION>\\nWe ignore all SVN properties except svn:executable. Any unhandled\\n       properties are logged to $GIT_DIR/svn/<refname>/unhandled.log\\n\\n       Renamed and copied directories are not detected by Git and hence not\\n       tracked when committing to SVN. I do not plan on adding support for\\n       this as it’s quite difficult and time-consuming to get working for all\\n       the possible corner cases (Git doesn’t do it, either). Committing\\n       renamed and copied files is fully supported if they’re similar enough\\n       for Git to detect them.\\n\\n       In SVN, it is possible (though discouraged) to commit changes to a tag\\n       (because a tag is just a directory copy, thus technically the same as a\\n       branch). When cloning an SVN repository, git svn cannot know if such a\\n       commit to a tag will happen in the future. Thus it acts conservatively\\n       and imports all SVN tags as branches, prefixing the tag name with\\n       tags/.\\n<SECTION>SEE ALSO</SECTION>\\ngit-rebase(1)\\n<SECTION>GIT</SECTION>\\n-SVN(1)                        Git Manual                        GIT-SVN(1)\\n\\nNAME\\n       git-svn - Bidirectional operation between a Subversion repository and\\n       Git\\n\\nSYNOPSIS\\n       git svn <command> [<options>] [<arguments>]\\n\\n\\nDESCRIPTION\\n       git svn is a simple conduit for changesets between Subversion and Git.\\n       It provides a bidirectional flow of changes between a Subversion and a\\n       Git repository.\\n\\n       git svn can track a standard Subversion repository, following the\\n       common \"trunk/branches/tags\" layout, with the --stdlayout option. It\\n       can also follow branches and tags in any layout with the -T/-t/-b\\n       options (see options to init below, and also the clone command).\\n\\n       Once tracking a Subversion repository (with any of the above methods),\\n       the Git repository can be updated from Subversion by the fetch command\\n       and Subversion updated from Git by the dcommit command.\\n\\nCOMMANDS\\n       init\\n           Initializes an empty Git repository with additional metadata\\n           directories for git svn. The Subversion URL may be specified as a\\n           command-line argument, or as full URL arguments to -T/-t/-b.\\n           Optionally, the target directory to operate on can be specified as\\n           a second argument. Normally this command initializes the current\\n           directory.\\n\\n           -T<trunk-subdir>, --trunk=<trunk-subdir>, -t<tags-subdir>,\\n           --tags=<tags-subdir>, -b<branches-subdir>,\\n           --branches=<branches-subdir>, -s, --stdlayout\\n               These are optional command-line options for init. Each of these\\n               flags can point to a relative repository path\\n               (--tags=project/tags) or a full url\\n               (--tags=https://foo.org/project/tags). You can specify more\\n               than one --tags and/or --branches options, in case your\\n               Subversion repository places tags or branches under multiple\\n               paths. The option --stdlayout is a shorthand way of setting\\n               trunk,tags,branches as the relative paths, which is the\\n               Subversion default. If any of the other options are given as\\n               well, they take precedence.\\n\\n           --no-metadata\\n               Set the noMetadata option in the [svn-remote] config. This\\n               option is not recommended, please read the svn.noMetadata\\n               section of this manpage before using this option.\\n\\n           --use-svm-props\\n               Set the useSvmProps option in the [svn-remote] config.\\n\\n           --use-svnsync-props\\n               Set the useSvnsyncProps option in the [svn-remote] config.\\n\\n           --rewrite-root=<URL>\\n               Set the rewriteRoot option in the [svn-remote] config.\\n\\n           --rewrite-uuid=<UUID>\\n               Set the rewriteUUID option in the [svn-remote] config.\\n\\n           --username=<user>\\n               For transports that SVN handles authentication for (http,\\n               https, and plain svn), specify the username. For other\\n               transports (e.g.  svn+ssh://), you must include the username in\\n               the URL, e.g.  svn+ssh://foo@svn.bar.com/project\\n\\n           --prefix=<prefix>\\n               This allows one to specify a prefix which is prepended to the\\n               names of remotes if trunk/branches/tags are specified. The\\n               prefix does not automatically include a trailing slash, so be\\n               sure you include one in the argument if that is what you want.\\n               If --branches/-b is specified, the prefix must include a\\n               trailing slash. Setting a prefix (with a trailing slash) is\\n               strongly encouraged in any case, as your SVN-tracking refs will\\n               then be located at \"refs/remotes/$prefix/\", which is compatible\\n               with Git’s own remote-tracking ref layout\\n               (refs/remotes/$remote/). Setting a prefix is also useful if you\\n               wish to track multiple projects that share a common repository.\\n               By default, the prefix is set to origin/.\\n\\n                   Note\\n                   Before Git v2.0, the default prefix was \"\" (no prefix).\\n                   This meant that SVN-tracking refs were put at\\n                   \"refs/remotes/*\", which is incompatible with how Git’s own\\n                   remote-tracking refs are organized. If you still want the\\n                   old default, you can get it by passing --prefix \"\" on the\\n                   command line (--prefix=\"\" may not work if your Perl’s\\n                   Getopt::Long is < v2.37).\\n\\n           --ignore-refs=<regex>\\n               When passed to init or clone this regular expression will be\\n               preserved as a config key. See fetch for a description of\\n               --ignore-refs.\\n\\n           --ignore-paths=<regex>\\n               When passed to init or clone this regular expression will be\\n               preserved as a config key. See fetch for a description of\\n               --ignore-paths.\\n\\n           --include-paths=<regex>\\n               When passed to init or clone this regular expression will be\\n               preserved as a config key. See fetch for a description of\\n               --include-paths.\\n\\n           --no-minimize-url\\n               When tracking multiple directories (using --stdlayout,\\n               --branches, or --tags options), git svn will attempt to connect\\n               to the root (or highest allowed level) of the Subversion\\n               repository. This default allows better tracking of history if\\n               entire projects are moved within a repository, but may cause\\n               issues on repositories where read access restrictions are in\\n               place. Passing --no-minimize-url will allow git svn to accept\\n               URLs as-is without attempting to connect to a higher level\\n               directory. This option is off by default when only one\\n               URL/branch is tracked (it would do little good).\\n\\n       fetch\\n           Fetch unfetched revisions from the Subversion remote we are\\n           tracking. The name of the [svn-remote \"...\"] section in the\\n           $GIT_DIR/config file may be specified as an optional command-line\\n           argument.\\n\\n           This automatically updates the rev_map if needed (see\\n           $GIT_DIR/svn/**/.rev_map.*  in the FILES section below for\\n           details).\\n\\n           --localtime\\n               Store Git commit times in the local time zone instead of UTC.\\n               This makes git log (even without --date=local) show the same\\n               times that svn log would in the local time zone.\\n\\n               This doesn’t interfere with interoperating with the Subversion\\n               repository you cloned from, but if you wish for your local Git\\n               repository to be able to interoperate with someone else’s local\\n               Git repository, either don’t use this option or you should both\\n               use it in the same local time zone.\\n\\n           --parent\\n               Fetch only from the SVN parent of the current HEAD.\\n\\n           --ignore-refs=<regex>\\n               Ignore refs for branches or tags matching the Perl regular\\n               expression. A \"negative look-ahead assertion\" like\\n               ^refs/remotes/origin/(?!tags/wanted-tag|wanted-branch).*$ can\\n               be used to allow only certain refs.\\n\\n                   config key: svn-remote.<name>.ignore-refs\\n\\n               If the ignore-refs configuration key is set, and the\\n               command-line option is also given, both regular expressions\\n               will be used.\\n\\n           --ignore-paths=<regex>\\n               This allows one to specify a Perl regular expression that will\\n               cause skipping of all matching paths from checkout from SVN.\\n               The --ignore-paths option should match for every fetch\\n               (including automatic fetches due to clone, dcommit, rebase,\\n               etc) on a given repository.\\n\\n                   config key: svn-remote.<name>.ignore-paths\\n\\n               If the ignore-paths configuration key is set, and the\\n               command-line option is also given, both regular expressions\\n               will be used.\\n\\n               Examples:\\n\\n               Skip \"doc*\" directory for every fetch\\n\\n                       --ignore-paths=\"^doc\"\\n\\n\\n               Skip \"branches\" and \"tags\" of first level directories\\n\\n                       --ignore-paths=\"^[^/]+/(?:branches|tags)\"\\n\\n\\n           --include-paths=<regex>\\n               This allows one to specify a Perl regular expression that will\\n               cause the inclusion of only matching paths from checkout from\\n               SVN. The --include-paths option should match for every fetch\\n               (including automatic fetches due to clone, dcommit, rebase,\\n               etc) on a given repository.  --ignore-paths takes precedence\\n               over --include-paths.\\n\\n                   config key: svn-remote.<name>.include-paths\\n\\n\\n           --log-window-size=<n>\\n               Fetch <n> log entries per request when scanning Subversion\\n               history. The default is 100. For very large Subversion\\n               repositories, larger values may be needed for clone/fetch to\\n               complete in reasonable time. But overly large values may lead\\n               to higher memory usage and request timeouts.\\n\\n       clone\\n           Runs init and fetch. It will automatically create a directory based\\n           on the basename of the URL passed to it; or if a second argument is\\n           passed; it will create a directory and work within that. It accepts\\n           all arguments that the init and fetch commands accept; with the\\n           exception of --fetch-all and --parent. After a repository is\\n           cloned, the fetch command will be able to update revisions without\\n           affecting the working tree; and the rebase command will be able to\\n           update the working tree with the latest changes.\\n\\n           --preserve-empty-dirs\\n               Create a placeholder file in the local Git repository for each\\n               empty directory fetched from Subversion. This includes\\n               directories that become empty by removing all entries in the\\n               Subversion repository (but not the directory itself). The\\n               placeholder files are also tracked and removed when no longer\\n               necessary.\\n\\n           --placeholder-filename=<filename>\\n               Set the name of placeholder files created by\\n               --preserve-empty-dirs. Default: \".gitignore\"\\n\\n       rebase\\n           This fetches revisions from the SVN parent of the current HEAD and\\n           rebases the current (uncommitted to SVN) work against it.\\n\\n           This works similarly to svn update or git pull except that it\\n           preserves linear history with git rebase instead of git merge for\\n           ease of dcommitting with git svn.\\n\\n           This accepts all options that git svn fetch and git rebase accept.\\n           However, --fetch-all only fetches from the current [svn-remote],\\n           and not all [svn-remote] definitions.\\n\\n           Like git rebase; this requires that the working tree be clean and\\n           have no uncommitted changes.\\n\\n           This automatically updates the rev_map if needed (see\\n           $GIT_DIR/svn/**/.rev_map.*  in the FILES section below for\\n           details).\\n\\n           -l, --local\\n               Do not fetch remotely; only run git rebase against the last\\n               fetched commit from the upstream SVN.\\n\\n       dcommit\\n           Commit each diff from the current branch directly to the SVN\\n           repository, and then rebase or reset (depending on whether or not\\n           there is a diff between SVN and head). This will create a revision\\n           in SVN for each commit in Git.\\n\\n           When an optional Git branch name (or a Git commit object name) is\\n           specified as an argument, the subcommand works on the specified\\n           branch, not on the current branch.\\n\\n           Use of dcommit is preferred to set-tree (below).\\n\\n           --no-rebase\\n               After committing, do not rebase or reset.\\n\\n           --commit-url <URL>\\n               Commit to this SVN URL (the full path). This is intended to\\n               allow existing git svn repositories created with one transport\\n               method (e.g.  svn:// or http:// for anonymous read) to be\\n               reused if a user is later given access to an alternate\\n               transport method (e.g.  svn+ssh:// or https://) for commit.\\n\\n                   config key: svn-remote.<name>.commiturl\\n                   config key: svn.commiturl (overwrites all svn-remote.<name>.commiturl options)\\n\\n               Note that the SVN URL of the commiturl config key includes the\\n               SVN branch. If you rather want to set the commit URL for an\\n               entire SVN repository use svn-remote.<name>.pushurl instead.\\n\\n               Using this option for any other purpose (don’t ask) is very\\n               strongly discouraged.\\n\\n           --mergeinfo=<mergeinfo>\\n               Add the given merge information during the dcommit (e.g.\\n               --mergeinfo=\"/branches/foo:1-10\"). All svn server versions can\\n               store this information (as a property), and svn clients\\n               starting from version 1.5 can make use of it. To specify merge\\n               information from multiple branches, use a single space\\n               character between the branches (--mergeinfo=\"/branches/foo:1-10\\n               /branches/bar:3,5-6,8\")\\n\\n                   config key: svn.pushmergeinfo\\n\\n               This option will cause git-svn to attempt to automatically\\n               populate the svn:mergeinfo property in the SVN repository when\\n               possible. Currently, this can only be done when dcommitting\\n               non-fast-forward merges where all parents but the first have\\n               already been pushed into SVN.\\n\\n           --interactive\\n               Ask the user to confirm that a patch set should actually be\\n               sent to SVN. For each patch, one may answer \"yes\" (accept this\\n               patch), \"no\" (discard this patch), \"all\" (accept all patches),\\n               or \"quit\".\\n\\n               git svn dcommit returns immediately if answer is \"no\" or\\n               \"quit\", without committing anything to SVN.\\n\\n       branch\\n           Create a branch in the SVN repository.\\n\\n           -m, --message\\n               Allows to specify the commit message.\\n\\n           -t, --tag\\n               Create a tag by using the tags_subdir instead of the\\n               branches_subdir specified during git svn init.\\n\\n           -d<path>, --destination=<path>\\n               If more than one --branches (or --tags) option was given to the\\n               init or clone command, you must provide the location of the\\n               branch (or tag) you wish to create in the SVN repository.\\n               <path> specifies which path to use to create the branch or tag\\n               and should match the pattern on the left-hand side of one of\\n               the configured branches or tags refspecs. You can see these\\n               refspecs with the commands\\n\\n                   git config --get-all svn-remote.<name>.branches\\n                   git config --get-all svn-remote.<name>.tags\\n\\n               where <name> is the name of the SVN repository as specified by\\n               the -R option to init (or \"svn\" by default).\\n\\n           --username\\n               Specify the SVN username to perform the commit as. This option\\n               overrides the username configuration property.\\n\\n           --commit-url\\n               Use the specified URL to connect to the destination Subversion\\n               repository. This is useful in cases where the source SVN\\n               repository is read-only. This option overrides configuration\\n               property commiturl.\\n\\n                   git config --get-all svn-remote.<name>.commiturl\\n\\n           --parents\\n               Create parent folders. This parameter is equivalent to the\\n               parameter --parents on svn cp commands and is useful for\\n               non-standard repository layouts.\\n\\n       tag\\n           Create a tag in the SVN repository. This is a shorthand for branch\\n           -t.\\n\\n       log\\n           This should make it easy to look up svn log messages when svn users\\n           refer to -r/--revision numbers.\\n\\n           The following features from ‘svn log’ are supported:\\n\\n           -r <n>[:<n>], --revision=<n>[:<n>]\\n               is supported, non-numeric args are not: HEAD, NEXT, BASE, PREV,\\n               etc ...\\n\\n           -v, --verbose\\n               it’s not completely compatible with the --verbose output in svn\\n               log, but reasonably close.\\n\\n           --limit=<n>\\n               is NOT the same as --max-count, doesn’t count merged/excluded\\n               commits\\n\\n           --incremental\\n               supported\\n\\n           New features:\\n\\n           --show-commit\\n               shows the Git commit sha1, as well\\n\\n           --oneline\\n               our version of --pretty=oneline\\n\\n\\n               Note\\n               SVN itself only stores times in UTC and nothing else. The\\n               regular svn client converts the UTC time to the local time (or\\n               based on the TZ= environment). This command has the same\\n               behaviour.\\n           Any other arguments are passed directly to git log\\n\\n       blame\\n           Show what revision and author last modified each line of a file.\\n           The output of this mode is format-compatible with the output of\\n           ‘svn blame’ by default. Like the SVN blame command, local\\n           uncommitted changes in the working tree are ignored; the version of\\n           the file in the HEAD revision is annotated. Unknown arguments are\\n           passed directly to git blame.\\n\\n           --git-format\\n               Produce output in the same format as git blame, but with SVN\\n               revision numbers instead of Git commit hashes. In this mode,\\n               changes that haven’t been committed to SVN (including local\\n               working-copy edits) are shown as revision 0.\\n\\n       find-rev\\n           When given an SVN revision number of the form rN, returns the\\n           corresponding Git commit hash (this can optionally be followed by a\\n           tree-ish to specify which branch should be searched). When given a\\n           tree-ish, returns the corresponding SVN revision number.\\n\\n           -B, --before\\n               Don’t require an exact match if given an SVN revision, instead\\n               find the commit corresponding to the state of the SVN\\n               repository (on the current branch) at the specified revision.\\n\\n           -A, --after\\n               Don’t require an exact match if given an SVN revision; if there\\n               is not an exact match return the closest match searching\\n               forward in the history.\\n\\n       set-tree\\n           You should consider using dcommit instead of this command. Commit\\n           specified commit or tree objects to SVN. This relies on your\\n           imported fetch data being up to date. This makes absolutely no\\n           attempts to do patching when committing to SVN, it simply\\n           overwrites files with those specified in the tree or commit. All\\n           merging is assumed to have taken place independently of git svn\\n           functions.\\n\\n       create-ignore\\n           Recursively finds the svn:ignore property on directories and\\n           creates matching .gitignore files. The resulting files are staged\\n           to be committed, but are not committed. Use -r/--revision to refer\\n           to a specific revision.\\n\\n       show-ignore\\n           Recursively finds and lists the svn:ignore property on directories.\\n           The output is suitable for appending to the $GIT_DIR/info/exclude\\n           file.\\n\\n       mkdirs\\n           Attempts to recreate empty directories that core Git cannot track\\n           based on information in $GIT_DIR/svn/<refname>/unhandled.log files.\\n           Empty directories are automatically recreated when using \"git svn\\n           clone\" and \"git svn rebase\", so \"mkdirs\" is intended for use after\\n           commands like \"git checkout\" or \"git reset\". (See the\\n           svn-remote.<name>.automkdirs config file option for more\\n           information.)\\n\\n       commit-diff\\n           Commits the diff of two tree-ish arguments from the command-line.\\n           This command does not rely on being inside a git svn init-ed\\n           repository. This command takes three arguments, (a) the original\\n           tree to diff against, (b) the new tree result, (c) the URL of the\\n           target Subversion repository. The final argument (URL) may be\\n           omitted if you are working from a git svn-aware repository (that\\n           has been init-ed with git svn). The -r<revision> option is required\\n           for this.\\n\\n           The commit message is supplied either directly with the -m or -F\\n           option, or indirectly from the tag or commit when the second\\n           tree-ish denotes such an object, or it is requested by invoking an\\n           editor (see --edit option below).\\n\\n           -m <msg>, --message=<msg>\\n               Use the given msg as the commit message. This option disables\\n               the --edit option.\\n\\n           -F <filename>, --file=<filename>\\n               Take the commit message from the given file. This option\\n               disables the --edit option.\\n\\n       info\\n           Shows information about a file or directory similar to what ‘svn\\n           info’ provides. Does not currently support a -r/--revision\\n           argument. Use the --url option to output only the value of the URL:\\n           field.\\n\\n       proplist\\n           Lists the properties stored in the Subversion repository about a\\n           given file or directory. Use -r/--revision to refer to a specific\\n           Subversion revision.\\n\\n       propget\\n           Gets the Subversion property given as the first argument, for a\\n           file. A specific revision can be specified with -r/--revision.\\n\\n       propset\\n           Sets the Subversion property given as the first argument, to the\\n           value given as the second argument for the file given as the third\\n           argument.\\n\\n           Example:\\n\\n               git svn propset svn:keywords \"FreeBSD=%H\" devel/py-tipper/Makefile\\n\\n           This will set the property svn:keywords to FreeBSD=%H for the file\\n           devel/py-tipper/Makefile.\\n\\n       show-externals\\n           Shows the Subversion externals. Use -r/--revision to specify a\\n           specific revision.\\n\\n       gc\\n           Compress $GIT_DIR/svn/<refname>/unhandled.log files and remove\\n           $GIT_DIR/svn/<refname>/index files.\\n\\n       reset\\n           Undoes the effects of fetch back to the specified revision. This\\n           allows you to re-fetch an SVN revision. Normally the contents of an\\n           SVN revision should never change and reset should not be necessary.\\n           However, if SVN permissions change, or if you alter your\\n           --ignore-paths option, a fetch may fail with \"not found in commit\"\\n           (file not previously visible) or \"checksum mismatch\" (missed a\\n           modification). If the problem file cannot be ignored forever (with\\n           --ignore-paths) the only way to repair the repo is to use reset.\\n\\n           Only the rev_map and refs/remotes/git-svn are changed (see\\n           $GIT_DIR/svn/**/.rev_map.*  in the FILES section below for\\n           details). Follow reset with a fetch and then git reset or git\\n           rebase to move local branches onto the new tree.\\n\\n           -r <n>, --revision=<n>\\n               Specify the most recent revision to keep. All later revisions\\n               are discarded.\\n\\n           -p, --parent\\n               Discard the specified revision as well, keeping the nearest\\n               parent instead.\\n\\n           Example:\\n               Assume you have local changes in \"master\", but you need to\\n               refetch \"r2\".\\n\\n                       r1---r2---r3 remotes/git-svn\\n                                   \\\\\\n                                    A---B master\\n\\n               Fix the ignore-paths or SVN permissions problem that caused\\n               \"r2\" to be incomplete in the first place. Then:\\n\\n                   git svn reset -r2 -p\\n                   git svn fetch\\n\\n\\n\\n                       r1---r2\\'--r3\\' remotes/git-svn\\n                         \\\\\\n                          r2---r3---A---B master\\n\\n               Then fixup \"master\" with git rebase. Do NOT use git merge or\\n               your history will not be compatible with a future dcommit!\\n\\n                   git rebase --onto remotes/git-svn A^ master\\n\\n\\n\\n                       r1---r2\\'--r3\\' remotes/git-svn\\n                                   \\\\\\n                                    A\\'--B\\' master\\n\\n\\nOPTIONS\\n       --shared[=(false|true|umask|group|all|world|everybody)],\\n       --template=<template-directory>\\n           Only used with the init command. These are passed directly to git\\n           init.\\n\\n       -r <arg>, --revision <arg>\\n           Used with the fetch command.\\n\\n           This allows revision ranges for partial/cauterized history to be\\n           supported. $NUMBER, $NUMBER1:$NUMBER2 (numeric ranges),\\n           $NUMBER:HEAD, and BASE:$NUMBER are all supported.\\n\\n           This can allow you to make partial mirrors when running fetch; but\\n           is generally not recommended because history will be skipped and\\n           lost.\\n\\n       -, --stdin\\n           Only used with the set-tree command.\\n\\n           Read a list of commits from stdin and commit them in reverse order.\\n           Only the leading sha1 is read from each line, so git rev-list\\n           --pretty=oneline output can be used.\\n\\n       --rmdir\\n           Only used with the dcommit, set-tree and commit-diff commands.\\n\\n           Remove directories from the SVN tree if there are no files left\\n           behind. SVN can version empty directories, and they are not removed\\n           by default if there are no files left in them. Git cannot version\\n           empty directories. Enabling this flag will make the commit to SVN\\n           act like Git.\\n\\n               config key: svn.rmdir\\n\\n\\n       -e, --edit\\n           Only used with the dcommit, set-tree and commit-diff commands.\\n\\n           Edit the commit message before committing to SVN. This is off by\\n           default for objects that are commits, and forced on when committing\\n           tree objects.\\n\\n               config key: svn.edit\\n\\n\\n       -l<num>, --find-copies-harder\\n           Only used with the dcommit, set-tree and commit-diff commands.\\n\\n           They are both passed directly to git diff-tree; see git-diff-\\n           tree(1) for more information.\\n\\n               config key: svn.l\\n               config key: svn.findcopiesharder\\n\\n\\n       -A<filename>, --authors-file=<filename>\\n           Syntax is compatible with the file used by git cvsimport but an\\n           empty email address can be supplied with <>:\\n\\n                       loginname = Joe User <user@example.com>\\n\\n           If this option is specified and git svn encounters an SVN committer\\n           name that does not exist in the authors-file, git svn will abort\\n           operation. The user will then have to add the appropriate entry.\\n           Re-running the previous git svn command after the authors-file is\\n           modified should continue operation.\\n\\n               config key: svn.authorsfile\\n\\n\\n       --authors-prog=<filename>\\n           If this option is specified, for each SVN committer name that does\\n           not exist in the authors file, the given file is executed with the\\n           committer name as the first argument. The program is expected to\\n           return a single line of the form \"Name <email>\" or \"Name <>\", which\\n           will be treated as if included in the authors file.\\n\\n           Due to historical reasons a relative filename is first searched\\n           relative to the current directory for init and clone and relative\\n           to the root of the working tree for fetch. If filename is not\\n           found, it is searched like any other command in $PATH.\\n\\n               config key: svn.authorsProg\\n\\n\\n       -q, --quiet\\n           Make git svn less verbose. Specify a second time to make it even\\n           less verbose.\\n\\n       -m, --merge, -s<strategy>, --strategy=<strategy>, -p, --rebase-merges\\n           These are only used with the dcommit and rebase commands.\\n\\n           Passed directly to git rebase when using dcommit if a git reset\\n           cannot be used (see dcommit).\\n\\n       -n, --dry-run\\n           This can be used with the dcommit, rebase, branch and tag commands.\\n\\n           For dcommit, print out the series of Git arguments that would show\\n           which diffs would be committed to SVN.\\n\\n           For rebase, display the local branch associated with the upstream\\n           svn repository associated with the current branch and the URL of\\n           svn repository that will be fetched from.\\n\\n           For branch and tag, display the urls that will be used for copying\\n           when creating the branch or tag.\\n\\n       --use-log-author\\n           When retrieving svn commits into Git (as part of fetch, rebase, or\\n           dcommit operations), look for the first From: line or Signed-off-by\\n           trailer in the log message and use that as the author string.\\n\\n               config key: svn.useLogAuthor\\n\\n\\n       --add-author-from\\n           When committing to svn from Git (as part of set-tree or dcommit\\n           operations), if the existing log message doesn’t already have a\\n           From: or Signed-off-by trailer, append a From: line based on the\\n           Git commit’s author string. If you use this, then --use-log-author\\n           will retrieve a valid author string for all commits.\\n\\n               config key: svn.addAuthorFrom\\n\\n\\nADVANCED OPTIONS\\n       -i<GIT_SVN_ID>, --id <GIT_SVN_ID>\\n           This sets GIT_SVN_ID (instead of using the environment). This\\n           allows the user to override the default refname to fetch from when\\n           tracking a single URL. The log and dcommit commands no longer\\n           require this switch as an argument.\\n\\n       -R<remote-name>, --svn-remote <remote-name>\\n           Specify the [svn-remote \"<remote-name>\"] section to use, this\\n           allows SVN multiple repositories to be tracked. Default: \"svn\"\\n\\n       --follow-parent\\n           This option is only relevant if we are tracking branches (using one\\n           of the repository layout options --trunk, --tags, --branches,\\n           --stdlayout). For each tracked branch, try to find out where its\\n           revision was copied from, and set a suitable parent in the first\\n           Git commit for the branch. This is especially helpful when we’re\\n           tracking a directory that has been moved around within the\\n           repository. If this feature is disabled, the branches created by\\n           git svn will all be linear and not share any history, meaning that\\n           there will be no information on where branches were branched off or\\n           merged. However, following long/convoluted histories can take a\\n           long time, so disabling this feature may speed up the cloning\\n           process. This feature is enabled by default, use --no-follow-parent\\n           to disable it.\\n\\n               config key: svn.followparent\\n\\n\\nCONFIG FILE-ONLY OPTIONS\\n       svn.noMetadata, svn-remote.<name>.noMetadata\\n           This gets rid of the git-svn-id: lines at the end of every commit.\\n\\n           This option can only be used for one-shot imports as git svn will\\n           not be able to fetch again without metadata. Additionally, if you\\n           lose your $GIT_DIR/svn/**/.rev_map.*  files, git svn will not be\\n           able to rebuild them.\\n\\n           The git svn log command will not work on repositories using this,\\n           either. Using this conflicts with the useSvmProps option for\\n           (hopefully) obvious reasons.\\n\\n           This option is NOT recommended as it makes it difficult to track\\n           down old references to SVN revision numbers in existing\\n           documentation, bug reports, and archives. If you plan to eventually\\n           migrate from SVN to Git and are certain about dropping SVN history,\\n           consider git-filter-repo[1] instead. filter-repo also allows\\n           reformatting of metadata for ease-of-reading and rewriting\\n           authorship info for non-\"svn.authorsFile\" users.\\n\\n       svn.useSvmProps, svn-remote.<name>.useSvmProps\\n           This allows git svn to re-map repository URLs and UUIDs from\\n           mirrors created using SVN::Mirror (or svk) for metadata.\\n\\n           If an SVN revision has a property, \"svm:headrev\", it is likely that\\n           the revision was created by SVN::Mirror (also used by SVK). The\\n           property contains a repository UUID and a revision. We want to make\\n           it look like we are mirroring the original URL, so introduce a\\n           helper function that returns the original identity URL and UUID,\\n           and use it when generating metadata in commit messages.\\n\\n       svn.useSvnsyncProps, svn-remote.<name>.useSvnsyncprops\\n           Similar to the useSvmProps option; this is for users of the\\n           svnsync(1) command distributed with SVN 1.4.x and later.\\n\\n       svn-remote.<name>.rewriteRoot\\n           This allows users to create repositories from alternate URLs. For\\n           example, an administrator could run git svn on the server locally\\n           (accessing via file://) but wish to distribute the repository with\\n           a public http:// or svn:// URL in the metadata so users of it will\\n           see the public URL.\\n\\n       svn-remote.<name>.rewriteUUID\\n           Similar to the useSvmProps option; this is for users who need to\\n           remap the UUID manually. This may be useful in situations where the\\n           original UUID is not available via either useSvmProps or\\n           useSvnsyncProps.\\n\\n       svn-remote.<name>.pushurl\\n           Similar to Git’s remote.<name>.pushurl, this key is designed to be\\n           used in cases where url points to an SVN repository via a read-only\\n           transport, to provide an alternate read/write transport. It is\\n           assumed that both keys point to the same repository. Unlike\\n           commiturl, pushurl is a base path. If either commiturl or pushurl\\n           could be used, commiturl takes precedence.\\n\\n       svn.brokenSymlinkWorkaround\\n           This disables potentially expensive checks to workaround broken\\n           symlinks checked into SVN by broken clients. Set this option to\\n           \"false\" if you track a SVN repository with many empty blobs that\\n           are not symlinks. This option may be changed while git svn is\\n           running and take effect on the next revision fetched. If unset, git\\n           svn assumes this option to be \"true\".\\n\\n       svn.pathnameencoding\\n           This instructs git svn to recode pathnames to a given encoding. It\\n           can be used by windows users and by those who work in non-utf8\\n           locales to avoid corrupted file names with non-ASCII characters.\\n           Valid encodings are the ones supported by Perl’s Encode module.\\n\\n       svn-remote.<name>.automkdirs\\n           Normally, the \"git svn clone\" and \"git svn rebase\" commands attempt\\n           to recreate empty directories that are in the Subversion\\n           repository. If this option is set to \"false\", then empty\\n           directories will only be created if the \"git svn mkdirs\" command is\\n           run explicitly. If unset, git svn assumes this option to be \"true\".\\n\\n       Since the noMetadata, rewriteRoot, rewriteUUID, useSvnsyncProps and\\n       useSvmProps options all affect the metadata generated and used by git\\n       svn; they must be set in the configuration file before any history is\\n       imported and these settings should never be changed once they are set.\\n\\n       Additionally, only one of these options can be used per svn-remote\\n       section because they affect the git-svn-id: metadata line, except for\\n       rewriteRoot and rewriteUUID which can be used together.\\n\\nBASIC EXAMPLES\\n       Tracking and contributing to the trunk of a Subversion-managed project\\n       (ignoring tags and branches):\\n\\n           # Clone a repo (like git clone):\\n                   git svn clone http://svn.example.com/project/trunk\\n           # Enter the newly cloned directory:\\n                   cd trunk\\n           # You should be on master branch, double-check with \\'git branch\\'\\n                   git branch\\n           # Do some work and commit locally to Git:\\n                   git commit ...\\n           # Something is committed to SVN, rebase your local changes against the\\n           # latest changes in SVN:\\n                   git svn rebase\\n           # Now commit your changes (that were committed previously using Git) to SVN,\\n           # as well as automatically updating your working HEAD:\\n                   git svn dcommit\\n           # Append svn:ignore settings to the default Git exclude file:\\n                   git svn show-ignore >> .git/info/exclude\\n\\n\\n       Tracking and contributing to an entire Subversion-managed project\\n       (complete with a trunk, tags and branches):\\n\\n           # Clone a repo with standard SVN directory layout (like git clone):\\n                   git svn clone http://svn.example.com/project --stdlayout --prefix svn/\\n           # Or, if the repo uses a non-standard directory layout:\\n                   git svn clone http://svn.example.com/project -T tr -b branch -t tag --prefix svn/\\n           # View all branches and tags you have cloned:\\n                   git branch -r\\n           # Create a new branch in SVN\\n                   git svn branch waldo\\n           # Reset your master to trunk (or any other branch, replacing \\'trunk\\'\\n           # with the appropriate name):\\n                   git reset --hard svn/trunk\\n           # You may only dcommit to one branch/tag/trunk at a time.  The usage\\n           # of dcommit/rebase/show-ignore should be the same as above.\\n\\n\\n       The initial git svn clone can be quite time-consuming (especially for\\n       large Subversion repositories). If multiple people (or one person with\\n       multiple machines) want to use git svn to interact with the same\\n       Subversion repository, you can do the initial git svn clone to a\\n       repository on a server and have each person clone that repository with\\n       git clone:\\n\\n           # Do the initial import on a server\\n                   ssh server \"cd /pub && git svn clone http://svn.example.com/project [options...]\"\\n           # Clone locally - make sure the refs/remotes/ space matches the server\\n                   mkdir project\\n                   cd project\\n                   git init\\n                   git remote add origin server:/pub/project\\n                   git config --replace-all remote.origin.fetch \\'+refs/remotes/*:refs/remotes/*\\'\\n                   git fetch\\n           # Prevent fetch/pull from remote Git server in the future,\\n           # we only want to use git svn for future updates\\n                   git config --remove-section remote.origin\\n           # Create a local branch from one of the branches just fetched\\n                   git checkout -b master FETCH_HEAD\\n           # Initialize \\'git svn\\' locally (be sure to use the same URL and\\n           # --stdlayout/-T/-b/-t/--prefix options as were used on server)\\n                   git svn init http://svn.example.com/project [options...]\\n           # Pull the latest changes from Subversion\\n                   git svn rebase\\n\\n\\nREBASE VS. PULL/MERGE\\n       Prefer to use git svn rebase or git rebase, rather than git pull or git\\n       merge to synchronize unintegrated commits with a git svn branch. Doing\\n       so will keep the history of unintegrated commits linear with respect to\\n       the upstream SVN repository and allow the use of the preferred git svn\\n       dcommit subcommand to push unintegrated commits back into SVN.\\n\\n       Originally, git svn recommended that developers pulled or merged from\\n       the git svn branch. This was because the author favored git svn\\n       set-tree B to commit a single head rather than the git svn set-tree\\n       A..B notation to commit multiple commits. Use of git pull or git merge\\n       with git svn set-tree A..B will cause non-linear history to be\\n       flattened when committing into SVN and this can lead to merge commits\\n       unexpectedly reversing previous commits in SVN.\\n\\nMERGE TRACKING\\n       While git svn can track copy history (including branches and tags) for\\n       repositories adopting a standard layout, it cannot yet represent merge\\n       history that happened inside git back upstream to SVN users. Therefore\\n       it is advised that users keep history as linear as possible inside Git\\n       to ease compatibility with SVN (see the CAVEATS section below).\\n\\nHANDLING OF SVN BRANCHES\\n       If git svn is configured to fetch branches (and --follow-branches is in\\n       effect), it sometimes creates multiple Git branches for one SVN branch,\\n       where the additional branches have names of the form branchname@nnn\\n       (with nnn an SVN revision number). These additional branches are\\n       created if git svn cannot find a parent commit for the first commit in\\n       an SVN branch, to connect the branch to the history of the other\\n       branches.\\n\\n       Normally, the first commit in an SVN branch consists of a copy\\n       operation. git svn will read this commit to get the SVN revision the\\n       branch was created from. It will then try to find the Git commit that\\n       corresponds to this SVN revision, and use that as the parent of the\\n       branch. However, it is possible that there is no suitable Git commit to\\n       serve as parent. This will happen, among other reasons, if the SVN\\n       branch is a copy of a revision that was not fetched by git svn (e.g.\\n       because it is an old revision that was skipped with --revision), or if\\n       in SVN a directory was copied that is not tracked by git svn (such as a\\n       branch that is not tracked at all, or a subdirectory of a tracked\\n       branch). In these cases, git svn will still create a Git branch, but\\n       instead of using an existing Git commit as the parent of the branch, it\\n       will read the SVN history of the directory the branch was copied from\\n       and create appropriate Git commits. This is indicated by the message\\n       \"Initializing parent: <branchname>\".\\n\\n       Additionally, it will create a special branch named\\n       <branchname>@<SVN-Revision>, where <SVN-Revision> is the SVN revision\\n       number the branch was copied from. This branch will point to the newly\\n       created parent commit of the branch. If in SVN the branch was deleted\\n       and later recreated from a different version, there will be multiple\\n       such branches with an @.\\n\\n       Note that this may mean that multiple Git commits are created for a\\n       single SVN revision.\\n\\n       An example: in an SVN repository with a standard trunk/tags/branches\\n       layout, a directory trunk/sub is created in r.100. In r.200, trunk/sub\\n       is branched by copying it to branches/. git svn clone -s will then\\n       create a branch sub. It will also create new Git commits for r.100\\n       through r.199 and use these as the history of branch sub. Thus there\\n       will be two Git commits for each revision from r.100 to r.199 (one\\n       containing trunk/, one containing trunk/sub/). Finally, it will create\\n       a branch sub@200 pointing to the new parent commit of branch sub (i.e.\\n       the commit for r.200 and trunk/sub/).\\n\\nCAVEATS\\n       For the sake of simplicity and interoperating with Subversion, it is\\n       recommended that all git svn users clone, fetch and dcommit directly\\n       from the SVN server, and avoid all git clone/pull/merge/push operations\\n       between Git repositories and branches. The recommended method of\\n       exchanging code between Git branches and users is git format-patch and\\n       git am, or just \\'dcommit’ing to the SVN repository.\\n\\n       Running git merge or git pull is NOT recommended on a branch you plan\\n       to dcommit from because Subversion users cannot see any merges you’ve\\n       made. Furthermore, if you merge or pull from a Git branch that is a\\n       mirror of an SVN branch, dcommit may commit to the wrong branch.\\n\\n       If you do merge, note the following rule: git svn dcommit will attempt\\n       to commit on top of the SVN commit named in\\n\\n           git log --grep=^git-svn-id: --first-parent -1\\n\\n\\n       You must therefore ensure that the most recent commit of the branch you\\n       want to dcommit to is the first parent of the merge. Chaos will ensue\\n       otherwise, especially if the first parent is an older commit on the\\n       same SVN branch.\\n\\n       git clone does not clone branches under the refs/remotes/ hierarchy or\\n       any git svn metadata, or config. So repositories created and managed\\n       with using git svn should use rsync for cloning, if cloning is to be\\n       done at all.\\n\\n       Since dcommit uses rebase internally, any Git branches you git push to\\n       before dcommit on will require forcing an overwrite of the existing ref\\n       on the remote repository. This is generally considered bad practice,\\n       see the git-push(1) documentation for details.\\n\\n       Do not use the --amend option of git-commit(1) on a change you’ve\\n       already dcommitted. It is considered bad practice to --amend commits\\n       you’ve already pushed to a remote repository for other users, and\\n       dcommit with SVN is analogous to that.\\n\\n       When cloning an SVN repository, if none of the options for describing\\n       the repository layout is used (--trunk, --tags, --branches,\\n       --stdlayout), git svn clone will create a Git repository with\\n       completely linear history, where branches and tags appear as separate\\n       directories in the working copy. While this is the easiest way to get a\\n       copy of a complete repository, for projects with many branches it will\\n       lead to a working copy many times larger than just the trunk. Thus for\\n       projects using the standard directory structure (trunk/branches/tags),\\n       it is recommended to clone with option --stdlayout. If the project uses\\n       a non-standard structure, and/or if branches and tags are not required,\\n       it is easiest to only clone one directory (typically trunk), without\\n       giving any repository layout options. If the full history with branches\\n       and tags is required, the options --trunk / --branches / --tags must be\\n       used.\\n\\n       When using multiple --branches or --tags, git svn does not\\n       automatically handle name collisions (for example, if two branches from\\n       different paths have the same name, or if a branch and a tag have the\\n       same name). In these cases, use init to set up your Git repository\\n       then, before your first fetch, edit the $GIT_DIR/config file so that\\n       the branches and tags are associated with different name spaces. For\\n       example:\\n\\n           branches = stable/*:refs/remotes/svn/stable/*\\n           branches = debug/*:refs/remotes/svn/debug/*\\n\\nCONFIGURATION\\n       git svn stores [svn-remote] configuration information in the repository\\n       $GIT_DIR/config file. It is similar the core Git [remote] sections\\n       except fetch keys do not accept glob arguments; but they are instead\\n       handled by the branches and tags keys. Since some SVN repositories are\\n       oddly configured with multiple projects glob expansions such those\\n       listed below are allowed:\\n\\n           [svn-remote \"project-a\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/project-a:refs/remotes/project-a/trunk\\n                   branches = branches/*/project-a:refs/remotes/project-a/branches/*\\n                   branches = branches/release_*:refs/remotes/project-a/branches/release_*\\n                   branches = branches/re*se:refs/remotes/project-a/branches/*\\n                   tags = tags/*/project-a:refs/remotes/project-a/tags/*\\n\\n\\n       Keep in mind that the * (asterisk) wildcard of the local ref (right of\\n       the :) must be the farthest right path component; however the remote\\n       wildcard may be anywhere as long as it’s an independent path component\\n       (surrounded by / or EOL). This type of configuration is not\\n       automatically created by init and should be manually entered with a\\n       text-editor or using git config.\\n\\n       Also note that only one asterisk is allowed per word. For example:\\n\\n           branches = branches/re*se:refs/remotes/project-a/branches/*\\n\\n       will match branches release, rese, re123se, however\\n\\n           branches = branches/re*s*e:refs/remotes/project-a/branches/*\\n\\n       will produce an error.\\n\\n       It is also possible to fetch a subset of branches or tags by using a\\n       comma-separated list of names within braces. For example:\\n\\n           [svn-remote \"huge-project\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/src:refs/remotes/trunk\\n                   branches = branches/{red,green}/src:refs/remotes/project-a/branches/*\\n                   tags = tags/{1.0,2.0}/src:refs/remotes/project-a/tags/*\\n\\n\\n       Multiple fetch, branches, and tags keys are supported:\\n\\n           [svn-remote \"messy-repo\"]\\n                   url = http://server.org/svn\\n                   fetch = trunk/project-a:refs/remotes/project-a/trunk\\n                   fetch = branches/demos/june-project-a-demo:refs/remotes/project-a/demos/june-demo\\n                   branches = branches/server/*:refs/remotes/project-a/branches/*\\n                   branches = branches/demos/2011/*:refs/remotes/project-a/2011-demos/*\\n                   tags = tags/server/*:refs/remotes/project-a/tags/*\\n\\n\\n       Creating a branch in such a configuration requires disambiguating which\\n       location to use using the -d or --destination flag:\\n\\n           $ git svn branch -d branches/server release-2-3-0\\n\\n\\n       Note that git-svn keeps track of the highest revision in which a branch\\n       or tag has appeared. If the subset of branches or tags is changed after\\n       fetching, then $GIT_DIR/svn/.metadata must be manually edited to remove\\n       (or reset) branches-maxRev and/or tags-maxRev as appropriate.\\n\\nFILES\\n       $GIT_DIR/svn/**/.rev_map.*\\n           Mapping between Subversion revision numbers and Git commit names.\\n           In a repository where the noMetadata option is not set, this can be\\n           rebuilt from the git-svn-id: lines that are at the end of every\\n           commit (see the svn.noMetadata section above for details).\\n\\n           git svn fetch and git svn rebase automatically update the rev_map\\n           if it is missing or not up to date.  git svn reset automatically\\n           rewinds it.\\n\\nBUGS\\n       We ignore all SVN properties except svn:executable. Any unhandled\\n       properties are logged to $GIT_DIR/svn/<refname>/unhandled.log\\n\\n       Renamed and copied directories are not detected by Git and hence not\\n       tracked when committing to SVN. I do not plan on adding support for\\n       this as it’s quite difficult and time-consuming to get working for all\\n       the possible corner cases (Git doesn’t do it, either). Committing\\n       renamed and copied files is fully supported if they’re similar enough\\n       for Git to detect them.\\n\\n       In SVN, it is possible (though discouraged) to commit changes to a tag\\n       (because a tag is just a directory copy, thus technically the same as a\\n       branch). When cloning an SVN repository, git svn cannot know if such a\\n       commit to a tag will happen in the future. Thus it acts conservatively\\n       and imports all SVN tags as branches, prefixing the tag name with\\n       tags/.\\n\\nSEE ALSO\\n       git-rebase(1)\\n\\nGIT\\n       Part of the git(1) suite\\n<SECTION>NOTES</SECTION>\\n1. git-filter-repo\\n           https://github.com/newren/git-filter-repo\\n\\nGit 2.46.1                        09/14/2024                        GIT-SVN(1)'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ds[\"train\"][2]\n",
    "# tokenized_ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029b817-f0ba-4a35-ad5e-31f418ea4fdd",
   "metadata": {
    "id": "b029b817-f0ba-4a35-ad5e-31f418ea4fdd"
   },
   "source": [
    "# Setting up the model+training parameters\n",
    "Here we are going to setup our LoRA config. LoRA, or LOw-Rank Adaptation, uses advanced linear algebra to reduce the number of parameters we need to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f394f81-e0e8-4d3c-9ac4-4c9da7405ee4",
   "metadata": {
    "id": "4f394f81-e0e8-4d3c-9ac4-4c9da7405ee4"
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce867e72-532d-47e9-af2b-8104fc533159",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce867e72-532d-47e9-af2b-8104fc533159",
    "outputId": "df8756ff-d1e8-4966-c5fd-447720c94863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o', 'wi', 'k', 'q', 'v']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6fd7b2-f8ba-48cc-85bc-674c98d69750",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e6fd7b2-f8ba-48cc-85bc-674c98d69750",
    "outputId": "e1fb0673-fedd-401d-d1f6-cdbcd320601e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input', 'output']\n"
     ]
    }
   ],
   "source": [
    "print(final_ds[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36ce5bdb-108d-4500-bc2b-bd1fec532316",
   "metadata": {
    "id": "36ce5bdb-108d-4500-bc2b-bd1fec532316"
   },
   "outputs": [],
   "source": [
    "# from numba import cuda\n",
    "# from datasets import load_metric\n",
    "#\n",
    "# label_pad_token_id = -100\n",
    "#\n",
    "# # Load the ROUGE metric\n",
    "# metric = load_metric(\"rouge\")\n",
    "#\n",
    "# # Data collator\n",
    "# data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "#     tokenizer,\n",
    "#     model=model,\n",
    "#     label_pad_token_id=label_pad_token_id,\n",
    "#     pad_to_multiple_of=8,\n",
    "# )\n",
    "#\n",
    "# new_model = \"t5-3b-man-pages\"\n",
    "# # Setting Hyperparamter\n",
    "# training_arguments = transformers.TrainingArguments(\n",
    "#     output_dir=new_model,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     optim=\"paged_adamw_32bit\",\n",
    "#     num_train_epochs=1,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps = 0.25,\n",
    "#     logging_steps=1,\n",
    "#     warmup_steps=10,\n",
    "#     logging_strategy=\"steps\",\n",
    "#     learning_rate=2e-4,\n",
    "#     fp16=True,\n",
    "#     bf16=False,\n",
    "#     group_by_length=True,\n",
    "#     report_to = \"none\",\n",
    "# )\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=tokenized_ds[\"train\"],\n",
    "#     eval_dataset=tokenized_ds[\"test\"],\n",
    "#     peft_config=peft_config,\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_seq_length=512,\n",
    "#     args=training_arguments,\n",
    "#     packing=False,\n",
    "#\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "097dd5ab-2113-408f-a4fe-8e3a9d609843",
   "metadata": {
    "id": "097dd5ab-2113-408f-a4fe-8e3a9d609843"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "# import numpy as np\n",
    "#\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#     # Replace -100 in the labels as we can't decode them.\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "#\n",
    "#     # Rouge expects a newline after each sentence\n",
    "#     decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "#     decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "#\n",
    "#     # Note that other metrics may not have a `use_aggregator` parameter\n",
    "#     # and thus will return a list, computing a metric for each sentence.\n",
    "#     result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "#     # Extract a few results\n",
    "#     result = {key: value * 100 for key, value in result.items()}\n",
    "#\n",
    "#     # Add mean generated length\n",
    "#     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "#     result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "#\n",
    "#     return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61d6b690-f21d-4aed-9323-df4910c52977",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61d6b690-f21d-4aed-9323-df4910c52977",
    "outputId": "5a4e11c2-ce26-4c59-a3d4-e36039564b45"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6871/3214720823.py:55: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "base_model = \"google-t5/t5-large\"\n",
    "new_model = \"t5-large-man-pages\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "# Configuartion of model quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "peft_config = peft.LoraConfig(\n",
    "    task_type=peft.TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map = \"cuda:0\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "model_name = \"t5-large\"\n",
    "\n",
    "model = peft.get_peft_model(model, peft_config)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps = 0.125,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    # compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27420ff0-2a1d-4081-beb4-473c2658c944",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "27420ff0-2a1d-4081-beb4-473c2658c944",
    "outputId": "44d85d77-a105-409c-876b-b849a52fdf03",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18805' max='28204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18805/28204 1:46:55 < 53:26, 2.93 it/s, Epoch 1.33/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3526</td>\n",
       "      <td>3.008600</td>\n",
       "      <td>2.710539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7052</td>\n",
       "      <td>2.812700</td>\n",
       "      <td>2.527979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10578</td>\n",
       "      <td>2.717000</td>\n",
       "      <td>2.425946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14104</td>\n",
       "      <td>2.629100</td>\n",
       "      <td>2.368666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17630</td>\n",
       "      <td>2.618600</td>\n",
       "      <td>2.324760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/trainer.py:2113\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 2113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2114\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2120\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/trainer.py:2474\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2474\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2477\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2479\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2480\u001b[0m ):\n\u001b[1;32m   2481\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3572\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3578\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/trainer.py:3625\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3623\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3624\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3625\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3626\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/peft_model.py:1859\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, decoder_input_ids, decoder_attention_mask, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1858\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1859\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1873\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1875\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1891\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1888\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1891\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1901\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1905\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1907\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1124\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         cache_position,\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:675\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    661\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m ):\n\u001b[0;32m--> 675\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     hidden_states, past_key_value \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    686\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:593\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    583\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    591\u001b[0m ):\n\u001b[1;32m    592\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 593\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    604\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:504\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    502\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m curr_past_key_value\u001b[38;5;241m.\u001b[39mvalue_cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx]\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 504\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv(current_states)\n\u001b[1;32m    506\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_value_proj_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m module \u001b[38;5;241m=\u001b[39m hook\u001b[38;5;241m.\u001b[39minit_hook(module)\n\u001b[1;32m    162\u001b[0m module\u001b[38;5;241m.\u001b[39m_hf_hook \u001b[38;5;241m=\u001b[39m hook\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95e6de21-21d8-4ce7-8d24-52e4ec90e96f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "59f60b65f13f4bcb88c477598befc5b1",
      "565601837a54461abd32372cd98ca32c",
      "3a92aa796fe843a78b482d0db8f1b57a",
      "11eb7059317c4dfabc7d4648e2307f8e",
      "bf5c0a88a03c45d9830b72c768c71b1c",
      "dace2be826664e86906a2650aae18d6f",
      "3a5de6f2752f4ed88bf9cffd3a41a83b",
      "960e4129039145ac9882cd7f0f59005d",
      "f6ce6405728c4afeb7d4b8d8285ae671",
      "d487efb02cc145b1a1b0d18e9a641354",
      "433c8f1e498745cc8219e7feca67645e"
     ]
    },
    "id": "95e6de21-21d8-4ce7-8d24-52e4ec90e96f",
    "outputId": "e6100b4e-49ae-42ef-98b8-262b66d3567c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.48M/9.48M [00:03<00:00, 2.43MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/asdfasda112312/t5-large-man-pages/commit/711442f5cfd9f4a8b3c6f49648d6c903b726d4cf', commit_message='Upload model', commit_description='', oid='711442f5cfd9f4a8b3c6f49648d6c903b726d4cf', pr_url=None, repo_url=RepoUrl('https://huggingface.co/asdfasda112312/t5-large-man-pages', endpoint='https://huggingface.co', repo_type='model', repo_id='asdfasda112312/t5-large-man-pages'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.model.save_pretrained(f\"{model_name}-finetuned-xsum\")\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.model.push_to_hub(new_model, use_temp_dir=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g4iOTKCkJuVI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g4iOTKCkJuVI",
    "outputId": "c5a01f5a-7a05-4f43-b323-3aa5e39fb597"
   },
   "outputs": [],
   "source": [
    "# # prompt: zip t5-large-man-pages and then download\n",
    "# \n",
    "# !zip -r t5-large-man-pages.zip t5-large-man-pages\n",
    "# !ls\n",
    "# from google.colab import files\n",
    "# files.download('t5-large-man-pages.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b08501-f8b6-48ea-b25d-ddcaf295d831",
   "metadata": {
    "collapsed": true,
    "id": "d0b08501-f8b6-48ea-b25d-ddcaf295d831",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "dfa6611b-b945-4d11-ee04-695bc8871172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: requires_grad=False\n",
      "encoder.block.0.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.0.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.1.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.1.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.1.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.1.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.2.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.2.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.2.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.2.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.3.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.3.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.3.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.3.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.4.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.4.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.4.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.4.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.5.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.5.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.5.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.5.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.6.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.6.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.6.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.6.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.6.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.6.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.7.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.7.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.7.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.7.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.7.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.7.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.8.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.8.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.8.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.8.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.8.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.8.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.8.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.9.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.9.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.9.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.9.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.9.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.9.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.9.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.10.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.10.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.10.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.10.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.10.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.10.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.10.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.11.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.11.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.11.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.11.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.11.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.11.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.11.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.12.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.12.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.12.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.12.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.12.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.12.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.12.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.12.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.12.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.13.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.13.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.13.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.13.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.13.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.13.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.13.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.13.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.13.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.14.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.14.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.14.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.14.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.14.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.14.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.14.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.14.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.14.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.15.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.15.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.15.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.15.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.15.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.15.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.15.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.15.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.15.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.16.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.16.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.16.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.16.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.16.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.16.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.16.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.16.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.16.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.17.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.17.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.17.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.17.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.17.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.17.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.17.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.17.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.17.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.18.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.18.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.18.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.18.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.18.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.18.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.18.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.18.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.18.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.19.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.19.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.19.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.19.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.19.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.19.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.19.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.19.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.19.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.20.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.20.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.20.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.20.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.20.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.20.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.20.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.20.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.20.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.21.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.21.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.21.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.21.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.21.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.21.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.21.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.21.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.21.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.22.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.22.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.22.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.22.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.22.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.22.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.22.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.22.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.22.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.block.23.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "encoder.block.23.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "encoder.block.23.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "encoder.block.23.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "encoder.block.23.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.0.layer_norm.weight: requires_grad=False\n",
      "encoder.block.23.layer.1.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "encoder.block.23.layer.1.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.1.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "encoder.block.23.layer.1.DenseReluDense.wo.weight: requires_grad=False\n",
      "encoder.block.23.layer.1.layer_norm.weight: requires_grad=False\n",
      "encoder.final_layer_norm.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: requires_grad=False\n",
      "decoder.block.0.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.0.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.0.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.1.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.1.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.1.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.2.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.2.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.2.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.3.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.3.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.3.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.4.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.4.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.4.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.5.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.5.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.5.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.6.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.6.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.6.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.6.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.6.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.6.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.7.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.7.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.7.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.7.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.7.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.7.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.8.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.8.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.8.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.8.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.8.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.8.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.9.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.9.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.9.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.9.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.9.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.9.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.10.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.10.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.10.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.10.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.10.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.10.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.11.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.11.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.11.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.11.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.11.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.11.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.12.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.12.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.12.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.12.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.12.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.12.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.13.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.13.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.13.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.13.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.13.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.13.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.14.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.14.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.14.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.14.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.14.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.14.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.15.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.15.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.15.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.15.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.15.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.15.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.16.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.16.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.16.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.16.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.16.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.16.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.17.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.17.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.17.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.17.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.17.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.17.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.18.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.18.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.18.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.18.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.18.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.18.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.19.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.19.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.19.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.19.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.19.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.19.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.20.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.20.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.20.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.20.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.20.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.20.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.21.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.21.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.21.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.21.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.21.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.21.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.22.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.22.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.22.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.22.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.22.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.22.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.block.23.layer.0.SelfAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.0.SelfAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.0.SelfAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.0.SelfAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.0.SelfAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.SelfAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.0.layer_norm.weight: requires_grad=False\n",
      "decoder.block.23.layer.1.EncDecAttention.q.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.1.EncDecAttention.q.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.q.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.k.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.1.EncDecAttention.k.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.k.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.v.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.1.EncDecAttention.v.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.v.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.o.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.1.EncDecAttention.o.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.EncDecAttention.o.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.1.layer_norm.weight: requires_grad=False\n",
      "decoder.block.23.layer.2.DenseReluDense.wi.base_layer.weight: requires_grad=False\n",
      "decoder.block.23.layer.2.DenseReluDense.wi.lora_A.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.2.DenseReluDense.wi.lora_B.default.weight: requires_grad=True\n",
      "decoder.block.23.layer.2.DenseReluDense.wo.weight: requires_grad=False\n",
      "decoder.block.23.layer.2.layer_norm.weight: requires_grad=False\n",
      "decoder.final_layer_norm.weight: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HBVZYYihNrWi",
   "metadata": {
    "id": "HBVZYYihNrWi"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f981a4b-ef0d-46c9-8c57-8e5c47a6f8e8",
   "metadata": {
    "id": "2f981a4b-ef0d-46c9-8c57-8e5c47a6f8e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_url = \"google-t5/t5-large\"\n",
    "new_model = \"asdfasda112312/t5-3b-man-pages\"\n",
    "\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_url)\n",
    "\n",
    "base_model_reload = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model_url,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8IQJmz9f3akO",
   "metadata": {
    "id": "8IQJmz9f3akO"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084a060-af61-4e87-b9b4-5b5a181ab8f5",
   "metadata": {
    "id": "c084a060-af61-4e87-b9b4-5b5a181ab8f5",
    "outputId": "8a118273-6f2d-43fb-d3c6-c8718bb7dd2e",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target modules {'base_layer'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_reload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/peft_model.py:578\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    571\u001b[0m         model,\n\u001b[1;32m    572\u001b[0m         config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m model\u001b[38;5;241m.\u001b[39mload_adapter(\n\u001b[1;32m    587\u001b[0m     model_id,\n\u001b[1;32m    588\u001b[0m     adapter_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/peft_model.py:1831\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1829\u001b[0m     \u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1830\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1831\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_encoder_decoder_kwargs_for_generation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1834\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39m_prepare_encoder_decoder_kwargs_for_generation\n\u001b[1;32m   1835\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/peft_model.py:171\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    169\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cast_adapter_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/tuners/lora/model.py:141\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name, low_cpu_mem_usage: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:184\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:509\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Handle X-LoRA case.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(peft_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_modules\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# It's important to set the adapter here (again), because otherwise it can happen that if a 2nd adapter is\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# added, and it targets different layer(s) than the first adapter (which is active), then those different\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# layers will be activated, which we don't want.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_adapter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters)\n",
      "\u001b[0;31mValueError\u001b[0m: Target modules {'base_layer'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(base_model_reload, new_model)\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df34574-f332-48f2-9117-78f911a1019e",
   "metadata": {
    "id": "7df34574-f332-48f2-9117-78f911a1019e",
    "outputId": "79de1b33-2d2c-4845-fb58-c81d3b30f578"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.88G/1.88G [11:31<00:00, 2.72MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/asdfasda112312/t5-3b-man-pages-merged/commit/0a09219b386fac2ad7bcaca839d395383ec6be77', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='0a09219b386fac2ad7bcaca839d395383ec6be77', pr_url=None, repo_url=RepoUrl('https://huggingface.co/asdfasda112312/t5-3b-man-pages-merged', endpoint='https://huggingface.co', repo_type='model', repo_id='asdfasda112312/t5-3b-man-pages-merged'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"t5-3b-man-pages-merged\")\n",
    "model.push_to_hub(\"t5-3b-man-pages-merged\", use_temp_dir=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6096fc6f-e876-4440-9511-d3ed76e5ece2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6096fc6f-e876-4440-9511-d3ed76e5ece2",
    "outputId": "429d0731-afeb-4fbe-a89a-fa6cd6dbbe83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <unk>SECTION>NAME<unk>/SECTION> codeshit - write a shit code <unk>SECTION>SYNOPSIS<unk>/SECTION> shit [-H] <unk>SECTION>DESCRIPTION<unk>/SECTION> shit writes code shit. On command line, shit reports the error code and gives the user instructions to write the code. Use shit to write the code. <unk>SECTION>SEE CUT STAT(1)</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"<SECTION>NAME</SECTION>codeshit - write shit code\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=1024, temperature=0.8, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6881204-7a9c-4af2-a84a-cb43924cea6b",
   "metadata": {
    "id": "b6881204-7a9c-4af2-a84a-cb43924cea6b"
   },
   "outputs": [],
   "source": [
    "model = model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Gemma-2-2b-Joke-Merged\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d74ff-36fd-491f-b266-93e97474ee4d",
   "metadata": {
    "id": "ae4d74ff-36fd-491f-b266-93e97474ee4d"
   },
   "outputs": [],
   "source": [
    "input_text = \"I was walking down the street, when I saw a dog with no leash.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb753b-1369-4cff-b015-dc607323d37e",
   "metadata": {
    "id": "7feb753b-1369-4cff-b015-dc607323d37e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a2fd6855667473ca87a48d95722f29f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ba03846819f4e3ca89dd28f4c939f70",
      "placeholder": "​",
      "style": "IPY_MODEL_e3577a5d7b3d496999e9467ac6c35747",
      "value": " 2029/2029 [00:31&lt;00:00, 61.56 examples/s]"
     }
    },
    "11eb7059317c4dfabc7d4648e2307f8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d487efb02cc145b1a1b0d18e9a641354",
      "placeholder": "​",
      "style": "IPY_MODEL_433c8f1e498745cc8219e7feca67645e",
      "value": " 9.48M/9.48M [00:02&lt;00:00, 7.62MB/s]"
     }
    },
    "12eece125e854c99b1b1a75a439fdd6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a929ec96f98240a18133cb3988c9139f",
      "max": 2029,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ee908ba9473404d9fd51bbf07b8ae98",
      "value": 2029
     }
    },
    "2ee908ba9473404d9fd51bbf07b8ae98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "33935c76b57948468b861d657fc9573f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_83a1afb24ea54eb18b30009a5be2f37f",
       "IPY_MODEL_4758f4169f7e44e2818648873f08e049",
       "IPY_MODEL_4e74976f45b441edb956a59c338c5772"
      ],
      "layout": "IPY_MODEL_a20db37b9f7341b8a256e5ce7c4e4658"
     }
    },
    "344eeba5c2cb4251a30ae4d71235cf89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36ec7f05c7f741e0a5a6afb9faf0d5a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a5de6f2752f4ed88bf9cffd3a41a83b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a92aa796fe843a78b482d0db8f1b57a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_960e4129039145ac9882cd7f0f59005d",
      "max": 9477544,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6ce6405728c4afeb7d4b8d8285ae671",
      "value": 9477544
     }
    },
    "3e5efa49cfc4412d9e8c0d9e26145e54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "433c8f1e498745cc8219e7feca67645e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4758f4169f7e44e2818648873f08e049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71980821352641788e0b9c8a90b1abfb",
      "max": 8116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8a57a71ca40a4dbc82b5b290fc68284a",
      "value": 8116
     }
    },
    "4e74976f45b441edb956a59c338c5772": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1afba226a4545f7b28c58f5bf970cd1",
      "placeholder": "​",
      "style": "IPY_MODEL_3e5efa49cfc4412d9e8c0d9e26145e54",
      "value": " 8116/8116 [01:09&lt;00:00, 102.35 examples/s]"
     }
    },
    "565601837a54461abd32372cd98ca32c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dace2be826664e86906a2650aae18d6f",
      "placeholder": "​",
      "style": "IPY_MODEL_3a5de6f2752f4ed88bf9cffd3a41a83b",
      "value": "adapter_model.safetensors: 100%"
     }
    },
    "59e3a962819345ed887a3746704de5d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36ec7f05c7f741e0a5a6afb9faf0d5a5",
      "placeholder": "​",
      "style": "IPY_MODEL_344eeba5c2cb4251a30ae4d71235cf89",
      "value": "Map: 100%"
     }
    },
    "59f60b65f13f4bcb88c477598befc5b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_565601837a54461abd32372cd98ca32c",
       "IPY_MODEL_3a92aa796fe843a78b482d0db8f1b57a",
       "IPY_MODEL_11eb7059317c4dfabc7d4648e2307f8e"
      ],
      "layout": "IPY_MODEL_bf5c0a88a03c45d9830b72c768c71b1c"
     }
    },
    "6ba03846819f4e3ca89dd28f4c939f70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71980821352641788e0b9c8a90b1abfb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73e2e453d9514749a2ce6e555b8857af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_59e3a962819345ed887a3746704de5d7",
       "IPY_MODEL_12eece125e854c99b1b1a75a439fdd6b",
       "IPY_MODEL_0a2fd6855667473ca87a48d95722f29f"
      ],
      "layout": "IPY_MODEL_a59cb746f96e406380b053c9ca2ed057"
     }
    },
    "7c7aceab5c044f0eb395b649bc78df36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83a1afb24ea54eb18b30009a5be2f37f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f63784a5672f45c0bca36d5afc3e4793",
      "placeholder": "​",
      "style": "IPY_MODEL_7c7aceab5c044f0eb395b649bc78df36",
      "value": "Map: 100%"
     }
    },
    "8a57a71ca40a4dbc82b5b290fc68284a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "960e4129039145ac9882cd7f0f59005d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a20db37b9f7341b8a256e5ce7c4e4658": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a59cb746f96e406380b053c9ca2ed057": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a929ec96f98240a18133cb3988c9139f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf5c0a88a03c45d9830b72c768c71b1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d487efb02cc145b1a1b0d18e9a641354": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dace2be826664e86906a2650aae18d6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3577a5d7b3d496999e9467ac6c35747": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1afba226a4545f7b28c58f5bf970cd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f63784a5672f45c0bca36d5afc3e4793": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6ce6405728c4afeb7d4b8d8285ae671": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
